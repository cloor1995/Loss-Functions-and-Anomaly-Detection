{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# [Thyroid Disease dataset](http://odds.cs.stonybrook.edu/thyroid-disease-dataset)\n",
        "\n",
        "\n",
        "Dataset information\n",
        "The original thyroid disease (ann-thyroid) dataset from UCI machine learning repository is a classification dataset, which is suited for training ANNs. It has 3772 training instances and 3428 testing instances. It has 15 categorical and 6 real attributes. The problem is to determine whether a patient referred to the clinic is hypothyroid. Therefore three classes are built: normal (not hypothyroid), hyperfunction and subnormal functioning. For outlier detection, 3772 training instances are used, with only 6 real attributes. The hyperfunction class is treated as outlier class and other two classes are inliers, because hyperfunction is a clear minority class.\n",
        "\n",
        "Source (citation)\n",
        "F. Keller, E. Muller, K. Bohm.“HiCS: High-contrast subspaces for density-based outlier ranking.” ICDE, 2012.\n",
        "\n",
        "C. C. Aggarwal and S. Sathe, “Theoretical foundations and algorithms for outlier ensembles.” ACM SIGKDD Explorations Newsletter, vol. 17, no. 1, pp. 24–47, 2015.Downloads\n",
        "\n",
        "Saket Sathe and Charu C. Aggarwal. LODES: Local Density meets Spectral Outlier Detection. SIAM Conference on Data Mining, 2016.\n",
        "\n",
        "Downloads\n",
        "File: thyroid.mat\n",
        "\n",
        "Description: X = Multi-dimensional point data, y = labels (1 = outliers, 0 = inliers)"
      ],
      "metadata": {
        "id": "vekpui3ROUuO"
      },
      "id": "vekpui3ROUuO"
    },
    {
      "cell_type": "markdown",
      "id": "534c9f8f",
      "metadata": {
        "papermill": {
          "duration": 0.034124,
          "end_time": "2021-07-25T13:21:32.083955",
          "exception": false,
          "start_time": "2021-07-25T13:21:32.049831",
          "status": "completed"
        },
        "tags": [],
        "id": "534c9f8f"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyod"
      ],
      "metadata": {
        "id": "gOvrCcCmGBXt"
      },
      "id": "gOvrCcCmGBXt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mlconfig"
      ],
      "metadata": {
        "id": "KdpZU0b3GZH-"
      },
      "id": "KdpZU0b3GZH-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install segmentation_models_3D"
      ],
      "metadata": {
        "id": "4Quy4RAt5Wjh"
      },
      "id": "4Quy4RAt5Wjh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/jonbarron/robust_loss_pytorch\n"
      ],
      "metadata": {
        "id": "Nnp6tRAC6iQp"
      },
      "id": "Nnp6tRAC6iQp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bbfd417",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:32.152997Z",
          "iopub.status.busy": "2021-07-25T13:21:32.151665Z",
          "iopub.status.idle": "2021-07-25T13:21:39.867360Z",
          "shell.execute_reply": "2021-07-25T13:21:39.866498Z",
          "shell.execute_reply.started": "2021-07-25T13:20:27.135966Z"
        },
        "papermill": {
          "duration": 7.748097,
          "end_time": "2021-07-25T13:21:39.867567",
          "exception": false,
          "start_time": "2021-07-25T13:21:32.119470",
          "status": "completed"
        },
        "tags": [],
        "id": "3bbfd417"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import seaborn as sns; sns.set()\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import mlconfig\n",
        "import random as rn\n",
        "import keras\n",
        "import segmentation_models_3D as sm\n",
        "import robust_loss_pytorch \n",
        "import shutil\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "from time import strftime\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, precision_recall_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import normalize\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n",
        "                             roc_curve, recall_score, classification_report, f1_score,\n",
        "                             precision_recall_fscore_support)\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score,cohen_kappa_score,roc_auc_score,f1_score,auc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Achtung: Alle Dateien wurden in einem Ordner auf Drive gespeichert. Um den Code zu replizieren empfehle ich, ebenfalls einen Ordner auf Drive zu erstellen und folgendermaßen das working directory anzupassen:\n",
        "\n",
        "```\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "```\n",
        "\n",
        "```\n",
        "cd /content/drive/MyDrive/<specify here your path>\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "a6NAmi0xmCGJ"
      },
      "id": "a6NAmi0xmCGJ"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "i3W2O7nA8Ywm"
      },
      "id": "i3W2O7nA8Ywm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "assert sys.version_info >= (3, 7) #python 3.7 or higher recommended"
      ],
      "metadata": {
        "id": "f5zzIbWeEQoF"
      },
      "id": "f5zzIbWeEQoF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from packaging import version\n",
        "import sklearn\n",
        "\n",
        "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\") #sklearn 1.01 or higher remmonded"
      ],
      "metadata": {
        "id": "bC4XPHKTEWsb"
      },
      "id": "bC4XPHKTEWsb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\") #tensorflow 2.8.0 or higher remmonded "
      ],
      "metadata": {
        "id": "2cVkUkOaEcrS"
      },
      "id": "2cVkUkOaEcrS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardeinstellungen für Plots anpassen"
      ],
      "metadata": {
        "id": "UBkgZ7vMDfc1"
      },
      "id": "UBkgZ7vMDfc1"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)"
      ],
      "metadata": {
        "id": "DmnRbn6sDanW"
      },
      "id": "DmnRbn6sDanW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Seminar2/ADAPL #specify here your path"
      ],
      "metadata": {
        "id": "LOCKjhUXFDvh"
      },
      "id": "LOCKjhUXFDvh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Erstellen eines Bildordners zum Sammeln aller Bildausgaben\n",
        "\n",
        "Erstellen Sie eine Funktion zum automatischen Speichern eines Plots in einem bestimmten Pfad."
      ],
      "metadata": {
        "id": "ujovvzSuE3Lg"
      },
      "id": "ujovvzSuE3Lg"
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "IMAGES_PATH = Path() / \"images\" / \"thyroid\"\n",
        "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ],
      "metadata": {
        "id": "59KLrFMFE034"
      },
      "id": "59KLrFMFE034",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "gAw-T9e69ZVL"
      },
      "id": "gAw-T9e69ZVL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "Vhz7fY5Q0MfF"
      },
      "id": "Vhz7fY5Q0MfF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a6f67029",
      "metadata": {
        "papermill": {
          "duration": 0.027964,
          "end_time": "2021-07-25T13:21:39.924769",
          "exception": false,
          "start_time": "2021-07-25T13:21:39.896805",
          "status": "completed"
        },
        "tags": [],
        "id": "a6f67029"
      },
      "source": [
        "# Data Wrapping "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lade .mat Datei und konventiere zu pandas Dataframe"
      ],
      "metadata": {
        "id": "rMJxmunt80gc"
      },
      "id": "rMJxmunt80gc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23f8686a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:39.989429Z",
          "iopub.status.busy": "2021-07-25T13:21:39.988272Z",
          "iopub.status.idle": "2021-07-25T13:21:40.061055Z",
          "shell.execute_reply": "2021-07-25T13:21:40.061646Z",
          "shell.execute_reply.started": "2021-07-25T13:20:27.151048Z"
        },
        "papermill": {
          "duration": 0.109014,
          "end_time": "2021-07-25T13:21:40.061847",
          "exception": false,
          "start_time": "2021-07-25T13:21:39.952833",
          "status": "completed"
        },
        "tags": [],
        "id": "23f8686a"
      },
      "outputs": [],
      "source": [
        "import scipy.io\n",
        "\n",
        "data_dict = scipy.io.loadmat(\"thyroid.mat\")\n",
        "data_dict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for key, values in data_dict.items():\n",
        "  data.append(values)\n",
        "data = data[3:]\n",
        "df = data[0]\n",
        "labels = data[1]\n",
        "df = pd.DataFrame(data=df)\n",
        "df[\"label\"] = labels\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "PxjBQAkdGcyq"
      },
      "id": "PxjBQAkdGcyq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e3e32da",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:40.138132Z",
          "iopub.status.busy": "2021-07-25T13:21:40.137413Z",
          "iopub.status.idle": "2021-07-25T13:21:40.233851Z",
          "shell.execute_reply": "2021-07-25T13:21:40.233292Z",
          "shell.execute_reply.started": "2021-07-25T13:20:27.192627Z"
        },
        "papermill": {
          "duration": 0.14124,
          "end_time": "2021-07-25T13:21:40.234017",
          "exception": false,
          "start_time": "2021-07-25T13:21:40.092777",
          "status": "completed"
        },
        "tags": [],
        "id": "1e3e32da"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e023884f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:40.315199Z",
          "iopub.status.busy": "2021-07-25T13:21:40.314023Z",
          "iopub.status.idle": "2021-07-25T13:21:40.318207Z",
          "shell.execute_reply": "2021-07-25T13:21:40.318690Z",
          "shell.execute_reply.started": "2021-07-25T13:20:27.274520Z"
        },
        "papermill": {
          "duration": 0.054244,
          "end_time": "2021-07-25T13:21:40.318870",
          "exception": false,
          "start_time": "2021-07-25T13:21:40.264626",
          "status": "completed"
        },
        "tags": [],
        "id": "e023884f"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4000e47d",
      "metadata": {
        "papermill": {
          "duration": 0.028919,
          "end_time": "2021-07-25T13:21:40.377325",
          "exception": false,
          "start_time": "2021-07-25T13:21:40.348406",
          "status": "completed"
        },
        "tags": [],
        "id": "4000e47d"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceccb042",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:40.695925Z",
          "iopub.status.busy": "2021-07-25T13:21:40.695283Z",
          "iopub.status.idle": "2021-07-25T13:21:40.698996Z",
          "shell.execute_reply": "2021-07-25T13:21:40.699704Z",
          "shell.execute_reply.started": "2021-07-25T13:20:27.310014Z"
        },
        "papermill": {
          "duration": 0.038717,
          "end_time": "2021-07-25T13:21:40.699919",
          "exception": false,
          "start_time": "2021-07-25T13:21:40.661202",
          "status": "completed"
        },
        "tags": [],
        "id": "ceccb042"
      },
      "outputs": [],
      "source": [
        "print(f'Data size is {df.shape}.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "RPpPZ23NK9df"
      },
      "id": "RPpPZ23NK9df",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check for NA-Values \n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "ho1HV5CaW8-T"
      },
      "id": "ho1HV5CaW8-T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keine NA vorhanden"
      ],
      "metadata": {
        "id": "6By5HWBN88xj"
      },
      "id": "6By5HWBN88xj"
    },
    {
      "cell_type": "markdown",
      "id": "7bf1b5a5",
      "metadata": {
        "papermill": {
          "duration": 0.029306,
          "end_time": "2021-07-25T13:21:40.759005",
          "exception": false,
          "start_time": "2021-07-25T13:21:40.729699",
          "status": "completed"
        },
        "tags": [],
        "id": "7bf1b5a5"
      },
      "source": [
        "# EDA and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print boxplot for each variable\n",
        "fig, axs = plt.subplots(ncols=4, nrows=2, figsize=(20, 10))\n",
        "index = 0\n",
        "axs = axs.flatten()\n",
        "for k,v in df.items():\n",
        "    sns.boxplot(y=k, data=df, ax=axs[index])\n",
        "    index += 1\n",
        "save_fig(\"boxplots\")\n",
        "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)"
      ],
      "metadata": {
        "id": "g5gOVHsoXZj2"
      },
      "id": "g5gOVHsoXZj2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k, v in df.items():\n",
        "        q1 = v.quantile(0.25)\n",
        "        q3 = v.quantile(0.75)\n",
        "        irq = q3 - q1\n",
        "        v_col = v[(v <= q1 - 1.5 * irq) | (v >= q3 + 1.5 * irq)]\n",
        "        perc = np.shape(v_col)[0] * 100.0 / np.shape(df)[0]\n",
        "        print(\"Column %s outliers = %.2f%%\" % (k, perc))"
      ],
      "metadata": {
        "id": "kF4jz7nKYmr_"
      },
      "id": "kF4jz7nKYmr_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print distributions\n",
        "fig, axs = plt.subplots(ncols=4, nrows=2, figsize=(20, 10))\n",
        "index = 0\n",
        "axs = axs.flatten()\n",
        "for k,v in df.items():\n",
        "    sns.distplot(v, ax=axs[index])\n",
        "    index += 1\n",
        "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)\n",
        "save_fig(\"distrubutions\")"
      ],
      "metadata": {
        "id": "CoEAggn0Y2Je"
      },
      "id": "CoEAggn0Y2Je",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a45194fb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:40.824938Z",
          "iopub.status.busy": "2021-07-25T13:21:40.824315Z",
          "iopub.status.idle": "2021-07-25T13:21:41.055800Z",
          "shell.execute_reply": "2021-07-25T13:21:41.056331Z",
          "shell.execute_reply.started": "2021-07-25T13:20:27.323572Z"
        },
        "papermill": {
          "duration": 0.26791,
          "end_time": "2021-07-25T13:21:41.056508",
          "exception": false,
          "start_time": "2021-07-25T13:21:40.788598",
          "status": "completed"
        },
        "tags": [],
        "id": "a45194fb"
      },
      "outputs": [],
      "source": [
        "df['label'].value_counts().plot(kind='bar', figsize=(8, 4));\n",
        "plt.title('Distrubution of outliers');\n",
        "plt.xlabel('Diagnosis');\n",
        "plt.ylabel('Frequency');\n",
        "plt.xticks([0.0, 1.0], ['Normal', 'Anomaly'], rotation=45);\n",
        "save_fig(\"distrubution outliers\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "\n",
        "sns.set(style='whitegrid', context='notebook')\n",
        "\n",
        "def tsne_scatter(features, labels, dimensions=2, save_as='graph.png', RANDOM_SEED = 42):\n",
        "    if dimensions not in (2, 3):\n",
        "        raise ValueError('tsne_scatter can only plot in 2d or 3d')\n",
        "\n",
        "    # t-SNE dimensionality reduction\n",
        "    features_embedded = TSNE(n_components=dimensions, random_state=RANDOM_SEED).fit_transform(features)\n",
        "    \n",
        "    # initialising the plot\n",
        "    fig, ax = plt.subplots(figsize=(8,8))\n",
        "    \n",
        "    # counting dimensions\n",
        "    if dimensions == 3: ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    # plotting data\n",
        "    ax.scatter(\n",
        "        *zip(*features_embedded[np.where(labels==1)]),\n",
        "        marker='o',\n",
        "        color='r',\n",
        "        s=2,\n",
        "        alpha=0.7,\n",
        "        label='Anomaly'\n",
        "    )\n",
        "    ax.scatter(\n",
        "        *zip(*features_embedded[np.where(labels==0)]),\n",
        "        marker='o',\n",
        "        color='g',\n",
        "        s=2,\n",
        "        alpha=0.3,\n",
        "        label='Normal'\n",
        "    )\n",
        "\n",
        "    # storing it to be displayed later\n",
        "    sns.set(style='whitegrid', context='notebook')\n",
        "    save_fig(save_as)\n",
        "    plt.show;"
      ],
      "metadata": {
        "id": "ZJLkGL0MyQ91"
      },
      "id": "ZJLkGL0MyQ91",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26491ce4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:41.122609Z",
          "iopub.status.busy": "2021-07-25T13:21:41.121980Z",
          "iopub.status.idle": "2021-07-25T13:21:41.147594Z",
          "shell.execute_reply": "2021-07-25T13:21:41.148118Z",
          "shell.execute_reply.started": "2021-07-25T13:20:27.473227Z"
        },
        "papermill": {
          "duration": 0.060924,
          "end_time": "2021-07-25T13:21:41.148323",
          "exception": false,
          "start_time": "2021-07-25T13:21:41.087399",
          "status": "completed"
        },
        "tags": [],
        "id": "26491ce4"
      },
      "outputs": [],
      "source": [
        "df['anomaly'] = df['label'] == 1.0\n",
        "anomaly = df[df['anomaly'] == True]\n",
        "normal = df[df['anomaly'] == False]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29a4a399",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:41.214548Z",
          "iopub.status.busy": "2021-07-25T13:21:41.213867Z",
          "iopub.status.idle": "2021-07-25T13:21:42.159895Z",
          "shell.execute_reply": "2021-07-25T13:21:42.160398Z",
          "shell.execute_reply.started": "2021-07-25T13:20:27.481699Z"
        },
        "papermill": {
          "duration": 0.980319,
          "end_time": "2021-07-25T13:21:42.160614",
          "exception": false,
          "start_time": "2021-07-25T13:21:41.180295",
          "status": "completed"
        },
        "tags": [],
        "id": "29a4a399"
      },
      "outputs": [],
      "source": [
        "sns.distplot(normal);\n",
        "sns.distplot(anomaly);\n",
        "\n",
        "plt.title('normal vs  anomaly Dist.');\n",
        "plt.ylabel('Dist.');\n",
        "save_fig(\"normal vs anomaly distrubution\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fcb1223",
      "metadata": {
        "papermill": {
          "duration": 0.033415,
          "end_time": "2021-07-25T13:21:42.227161",
          "exception": false,
          "start_time": "2021-07-25T13:21:42.193746",
          "status": "completed"
        },
        "tags": [],
        "id": "4fcb1223"
      },
      "source": [
        "# Normalize The Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = df.iloc[:, 0:6]\n",
        "data"
      ],
      "metadata": {
        "id": "ejQS1dtvS3EV"
      },
      "id": "ejQS1dtvS3EV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0bf83c2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:42.296276Z",
          "iopub.status.busy": "2021-07-25T13:21:42.295602Z",
          "iopub.status.idle": "2021-07-25T13:21:42.303352Z",
          "shell.execute_reply": "2021-07-25T13:21:42.303827Z",
          "shell.execute_reply.started": "2021-07-25T13:20:28.117717Z"
        },
        "papermill": {
          "duration": 0.043478,
          "end_time": "2021-07-25T13:21:42.304045",
          "exception": false,
          "start_time": "2021-07-25T13:21:42.260567",
          "status": "completed"
        },
        "tags": [],
        "id": "f0bf83c2"
      },
      "outputs": [],
      "source": [
        "# The last element contains the labels\n",
        "labels_bool = df.iloc[:,-1]\n",
        "labels  = df.iloc[:,-2]\n",
        "# The other data points are the features\n",
        "data = df.iloc[:, 0:6]\n",
        "\n",
        "data = normalize(data) #normalize data\n",
        "n_features = 6"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "OocSN144Suyx"
      },
      "id": "OocSN144Suyx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tsne_scatter(data, labels, dimensions=2, save_as='tsne_initial_2d')\n",
        "tsne_scatter(data, labels, dimensions=3, save_as='tsne_initial_3d')"
      ],
      "metadata": {
        "id": "UW_vlLmIy_Qq"
      },
      "id": "UW_vlLmIy_Qq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Die Anomalien werden mit T-SNE zum Großteil in Cluster eingeteilt. "
      ],
      "metadata": {
        "id": "LI47kaeb9G3J"
      },
      "id": "LI47kaeb9G3J"
    },
    {
      "cell_type": "markdown",
      "id": "b7a3e8c0",
      "metadata": {
        "papermill": {
          "duration": 0.031913,
          "end_time": "2021-07-25T13:21:42.369025",
          "exception": false,
          "start_time": "2021-07-25T13:21:42.337112",
          "status": "completed"
        },
        "tags": [],
        "id": "b7a3e8c0"
      },
      "source": [
        "# Split the data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generiere Trainings- und Testdaten"
      ],
      "metadata": {
        "id": "kFlKmZZDUk-B"
      },
      "id": "kFlKmZZDUk-B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "968cc560",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:42.436944Z",
          "iopub.status.busy": "2021-07-25T13:21:42.436304Z",
          "iopub.status.idle": "2021-07-25T13:21:42.442700Z",
          "shell.execute_reply": "2021-07-25T13:21:42.443205Z",
          "shell.execute_reply.started": "2021-07-25T13:20:28.126744Z"
        },
        "papermill": {
          "duration": 0.042295,
          "end_time": "2021-07-25T13:21:42.443403",
          "exception": false,
          "start_time": "2021-07-25T13:21:42.401108",
          "status": "completed"
        },
        "tags": [],
        "id": "968cc560"
      },
      "outputs": [],
      "source": [
        "train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "    data, labels_bool,\n",
        "    test_size=0.2,\n",
        "    \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generiere das Validation-Set"
      ],
      "metadata": {
        "id": "EfXD36r91arN"
      },
      "id": "EfXD36r91arN"
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_no_validation, train_data_validation, train_data_no_validation_labels, train_data_validation_labels = train_test_split(\n",
        "    train_data,\n",
        "    train_labels,\n",
        "    test_size = 0.2,\n",
        ")\n",
        "# only train_data_validation and train_data_validation_labels are needed for further analysis"
      ],
      "metadata": {
        "id": "kJ8o_BX00ock"
      },
      "id": "kJ8o_BX00ock",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cef295c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:42.510580Z",
          "iopub.status.busy": "2021-07-25T13:21:42.509856Z",
          "iopub.status.idle": "2021-07-25T13:21:42.515158Z",
          "shell.execute_reply": "2021-07-25T13:21:42.515651Z",
          "shell.execute_reply.started": "2021-07-25T13:20:28.142992Z"
        },
        "papermill": {
          "duration": 0.040807,
          "end_time": "2021-07-25T13:21:42.515830",
          "exception": false,
          "start_time": "2021-07-25T13:21:42.475023",
          "status": "completed"
        },
        "tags": [],
        "id": "5cef295c"
      },
      "outputs": [],
      "source": [
        "anomalous_train_data = train_data[train_labels] #training data with outlier-label == True\n",
        "anomalous_test_data = test_data[test_labels] #test data with outlier-label == True\n",
        "\n",
        "normal_train_data = train_data[~train_labels] #training data with outlier-label == False\n",
        "normal_test_data = test_data[~test_labels] #test data with outlier-label == False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for k in [train_data, train_labels, train_data_validation, train_data_validation_labels]: \n",
        " print(k.shape)"
      ],
      "metadata": {
        "id": "EGsGaiP01R7w"
      },
      "id": "EGsGaiP01R7w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in [train_data, test_data, train_labels, test_labels]: \n",
        " print(k.shape)"
      ],
      "metadata": {
        "id": "vZhjp7VNZIMr"
      },
      "id": "vZhjp7VNZIMr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in [anomalous_train_data, anomalous_test_data, normal_train_data, normal_test_data]: \n",
        " print(k.shape)"
      ],
      "metadata": {
        "id": "rOIRuAXJngZm"
      },
      "id": "rOIRuAXJngZm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Anomaly Share for training data equals ={:10.4f}\".format(len(anomalous_train_data)/len(train_data)))\n",
        "print(\"Anomaly Share for test data equals ={:10.4f}\".format(len(anomalous_test_data)/len(test_data)))"
      ],
      "metadata": {
        "id": "b9uGwVVTnr6j"
      },
      "id": "b9uGwVVTnr6j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "41902a32",
      "metadata": {
        "papermill": {
          "duration": 0.032604,
          "end_time": "2021-07-25T13:21:42.580702",
          "exception": false,
          "start_time": "2021-07-25T13:21:42.548098",
          "status": "completed"
        },
        "tags": [],
        "id": "41902a32"
      },
      "source": [
        "# Train the model with train & validation data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Randomized autoencoders\n",
        "\n",
        "Die Implementierung für die RandAE-Klasse stammt aus folgendem [Repository](https://github.com/danieltsoukup/autoencoders):"
      ],
      "metadata": {
        "id": "PKnXqpbJfU8j"
      },
      "id": "PKnXqpbJfU8j"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aktivierungsfunktionen\n",
        "\n",
        "*   Hidden Layer: Relu\n",
        "*   Output Layer: Sigmoid\n",
        "\n",
        "Es wird ein Drop_ratio von 0.5 festgelegt. Ein Drop_ratio von 0.0 entspricht einer *fully connected architecture*\n",
        "\n"
      ],
      "metadata": {
        "id": "AYCNoY07UZF3"
      },
      "id": "AYCNoY07UZF3"
    },
    {
      "cell_type": "code",
      "source": [
        "class RandAE(tf.keras.Sequential):\n",
        "    def __init__(self, input_dim, hidden_dims, drop_ratio=0.5, **kwargs):\n",
        "        super(RandAE, self).__init__(**kwargs)\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.drop_ratio = drop_ratio\n",
        "        \n",
        "        self.layer_masks = dict()\n",
        "        \n",
        "        self.build_model()\n",
        "                \n",
        "    def build_model(self) -> None:\n",
        "        \"\"\"\n",
        "        Adds the layers and records masks.\n",
        "        \"\"\"\n",
        "        \n",
        "        self.add(layers.Input(self.input_dim, name=\"input\"))\n",
        "        \n",
        "        for i, dim in enumerate(self.hidden_dims):\n",
        "            layer_name = f\"hidden_{i}\"\n",
        "            layer = layers.Dense(dim, \n",
        "                                 activation=\"relu\" if i > 0 else \"sigmoid\", \n",
        "                                 name=layer_name)\n",
        "            self.add(layer)\n",
        "            \n",
        "            # add layer mask\n",
        "            self.layer_masks[layer_name] = self.get_mask(layer)\n",
        "        \n",
        "        layer_name = \"output\"\n",
        "        output_layer = layers.Dense(self.input_dim, activation=\"sigmoid\", name=layer_name)\n",
        "        self.add(output_layer)\n",
        "        self.layer_masks[layer_name] = self.get_mask(output_layer)\n",
        "            \n",
        "    def get_mask(self, layer) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Build mask for a layer.\n",
        "        \"\"\"\n",
        "        \n",
        "        shape = layer.input_shape[1], layer.output_shape[1]\n",
        "        \n",
        "        return np.random.choice([0., 1.], size=shape, p=[self.drop_ratio, 1-self.drop_ratio])\n",
        "        \n",
        "    def load_masks(self, mask_pickle_path) -> None:\n",
        "        \"\"\"\n",
        "        Load the masks from a pickled dictionary.\n",
        "        \"\"\"\n",
        "        \n",
        "        with open(mask_pickle_path, 'rb') as handle:\n",
        "            self.layer_masks = pickle.load(handle)    \n",
        "            \n",
        "    def get_encoder(self) -> keras.Sequential:\n",
        "        \"\"\"\n",
        "        Get the encoder from the full model.\n",
        "        \"\"\"\n",
        "        \n",
        "        n_layers = (len(self.hidden_dims)+1)//2\n",
        "        encoder_layers = [layers.Input(self.input_dim)] + self.layers[:n_layers]\n",
        "\n",
        "        return keras.Sequential(encoder_layers)\n",
        "        \n",
        "    \n",
        "    def mask_weights(self) -> None:\n",
        "        \"\"\"\n",
        "        Apply the masks to each layer in the encoder and decoder.\n",
        "        \"\"\"\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            layer_name = layer.name\n",
        "            if layer_name in self.layer_masks:\n",
        "                masked_w = layer.weights[0].numpy()*self.layer_masks[layer_name]\n",
        "                b = layer.weights[1].numpy()\n",
        "                layer.set_weights((masked_w, b))        \n",
        "\n",
        "    def call(self, data, training=True) -> tf.Tensor:\n",
        "        \n",
        "        # mask the weights before original forward pass\n",
        "        self.mask_weights()\n",
        "        \n",
        "        return super().call(data)"
      ],
      "metadata": {
        "id": "SvaT6tdyfYp3"
      },
      "id": "SvaT6tdyfYp3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_mse = RandAE(6,[24,12,6,3,6,12,24])\n",
        "model_mae = RandAE(6,[24,12,6,3,6,12,24])\n",
        "model_ce = RandAE(6,[24,12,6,3,6,12,24])\n",
        "model_focal = RandAE(6,[24,12,6,3,6,12,24])\n",
        "model_huber = RandAE(6,[24,12,6,3,6,12,24])"
      ],
      "metadata": {
        "id": "AGxkc_txfkAN"
      },
      "id": "AGxkc_txfkAN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "73sIBiifn4rk"
      },
      "id": "73sIBiifn4rk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trainiere das Modell mit dem MSE"
      ],
      "metadata": {
        "id": "x1lcz6QtUUFu"
      },
      "id": "x1lcz6QtUUFu"
    },
    {
      "cell_type": "code",
      "source": [
        "#load checkpoints\n",
        "# checkpoint_mse = tf.keras.callbacks.ModelCheckpoint(\"checkpoint_mse\",\n",
        "#                                                    save_weights_only=True) #generate checkpoints\n",
        "\n",
        "\n",
        "#define early stopping\n",
        "early_stopping_mse = tf.keras.callbacks.EarlyStopping(patience=15,\n",
        "                                                     restore_best_weights=True) \n",
        "\n",
        "\n",
        "\n",
        "                                                                                                                                                       \n",
        "model_mse.compile(loss=\"mean_squared_error\", optimizer=\"adam\", run_eagerly=True)\n",
        "\n",
        "\n",
        "history_mse = model_mse.fit(train_data,\n",
        "                    train_data,\n",
        "                    batch_size=16,\n",
        "                    epochs = 1000,\n",
        "                    validation_data=(train_data_validation, train_data_validation),\n",
        "                    callbacks=[\n",
        "                        # checkpoint_mse,\n",
        "                        early_stopping_mse,\n",
        "                        ],\n",
        "                    verbose = 0,\n",
        "                    validation_batch_size = 16,\n",
        "                    shuffle=True)\n",
        "model_mse.summary()"
      ],
      "metadata": {
        "id": "cES2dR_t2ms8"
      },
      "id": "cES2dR_t2ms8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint_mae = tf.keras.callbacks.ModelCheckpoint(\"checkpoint_mae\",\n",
        "#                                                    save_weights_only=True) #generate checkpoints\n",
        "early_stopping_mae = tf.keras.callbacks.EarlyStopping(patience=15,\n",
        "                                                     restore_best_weights=True) #define early stopping                                                  \n",
        "model_mae.compile(loss=\"mean_absolute_error\", optimizer=\"adam\", run_eagerly=True)\n",
        "history_mae = model_mae.fit(train_data,\n",
        "                    train_data,\n",
        "                    batch_size=16,\n",
        "                    epochs = 1000,\n",
        "                    validation_data=(train_data_validation, train_data_validation),\n",
        "                    callbacks=[\n",
        "                        # checkpoint_mae,\n",
        "                        early_stopping_mae],\n",
        "                    verbose = 0,\n",
        "                    validation_batch_size = 16,\n",
        "                    shuffle=True)\n",
        "model_mae.summary()"
      ],
      "metadata": {
        "id": "Z4g9XkAMOxNy"
      },
      "id": "Z4g9XkAMOxNy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# checkpoint_ce = tf.keras.callbacks.ModelCheckpoint(\"checkpoint_ce\",\n",
        "#                                                    save_weights_only=True) #generate checkpoints\n",
        "early_stopping_ce = tf.keras.callbacks.EarlyStopping(patience=15,\n",
        "                                                     restore_best_weights=True) #define early stopping\n",
        "\n",
        "model_ce.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",  run_eagerly=True)\n",
        "\n",
        "history_ce = model_ce.fit(train_data,\n",
        "                    train_data,\n",
        "                    batch_size=16,\n",
        "                    epochs = 1000,\n",
        "                    validation_data=(train_data_validation, train_data_validation),\n",
        "                    callbacks=[\n",
        "                        # checkpoint_ce,\n",
        "                        early_stopping_ce],\n",
        "                    verbose = 0,\n",
        "                    validation_batch_size = 16,\n",
        "                    shuffle=True)\n",
        "model_ce.summary()"
      ],
      "metadata": {
        "id": "-7dHzwjCOxwn"
      },
      "id": "-7dHzwjCOxwn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Der Focal Loss wird mit γ = 2 initalisiert.\n",
        "\n",
        "[Quelle](https://github.com/qubvel/segmentation_models/blob/master/segmentation_models/losses.py)"
      ],
      "metadata": {
        "id": "j0zI_qD6bgCB"
      },
      "id": "j0zI_qD6bgCB"
    },
    {
      "cell_type": "code",
      "source": [
        "focal_loss = sm.losses.BinaryFocalLoss()\n",
        "# checkpoint_focal = tf.keras.callbacks.ModelCheckpoint(\"checkpoint_focal\",\n",
        "#                                                    save_weights_only=True) #generate checkpoints\n",
        "early_stopping_focal = tf.keras.callbacks.EarlyStopping(patience=15,\n",
        "                                                     restore_best_weights=True) #define early stopping\n",
        "\n",
        "model_focal.compile(loss=focal_loss, optimizer=\"adam\",  run_eagerly=True)\n",
        "\n",
        "\n",
        "history_focal = model_focal.fit(train_data,\n",
        "                                train_data,\n",
        "                                batch_size=16,\n",
        "                                epochs = 1000,\n",
        "                    validation_data=(train_data_validation, train_data_validation),\n",
        "                    callbacks=[\n",
        "                        # checkpoint_focal,\n",
        "                        early_stopping_focal],\n",
        "                    verbose = 0,\n",
        "                    validation_batch_size = 16,\n",
        "                    shuffle=True)\n",
        "model_focal.summary()"
      ],
      "metadata": {
        "id": "hxnX31oHOyML"
      },
      "id": "hxnX31oHOyML",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Huber wird mit δ = 0,5 initalisiert"
      ],
      "metadata": {
        "id": "JYwQPlllYwT5"
      },
      "id": "JYwQPlllYwT5"
    },
    {
      "cell_type": "code",
      "source": [
        "huber = tf.keras.losses.Huber(delta= 0.5, reduction=\"auto\", name=\"huber_loss\")\n",
        "# checkpoint_huber = tf.keras.callbacks.ModelCheckpoint(\"checkpoint_huber\",\n",
        "#                                                    save_weights_only=True) #generate checkpoints\n",
        "early_stopping_huber = tf.keras.callbacks.EarlyStopping(patience=15,\n",
        "                                                     restore_best_weights=True) #define early stopping\n",
        "\n",
        "model_huber.compile(loss=huber, optimizer=\"adam\",  run_eagerly=True) \n",
        "\n",
        "history_huber = model_huber.fit(train_data,\n",
        "                    train_data,\n",
        "                    batch_size=16,\n",
        "                    epochs = 1000,\n",
        "                    validation_data=(train_data_validation, train_data_validation),\n",
        "                    callbacks=[\n",
        "                        #checkpoint_huber,\n",
        "                        early_stopping_huber],\n",
        "                    verbose = 0,\n",
        "                    validation_batch_size = 16,\n",
        "                    shuffle=True)\n",
        "model_huber.summary()"
      ],
      "metadata": {
        "id": "Vq_hbVqLOyhb"
      },
      "id": "Vq_hbVqLOyhb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotte den Trainingsverlauf für jedes Modell"
      ],
      "metadata": {
        "id": "-bv_TOU8V-8E"
      },
      "id": "-bv_TOU8V-8E"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_performance():\n",
        "    '''\n",
        "    Plot the training performance of each model \n",
        "    '''\n",
        "    figure, axis = plt.subplots(ncols=1,nrows=5, figsize=(10, 20))\n",
        "    # first row: complicated architecture\n",
        "    axis[0].plot(history_mse.history[\"loss\"], label=\"Training Loss\")\n",
        "    axis[0].plot(history_mse.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "    axis[0].set_title('Performance Training with model_mse')\n",
        "    axis[0].set(xlabel='Epochs', ylabel='Loss')\n",
        "    axis[0].legend()\n",
        "    \n",
        "    axis[1].plot(history_mae.history[\"loss\"], label=\"Training Loss\")\n",
        "    axis[1].plot(history_mae.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "    axis[1].set_title('Performance Training with model_mae')\n",
        "    axis[1].set(xlabel='Epochs', ylabel='Loss')\n",
        "    axis[1].legend()\n",
        "    \n",
        "    axis[2].plot(history_huber.history[\"loss\"], label=\"Training Loss\")\n",
        "    axis[2].plot(history_huber.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "    axis[2].set_title('Performance Training with model_huber')\n",
        "    axis[2].set(xlabel='Epochs', ylabel='Loss')\n",
        "    axis[2].legend()\n",
        "    \n",
        "    #second row: simple architectures \n",
        "    axis[3].plot(history_ce.history[\"loss\"], label=\"Training Loss\")\n",
        "    axis[3].plot(history_ce.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "    axis[3].set_title('Performance Training by model_ce')\n",
        "    axis[3].set(xlabel='Epochs', ylabel='Loss')\n",
        "    axis[3].legend()\n",
        "    \n",
        "    axis[4].plot(history_focal.history[\"loss\"], label=\"Training Loss\")\n",
        "    axis[4].plot(history_focal.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "    axis[4].set_title('Performance Training by model_focal')\n",
        "    axis[4].set(xlabel='Epochs', ylabel='Loss')\n",
        "    axis[4].legend()\n",
        "\n",
        "    save_fig(\"Performances for training\")\n",
        "    plt.show()\n",
        "\n",
        "plot_training_performance()"
      ],
      "metadata": {
        "id": "uDAa0dQZ3eBt"
      },
      "id": "uDAa0dQZ3eBt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Diese Funktion extrahiert die letzte ausgeführte Epoche für ein Model"
      ],
      "metadata": {
        "id": "vuHx_Q-XWG__"
      },
      "id": "vuHx_Q-XWG__"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_last_epoch(history, max = \"max\") -> int:\n",
        "  epochs = np.array([])\n",
        "  stopped_epoch = np.array([])\n",
        "  for key, value in enumerate(history.history[\"loss\"]):\n",
        "    epochs = np.append(epochs,key)\n",
        "  stopped_epoch = np.append(stopped_epoch,int(np.max(epochs)))\n",
        "  if max == \"max\":\n",
        "    return int(np.max(epochs))\n",
        "  else:\n",
        "    return epochs\n"
      ],
      "metadata": {
        "id": "ORFJfp_z2g-_"
      },
      "id": "ORFJfp_z2g-_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for history in [\n",
        "history_mse,\n",
        "    history_mae,\n",
        "    history_ce,\n",
        "    history_focal,\n",
        "    history_huber ]:\n",
        "    print(get_last_epoch(history))"
      ],
      "metadata": {
        "id": "8E7sn_dipW0a"
      },
      "id": "8E7sn_dipW0a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotte den TNSE-Graph für den latenten Raum in einem Modell"
      ],
      "metadata": {
        "id": "N4DXnaMkWe86"
      },
      "id": "N4DXnaMkWe86"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_tnse_performance_bottleneck_layer():\n",
        "    '''\n",
        "    Plot the training performance of each model \n",
        "    '''\n",
        "    figure, axis = plt.subplots(ncols=1,nrows=5, figsize=(10, 20))\n",
        "    # first row: complicated architecture\n",
        "    encoder = model_mse.get_encoder()\n",
        "    vis_data_latent = encoder.predict(train_data)\n",
        "    tsne = TSNE(n_components=2)\n",
        "    tsne_data = tsne.fit_transform(vis_data_latent)\n",
        "    axis[0].scatter(tsne_data[train_labels == 0, 0], \n",
        "            tsne_data[train_labels == 0, 1], c=\"grey\", alpha=0.1, label=\"inlier\")\n",
        "    axis[0].scatter(tsne_data[train_labels == 1, 0], \n",
        "            tsne_data[train_labels == 1, 1], c=\"crimson\", alpha=1, label=\"outlier\")\n",
        "    axis[0].set_title('t-SNE for outliers/inliers on the latent manifold for model_mse')\n",
        "    axis[0].legend()\n",
        "    \n",
        "    encoder = model_mae.get_encoder()\n",
        "    vis_data_latent = encoder.predict(train_data)\n",
        "    tsne = TSNE(n_components=2)\n",
        "    tsne_data = tsne.fit_transform(vis_data_latent)\n",
        "    axis[1].scatter(tsne_data[train_labels == 0, 0], \n",
        "            tsne_data[train_labels == 0, 1], c=\"grey\", alpha=0.1, label=\"inlier\")\n",
        "    axis[1].scatter(tsne_data[train_labels == 1, 0], \n",
        "            tsne_data[train_labels == 1, 1], c=\"crimson\", alpha=1, label=\"outlier\")\n",
        "    axis[1].set_title('t-SNE for outliers/inliers on the latent manifold for model_mae')\n",
        "    axis[1].legend()\n",
        "    \n",
        "    encoder = model_huber.get_encoder()\n",
        "    vis_data_latent = encoder.predict(train_data)\n",
        "    tsne = TSNE(n_components=2)\n",
        "    tsne_data = tsne.fit_transform(vis_data_latent)\n",
        "    axis[2].scatter(tsne_data[train_labels == 0, 0], \n",
        "            tsne_data[train_labels == 0, 1], c=\"grey\", alpha=0.1, label=\"inlier\")\n",
        "    axis[2].scatter(tsne_data[train_labels == 1, 0], \n",
        "            tsne_data[train_labels == 1, 1], c=\"crimson\", alpha=1, label=\"outlier\")\n",
        "    axis[2].set_title('t-SNE for outliers/inliers on the latent manifold for model_huber')\n",
        "    axis[2].legend()\n",
        "    \n",
        "    encoder = model_ce.get_encoder()\n",
        "    vis_data_latent = encoder.predict(train_data)\n",
        "    tsne = TSNE(n_components=2)\n",
        "    tsne_data = tsne.fit_transform(vis_data_latent)\n",
        "    axis[3].scatter(tsne_data[train_labels == 0, 0], \n",
        "            tsne_data[train_labels == 0, 1], c=\"grey\", alpha=0.1, label=\"inlier\")\n",
        "    axis[3].scatter(tsne_data[train_labels == 1, 0], \n",
        "            tsne_data[train_labels == 1, 1], c=\"crimson\", alpha=1, label=\"outlier\")\n",
        "    axis[3].set_title('t-SNE for outliers/inliers on the latent manifold for model_ce')\n",
        "    axis[3].legend()\n",
        "    \n",
        "    encoder = model_focal.get_encoder()\n",
        "    vis_data_latent = encoder.predict(train_data)\n",
        "    tsne = TSNE(n_components=2)\n",
        "    tsne_data = tsne.fit_transform(vis_data_latent)\n",
        "    axis[4].scatter(tsne_data[train_labels == 0, 0], \n",
        "            tsne_data[train_labels == 0, 1], c=\"grey\", alpha=0.1, label=\"inlier\")\n",
        "    axis[4].scatter(tsne_data[train_labels == 1, 0], \n",
        "            tsne_data[train_labels == 1, 1], c=\"crimson\", alpha=1, label=\"outlier\")\n",
        "    axis[4].set_title('t-SNE for outliers/inliers on the latent manifold for model_focal')\n",
        "    axis[4].legend()\n",
        "\n",
        "    save_fig(\"TNSE-Distrubution in bottleneck-layer for all models\")\n",
        "    plt.show()\n",
        "\n",
        "plot_tnse_performance_bottleneck_layer()"
      ],
      "metadata": {
        "id": "5nHCyBoqZz_L"
      },
      "id": "5nHCyBoqZz_L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation on test data"
      ],
      "metadata": {
        "id": "84g4adTVfcmC"
      },
      "id": "84g4adTVfcmC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generiere den Rekonstruktionsfehler"
      ],
      "metadata": {
        "id": "0xn6JL8yjgU8"
      },
      "id": "0xn6JL8yjgU8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Um den Rekonstruktionsfehler zu berechnen, wird als Metrik der MSE verwendet."
      ],
      "metadata": {
        "id": "_izo1C8vjVCt"
      },
      "id": "_izo1C8vjVCt"
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_error_df(model, metric:str = \"mean_squared_error\", data = test_data):\n",
        "  if model in [model_mse,model_mae,model_ce,model_focal,model_huber]:\n",
        "    print(\"Calculate reconstruction error for model: \", model)\n",
        "    if metric == \"mean_squared_error\":\n",
        "      mse = np.mean(np.power(data - model.predict(data), 2), axis=1)\n",
        "      return(pd.DataFrame({'reconstruction_error': mse,\n",
        "                              'anomaly': test_labels}))\n",
        "    else:\n",
        "      print(\"No further metric functions are implemented yet\")\n",
        "      return\n",
        "  else:\n",
        "    raise(\"Choosen model is not supported yet. Please choose an model in [model_mse,model_mae,model_ce,model_focal,model_huber]\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L0PLrhpKbBXz"
      },
      "id": "L0PLrhpKbBXz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse_error_df = gen_error_df(model_mse)\n",
        "mae_error_df = gen_error_df(model_mae)\n",
        "ce_error_df = gen_error_df(model_ce)\n",
        "focal_error_df = gen_error_df(model_focal)\n",
        "huber_error_df = gen_error_df(model_huber)"
      ],
      "metadata": {
        "id": "Uq-KLMRCBXBF"
      },
      "id": "Uq-KLMRCBXBF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ROC und AUC"
      ],
      "metadata": {
        "id": "BkPb4nFSj6hZ"
      },
      "id": "BkPb4nFSj6hZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualisiere das Ergebnis für die Testdaten für alle Modelle"
      ],
      "metadata": {
        "id": "aBEolzL2kucr"
      },
      "id": "aBEolzL2kucr"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_test_performance_roc(error_df = [mse_error_df,\n",
        "                                          mae_error_df,\n",
        "                                          huber_error_df,\n",
        "                                          ce_error_df,\n",
        "                                          focal_error_df]):\n",
        "    '''\n",
        "    Plot the ROC curve performance of each model \n",
        "    '''\n",
        "    figure, axis = plt.subplots(ncols=1,nrows=5, figsize=(10, 20))\n",
        "    # first row: complicated architecture\n",
        "    fpr, tpr, thresholds = roc_curve(error_df[0].anomaly, error_df[0].reconstruction_error)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    axis[0].plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\n",
        "    axis[0].plot([0,1],[0,1],'r--')\n",
        "    axis[0].legend(loc='lower right')\n",
        "    axis[0].set_title('Receiver Operating Characteristic for Model_MSE')\n",
        "    axis[0].set_xlim([-0.001, 1])\n",
        "    axis[0].set_ylim([0, 1.001])\n",
        "    axis[0].set(xlabel='False Positive Rate', ylabel='True Positive Rate')\n",
        "    axis[0].legend()\n",
        "    \n",
        "    fpr, tpr, thresholds = roc_curve(error_df[1].anomaly, error_df[1].reconstruction_error)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    axis[1].plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\n",
        "    axis[1].plot([0,1],[0,1],'r--')\n",
        "    axis[1].legend(loc='lower right')\n",
        "    axis[1].set_title('Receiver Operating Characteristic for Model_MAE')\n",
        "    axis[1].set_xlim([-0.001, 1])\n",
        "    axis[1].set_ylim([0, 1.001])\n",
        "    axis[1].set(xlabel='False Positive Rate', ylabel='True Positive Rate')\n",
        "    axis[1].legend()\n",
        "    \n",
        "    fpr, tpr, thresholds = roc_curve(error_df[2].anomaly, error_df[2].reconstruction_error)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    axis[2].plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\n",
        "    axis[2].plot([0,1],[0,1],'r--')\n",
        "    axis[2].legend(loc='lower right')\n",
        "    axis[2].set_title('Receiver Operating Characteristic for Model_Huber')\n",
        "    axis[2].set_xlim([-0.001, 1])\n",
        "    axis[2].set_ylim([0, 1.001])\n",
        "    axis[2].set(xlabel='False Positive Rate', ylabel='True Positive Rate')\n",
        "    axis[2].legend()\n",
        "    \n",
        "    #second row: simple architectures \n",
        "    fpr, tpr, thresholds = roc_curve(error_df[3].anomaly, error_df[3].reconstruction_error)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    axis[3].plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\n",
        "    axis[3].plot([0,1],[0,1],'r--')\n",
        "    axis[3].legend(loc='lower right')\n",
        "    axis[3].set_title('Receiver Operating Characteristic for Model_CE')\n",
        "    axis[3].set_xlim([-0.001, 1])\n",
        "    axis[3].set_ylim([0, 1.001])\n",
        "    axis[3].set(xlabel='False Positive Rate', ylabel='True Positive Rate')\n",
        "    axis[3].legend()\n",
        "    \n",
        "    fpr, tpr, thresholds = roc_curve(error_df[4].anomaly, error_df[4].reconstruction_error)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    axis[4].plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\n",
        "    axis[4].plot([0,1],[0,1],'r--')\n",
        "    axis[4].legend(loc='lower right')\n",
        "    axis[4].set_title('Receiver Operating Characteristic for Model_Focal')\n",
        "    axis[4].set_xlim([-0.001, 1])\n",
        "    axis[4].set_ylim([0, 1.001])\n",
        "    axis[4].set(xlabel='False Positive Rate', ylabel='True Positive Rate')\n",
        "    axis[4].legend()\n",
        "\n",
        "\n",
        "    save_fig(\"Roc-Curves for test data\")\n",
        "    plt.show()\n",
        "\n",
        "plot_test_performance_roc()"
      ],
      "metadata": {
        "id": "dbo0o_rvwZ-T"
      },
      "id": "dbo0o_rvwZ-T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Precision, Recall und F1-Score\n",
        "\n",
        "Da diese Metriken davon abhängig sind, welche Threshold man für die Klassifikation wählt, wird die Threshold wie folgt gewählt, um den F1-Score zu maximieren:\n",
        "\n",
        "\n",
        "*   Bestimme die Threshold, für die die Precision und der Recall gleich sind\n",
        "*   Da es sich bei dem F1-Score um ein harmonisches Mittel handelt, wird so der F1-Score maximiert\n",
        "\n"
      ],
      "metadata": {
        "id": "fda09jLOkFj3"
      },
      "id": "fda09jLOkFj3"
    },
    {
      "cell_type": "code",
      "source": [
        "prec, recall, thresholds = precision_recall_curve(focal_error_df[\"anomaly\"], focal_error_df[\"reconstruction_error\"])\n",
        "\n",
        "best_idx = np.argmin(np.abs(prec-recall)[:int(len(np.abs(prec-recall))*-0.1)]) #to resolve an bug, the last four obsvervations have to be excluded\n",
        "best_prec, best_recall = prec[best_idx], recall[best_idx]\n",
        "print(f\"Best precision {np.round(best_prec, 2)}, \"\\\n",
        "      f\"recall: {np.round(best_recall, 2)} at {np.round(thresholds[best_idx], 2)} threshold.\")"
      ],
      "metadata": {
        "id": "Qncty3ipkCyc"
      },
      "id": "Qncty3ipkCyc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "auc_score = auc(recall, prec)\n",
        "auc_score"
      ],
      "metadata": {
        "id": "X18MiBKgtuoj"
      },
      "id": "X18MiBKgtuoj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualisiere das Ergebnis für alle Ergebnisse"
      ],
      "metadata": {
        "id": "cHwMS5yjkqVZ"
      },
      "id": "cHwMS5yjkqVZ"
    },
    {
      "cell_type": "code",
      "source": [
        "## Recall Vs Precision\n",
        "def plot_test_performance_recall_vs_precision(error_df = [mse_error_df,\n",
        "                                          mae_error_df,\n",
        "                                          huber_error_df,\n",
        "                                          ce_error_df,\n",
        "                                          focal_error_df]):\n",
        "    figure, axis = plt.subplots(ncols=2,nrows=5, figsize=(30, 30))\n",
        "    # first row: complicated architecture\n",
        "    precision, recall, thresholds = precision_recall_curve(error_df[0].anomaly, error_df[0].reconstruction_error)\n",
        "    best_idx = np.argmin(np.abs(precision-recall)[:-4])\n",
        "    best_prec, best_recall = precision[best_idx], recall[best_idx]\n",
        "    precision_recall_auc = auc(recall, precision)\n",
        "    axis[0,0].plot(recall, precision, label='AUC = %0.4f'% precision_recall_auc)\n",
        "    axis[0,0].legend(loc='lower right')\n",
        "    axis[0,0].set_title('Recall vs Precision for Model_MSE')\n",
        "    axis[0,0].set(xlabel='Recall', ylabel='Precision')\n",
        "    axis[0,1].set_title('Determine the threshold for Model_MSE')\n",
        "    axis[0,1].plot(thresholds, precision[:-1], label=\"precision\")\n",
        "    axis[0,1].plot(thresholds, recall[:-1], label=\"recall\")\n",
        "    axis[0,1].axvline(thresholds[best_idx], 0, 1, c=\"grey\", linestyle=\"--\")\n",
        "    axis[0,1].set_xlabel(\"thresholds\")\n",
        "    axis[0,1].legend()\n",
        "\n",
        "    precision, recall, thresholds = precision_recall_curve(error_df[1].anomaly, error_df[1].reconstruction_error)\n",
        "    best_idx = np.argmin(np.abs(precision-recall)[:-4])\n",
        "    best_prec, best_recall = precision[best_idx], recall[best_idx]\n",
        "    precision_recall_auc = auc(recall, precision)\n",
        "    axis[1,0].plot(recall, precision, label='AUC = %0.4f'% precision_recall_auc)\n",
        "    axis[1,0].legend(loc='lower right')\n",
        "    axis[1,0].set_title('Recall vs Precision for Model_MAE')\n",
        "    axis[1,0].set(xlabel='Recall', ylabel='Precision')\n",
        "    axis[1,1].set_title('Determine the threshold for Model_MAE')\n",
        "    axis[1,1].plot(thresholds, precision[:-1], label=\"precision\")\n",
        "    axis[1,1].plot(thresholds, recall[:-1], label=\"recall\")\n",
        "    axis[1,1].axvline(thresholds[best_idx], 0, 1, c=\"grey\", linestyle=\"--\")\n",
        "    axis[1,1].set_xlabel(\"thresholds\")\n",
        "    axis[1,1].legend()\n",
        "\n",
        "    precision, recall, thresholds = precision_recall_curve(error_df[2].anomaly, error_df[2].reconstruction_error)\n",
        "    best_idx = np.argmin(np.abs(precision-recall)[:-4])\n",
        "    best_prec, best_recall = precision[best_idx], recall[best_idx]\n",
        "    precision_recall_auc = auc(recall, precision)\n",
        "    axis[2,0].plot(recall, precision, label='AUC = %0.4f'% precision_recall_auc)\n",
        "    axis[2,0].legend(loc='lower right')\n",
        "    axis[2,0].set_title('Recall vs Precision for Model_Huber')\n",
        "    axis[2,0].set(xlabel='Recall', ylabel='Precision')\n",
        "    axis[2,1].set_title('Determine the threshold for Model_Huber')\n",
        "    axis[2,1].plot(thresholds, precision[:-1], label=\"precision\")\n",
        "    axis[2,1].plot(thresholds, recall[:-1], label=\"recall\")\n",
        "    axis[2,1].axvline(thresholds[best_idx], 0, 1, c=\"grey\", linestyle=\"--\")\n",
        "    axis[2,1].set_xlabel(\"thresholds\")\n",
        "    axis[2,1].legend()\n",
        "\n",
        "    precision, recall, thresholds = precision_recall_curve(error_df[3].anomaly, error_df[3].reconstruction_error)\n",
        "    best_idx = np.argmin(np.abs(precision-recall)[:-4])\n",
        "    best_prec, best_recall = precision[best_idx], recall[best_idx]\n",
        "    precision_recall_auc = auc(recall, precision)\n",
        "    axis[3,0].plot(recall, precision, label='AUC = %0.4f'% precision_recall_auc)\n",
        "    axis[3,0].legend(loc='lower right')\n",
        "    axis[3,0].set_title('Recall vs Precision for Model_CE')\n",
        "    axis[3,0].set(xlabel='Recall', ylabel='Precision')\n",
        "    axis[3,1].set_title('Determine the threshold for Model_CE')\n",
        "    axis[3,1].plot(thresholds, precision[:-1], label=\"precision\")\n",
        "    axis[3,1].plot(thresholds, recall[:-1], label=\"recall\")\n",
        "    axis[3,1].axvline(thresholds[best_idx], 0, 1, c=\"grey\", linestyle=\"--\")\n",
        "    axis[3,1].set_xlabel(\"thresholds\")\n",
        "    axis[3,1].legend()\n",
        "\n",
        "    precision, recall, thresholds = precision_recall_curve(error_df[4].anomaly, error_df[4].reconstruction_error)\n",
        "    best_idx = np.argmin(np.abs(precision-recall)[:-4])\n",
        "    best_prec, best_recall = precision[best_idx], recall[best_idx]\n",
        "    precision_recall_auc = auc(recall, precision)\n",
        "    axis[4,0].plot(recall, precision, label='AUC = %0.4f'% precision_recall_auc)\n",
        "    axis[4,0].legend(loc='lower right')\n",
        "    axis[4,0].set_title('Recall vs Precision for Model_FL')\n",
        "    axis[4,0].set(xlabel='Recall', ylabel='Precision')\n",
        "    axis[4,1].set_title('Determine the threshold for Model_Focal')\n",
        "    axis[4,1].plot(thresholds, precision[:-1], label=\"precision\")\n",
        "    axis[4,1].plot(thresholds, recall[:-1], label=\"recall\")\n",
        "    axis[4,1].axvline(thresholds[best_idx], 0, 1, c=\"grey\", linestyle=\"--\")\n",
        "    axis[4,1].set_xlabel(\"thresholds\")\n",
        "    axis[4,1].legend()\n",
        "\n",
        "    save_fig(\"Recall vs Precision curves for test data\")\n",
        "    sns.despine()\n",
        "    plt.show()\n",
        "\n",
        "plot_test_performance_recall_vs_precision()"
      ],
      "metadata": {
        "id": "fUY3nZZYg3ty"
      },
      "id": "fUY3nZZYg3ty",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_precision_threshold(error_df):\n",
        "  '''\n",
        "  Return an array, which includes the thresholds with the second highest precision score for each model\n",
        "  '''\n",
        "  prec, recall, threshold = precision_recall_curve(error_df[\"anomaly\"], error_df[\"reconstruction_error\"])\n",
        "  best_idx = np.argmin(np.abs(prec-recall)[:-4])\n",
        "  best_prec, best_recall = prec[best_idx], recall[best_idx]\n",
        "\n",
        "  return threshold[best_idx]\n",
        "\n",
        "for i in [mse_error_df, mae_error_df, huber_error_df, ce_error_df, focal_error_df]:   \n",
        "  print(get_precision_threshold(i))"
      ],
      "metadata": {
        "id": "pQXAUcPkbAd-"
      },
      "id": "pQXAUcPkbAd-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_thresholds_data(index:int,save_name:str,error_df = [mse_error_df,\n",
        "                                          mae_error_df,\n",
        "                                          huber_error_df,\n",
        "                                          ce_error_df,\n",
        "                                          focal_error_df]):\n",
        "  if index > len(error_df) - 1 or index < 0:\n",
        "    raise(\"Index out of range implemented. Please correct your statement\")\n",
        "  else:\n",
        "    groups = error_df[index].groupby('anomaly')\n",
        "    threshold = get_precision_threshold(error_df[index])\n",
        "    fig, ax = plt.subplots()\n",
        "    for name, group in groups:\n",
        "      ax.plot(group.index, group.reconstruction_error, marker='o', ms=3.5, linestyle='',\n",
        "              label= \"Anomaly\" if name == 1 else \"Normal\")\n",
        "    ax.hlines(threshold, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", zorder=100, label='Threshold')\n",
        "    ax.legend()\n",
        "    plt.title(\"Reconstruction error visualisation to identify anomalies\")\n",
        "    plt.ylabel(\"Reconstruction error\")\n",
        "    plt.xlabel(\"Data point index\")\n",
        "    save_fig(save_name)\n",
        "    plt.show();\n"
      ],
      "metadata": {
        "id": "7irw2FnvhxlO"
      },
      "id": "7irw2FnvhxlO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_thresholds_data(0, \"Reconstruction error visualisation to identify anomalies with model_mse\")\n",
        "plot_thresholds_data(1, \"Reconstruction error visualisation to identify anomalies with model_mae\")\n",
        "plot_thresholds_data(2, \"Reconstruction error visualisation to identify anomalies with model_huber\")\n",
        "plot_thresholds_data(3, \"Reconstruction error visualisation to identify anomalies with model_ce\")\n",
        "plot_thresholds_data(4, \"Reconstruction error visualisation to identify anomalies with model_focal\")"
      ],
      "metadata": {
        "id": "fNyor5Gpl986"
      },
      "id": "fNyor5Gpl986",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Predict class based on error threshold\n",
        "\n",
        "# Weitermachen\n",
        "def plot_cm_matrix(index:int,save_name:str,error_df = [mse_error_df,\n",
        "                                          mae_error_df,\n",
        "                                          huber_error_df,\n",
        "                                          ce_error_df,\n",
        "                                          focal_error_df],\n",
        "                   LABELS = [\"Non Fraud\", \"Fraud\"]):\n",
        "  if index > len(error_df) - 1 or index < 0:\n",
        "    raise(\"Index out of range implemented. Please correct your statement\")\n",
        "  else:\n",
        "    error_df = error_df[index]\n",
        "    threshold = get_precision_threshold(error_df)\n",
        "    y_pred = [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\n",
        "    conf_matrix = confusion_matrix(error_df.anomaly, y_pred,labels=[0,1])\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    sns.heatmap(conf_matrix,xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\",cmap='Blues');\n",
        "    plt.title(\"Confusion matrix\")\n",
        "    plt.ylabel('True class')\n",
        "    plt.xlabel('Predicted class')\n",
        "    save_fig(save_name)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "5TALJXevjfyZ"
      },
      "id": "5TALJXevjfyZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cm_matrix(0, \"CM-matrix visualisation to identify anomalies with model_mse\")\n",
        "plot_cm_matrix(1, \"CM-matrix visualisation to identify anomalies with model_mae\")\n",
        "plot_cm_matrix(2, \"CM-matrix visualisation to identify anomalies with model_huber\")\n",
        "plot_cm_matrix(3, \"CM-matrix visualisation to identify anomalies with model_ce\")\n",
        "plot_cm_matrix(4, \"CM-matrix visualisation to identify anomalies with model_focal\")"
      ],
      "metadata": {
        "id": "pEr2o-V2mij-"
      },
      "id": "pEr2o-V2mij-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_f1_scores(index, error_df =  \n",
        "                        [mse_error_df,\n",
        "                         mae_error_df,\n",
        "                         huber_error_df,\n",
        "                         ce_error_df,\n",
        "                         focal_error_df]\n",
        "                        ):\n",
        "  if index > len(error_df) - 1 or index < 0:\n",
        "    raise(\"Index out of range implemented. Please correct your statement\")\n",
        "  error_df = error_df[index]\n",
        "  threshold = get_precision_threshold(error_df)\n",
        "  y_pred =  [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\n",
        "  print('F1_Score with threshold:', threshold)\n",
        "  return(f1_score(error_df.anomaly, y_pred))\n",
        "calculate_f1_scores(0)"
      ],
      "metadata": {
        "id": "z6SVA051j1ei"
      },
      "id": "z6SVA051j1ei",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "error_df = [mse_error_df,\n",
        "             mae_error_df,\n",
        "             huber_error_df,\n",
        "             ce_error_df,\n",
        "             focal_error_df]\n",
        "for i in error_df:\n",
        "  threshold = get_precision_threshold(i) #specify here the index \n",
        "  y_pred =  [1 if e > threshold else 0 for e in i.reconstruction_error.values]\n",
        "\n",
        "  cm1 = confusion_matrix(i.anomaly, y_pred,labels=[1,0])\n",
        "  print('Confusion Matrix Val: \\n', cm1)\n",
        "\n",
        "\n",
        "  total1=sum(sum(cm1))\n",
        "  #####from confusion matrix calculate accuracy\n",
        "\n",
        "  accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
        "  print ('Accuracy Val: ', accuracy1)\n",
        "\n",
        "\n",
        "  sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
        "  print('Sensitivity Val: ', sensitivity1 )\n",
        "\n",
        "\n",
        "  specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
        "  print('Specificity Val: ', specificity1)\n",
        "\n",
        "  KappaValue=cohen_kappa_score(i.anomaly, y_pred)\n",
        "  print(\"Kappa Value :\",KappaValue)\n",
        "  AUC=roc_auc_score(i.anomaly, y_pred)\n",
        "\n",
        "  print(\"AUC         :\",AUC)\n",
        "\n",
        "  print(\"F1-Score Val  : \",f1_score(i.anomaly, y_pred))\n",
        "\n",
        "  print(\"_______________________________________\")"
      ],
      "metadata": {
        "id": "Z_rMWW0NcR0r"
      },
      "id": "Z_rMWW0NcR0r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simulation"
      ],
      "metadata": {
        "id": "4tHAq-c3oQn6"
      },
      "id": "4tHAq-c3oQn6"
    },
    {
      "cell_type": "code",
      "source": [
        "def simulation(\n",
        "    loss,\n",
        "    model,\n",
        "    patience,\n",
        "    epochs,\n",
        "    n:int = 0,\n",
        "    end:int = 30,\n",
        "    optimizer=\"adam\",\n",
        "    test_size = 0.2,\n",
        "    batch_size=16,\n",
        "    validation_batch_size = 16,\n",
        "    delta = 0.5\n",
        "    ):\n",
        "  \n",
        "  stopped_epochs = []\n",
        "  auc_values_roc = []\n",
        "  auc_values_recall_prec = []\n",
        "  f1_values = []\n",
        "\n",
        "  if loss == \"focal_loss\":\n",
        "    loss = sm.losses.BinaryFocalLoss()\n",
        "    print(\"Start the Simulation:\")\n",
        "    while(True):\n",
        "      print(\"Lossfunction: \", loss),\n",
        "      print(\"Iteration: \",n)\n",
        "      #Step 1: Splitt data\n",
        "      train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "        data,\n",
        "        labels_bool,\n",
        "        test_size=test_size\n",
        "        )\n",
        "      train_data_no_validation, train_data_validation, train_data_no_validation_labels, train_data_validation_labels = train_test_split(\n",
        "        train_data,\n",
        "        train_labels,\n",
        "        test_size = test_size\n",
        "        )\n",
        "      \n",
        "      #Step2: Train data\n",
        "      model = model\n",
        "      early_stopping = tf.keras.callbacks.EarlyStopping(patience=patience,\n",
        "                                                      restore_best_weights=True) #generate callback\n",
        "      model.compile(loss=loss, optimizer=optimizer, run_eagerly=True)\n",
        "      history = model.fit(train_data,\n",
        "                      train_data,\n",
        "                      batch_size=batch_size,\n",
        "                      epochs = epochs,\n",
        "                      validation_data=(train_data_validation, train_data_validation),\n",
        "                      callbacks=[\n",
        "                          early_stopping,\n",
        "                          ],\n",
        "                      verbose = 0,\n",
        "                      validation_batch_size = validation_batch_size,\n",
        "                      shuffle=True)\n",
        "      stopped_epochs.append(get_last_epoch(history)) #append stopped epoch \n",
        "\n",
        "      #Step3: Evaluate test data with AUC\n",
        "      error_df = gen_error_df(model)\n",
        "      fpr, tpr, thresholds = roc_curve(error_df.anomaly, error_df.reconstruction_error)\n",
        "      auc_values_roc.append(auc(fpr, tpr))\n",
        "\n",
        "      #Step4: Evaluate test data with AUC\n",
        "      prec, recall, thresholds = precision_recall_curve(error_df[\"anomaly\"], error_df[\"reconstruction_error\"])\n",
        "      auc_values_recall_prec.append(auc(recall, prec))\n",
        "      best_idx = np.argmin(np.abs(prec-recall)[:-4])\n",
        "      best_prec, best_recall = prec[best_idx], recall[best_idx]\n",
        "      threshold = thresholds[best_idx]\n",
        "      y_pred =  [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\n",
        "      f1_values.append(f1_score(error_df.anomaly, y_pred))\n",
        "\n",
        "      if (n >= end):\n",
        "        print(\"Final Output:\")\n",
        "        print(\"Return stopped epochs per iteration, \")\n",
        "        print(\"Return auc values per iteration, \")\n",
        "        print(\"Return precision scores with best thresholds per iteration, \")\n",
        "        print(\"Return recall scores with best thresholds per iteration, \")\n",
        "        print(\"Return f1 scores with best thresholds per iteration, \")\n",
        "        return(stopped_epochs,auc_values_roc,auc_values_recall_prec,f1_values)\n",
        "        break\n",
        "      n+=1\n",
        "  \n",
        "  elif loss == \"huber_loss\":\n",
        "    loss = tf.keras.losses.Huber(delta=delta, reduction=\"auto\", name=\"huber_loss\")\n",
        "    print(\"Start the Simulation:\")\n",
        "    while(True):\n",
        "      print(\"Lossfunction: \", loss),\n",
        "      print(\"Iteration: \",n)\n",
        "      #Step 1: Splitt data\n",
        "      train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "        data,\n",
        "        labels_bool,\n",
        "        test_size=test_size\n",
        "        )\n",
        "      train_data_no_validation, train_data_validation, train_data_no_validation_labels, train_data_validation_labels = train_test_split(\n",
        "        train_data,\n",
        "        train_labels,\n",
        "        test_size = test_size\n",
        "        )\n",
        "      \n",
        "      #Step2: Train data\n",
        "      model = model\n",
        "      early_stopping = tf.keras.callbacks.EarlyStopping(patience=patience,\n",
        "                                                      restore_best_weights=True) #generate callback\n",
        "      model.compile(loss=loss, optimizer=optimizer, run_eagerly=True)\n",
        "      history = model.fit(train_data,\n",
        "                      train_data,\n",
        "                      batch_size=batch_size,\n",
        "                      epochs = epochs,\n",
        "                      validation_data=(train_data_validation, train_data_validation),\n",
        "                      callbacks=[\n",
        "                          early_stopping,\n",
        "                          ],\n",
        "                      verbose = 0,\n",
        "                      validation_batch_size = validation_batch_size,\n",
        "                      shuffle=True)\n",
        "      stopped_epochs.append(get_last_epoch(history)) #append stopped epoch \n",
        "\n",
        "      #Step3: Evaluate test data with AUC\n",
        "      error_df = gen_error_df(model)\n",
        "      fpr, tpr, thresholds = roc_curve(error_df.anomaly, error_df.reconstruction_error)\n",
        "      auc_values_roc.append(auc(fpr, tpr))\n",
        "\n",
        "      #Step4: Evaluate test data with AUC\n",
        "      prec, recall, thresholds = precision_recall_curve(error_df[\"anomaly\"], error_df[\"reconstruction_error\"])\n",
        "      auc_values_recall_prec.append(auc(recall, prec))\n",
        "      best_idx = np.argmin(np.abs(prec-recall)[:-4])\n",
        "      best_prec, best_recall = prec[best_idx], recall[best_idx]\n",
        "      threshold = thresholds[best_idx]\n",
        "      y_pred =  [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\n",
        "      f1_values.append(f1_score(error_df.anomaly, y_pred))\n",
        "\n",
        "      if (n >= end):\n",
        "        print(\"Final Output:\")\n",
        "        print(\"Return stopped epochs per iteration, \")\n",
        "        print(\"Return auc values per iteration, \")\n",
        "        print(\"Return precision scores with best thresholds per iteration, \")\n",
        "        print(\"Return recall scores with best thresholds per iteration, \")\n",
        "        print(\"Return f1 scores with best thresholds per iteration, \")\n",
        "        return(stopped_epochs,auc_values_roc,auc_values_recall_prec,f1_values)\n",
        "        break\n",
        "      n+=1\n",
        "      \n",
        "  else:\n",
        "    print(\"Start the Simulation:\")\n",
        "    while(True):\n",
        "      print(\"Lossfunction: \", loss),\n",
        "      print(\"Iteration: \",n)\n",
        "      #Step 1: Splitt data\n",
        "      train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "        data,\n",
        "        labels_bool,\n",
        "        test_size=test_size\n",
        "        )\n",
        "      train_data_no_validation, train_data_validation, train_data_no_validation_labels, train_data_validation_labels = train_test_split(\n",
        "        train_data,\n",
        "        train_labels,\n",
        "        test_size = test_size\n",
        "        )\n",
        "      #Step2: Train data\n",
        "      model = model\n",
        "      early_stopping = tf.keras.callbacks.EarlyStopping(patience=patience,\n",
        "                                                      restore_best_weights=True) #generate callback\n",
        "      model.compile(loss=loss, optimizer=optimizer, run_eagerly=True)\n",
        "      history = model.fit(train_data,\n",
        "                      train_data,\n",
        "                      batch_size=batch_size,\n",
        "                      epochs = epochs,\n",
        "                      validation_data=(train_data_validation, train_data_validation),\n",
        "                      callbacks=[\n",
        "                          early_stopping,\n",
        "                          ],\n",
        "                      verbose = 0,\n",
        "                      validation_batch_size = validation_batch_size,\n",
        "                      shuffle=True)\n",
        "      stopped_epochs.append(get_last_epoch(history)) #append stopped epoch\n",
        "      \n",
        "      #Step3: Evaluate test data with AUC\n",
        "      error_df = gen_error_df(model)\n",
        "      fpr, tpr, thresholds = roc_curve(error_df.anomaly, error_df.reconstruction_error)\n",
        "      auc_values_roc.append(auc(fpr, tpr))\n",
        "\n",
        "      #Step4: Evaluate test data with AUC\n",
        "      prec, recall, thresholds = precision_recall_curve(error_df[\"anomaly\"], error_df[\"reconstruction_error\"])\n",
        "      auc_values_recall_prec.append(auc(recall, prec))\n",
        "      best_idx = np.argmin(np.abs(prec-recall)[:-4])\n",
        "      best_prec, best_recall = prec[best_idx], recall[best_idx]\n",
        "      threshold = thresholds[best_idx]\n",
        "      y_pred =  [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\n",
        "      f1_values.append(f1_score(error_df.anomaly, y_pred))\n",
        "\n",
        "      if (n >= end):\n",
        "        print(\"Final Output:\")\n",
        "        print(\"Return stopped epochs per iteration, \")\n",
        "        print(\"Return auc values per iteration, \")\n",
        "        print(\"Return precision scores with best thresholds per iteration, \")\n",
        "        print(\"Return recall scores with best thresholds per iteration, \")\n",
        "        print(\"Return f1 scores with best thresholds per iteration, \")\n",
        "        return(stopped_epochs,auc_values_roc,auc_values_recall_prec,f1_values)\n",
        "        break\n",
        "      n+=1"
      ],
      "metadata": {
        "id": "nxHH6UsB2uzQ"
      },
      "id": "nxHH6UsB2uzQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simulation_mse = simulation(model = model_mse,\n",
        "                            end = 30,\n",
        "                            patience = 10, #EaryStopping after 10 epochs,\n",
        "                            epochs = 1000,\n",
        "                            loss = \"mean_squared_error\")\n",
        "with open(\"simulation_mse_thyroid_.txt\", \"w\") as f:\n",
        "   # Convert the list to a string and write it to the file\n",
        "    f.write(str(simulation_mse))"
      ],
      "metadata": {
        "id": "fzwsv1odlPh6"
      },
      "id": "fzwsv1odlPh6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simulation_mae = simulation(model = model_mae,\n",
        "                            end = 30,\n",
        "                            patience = 10, #EaryStopping after 10 epochs,\n",
        "                            epochs = 1000,\n",
        "                            loss = \"mean_absolute_error\")\n",
        "with open(\"simulation_mae_thyroid_.txt\", \"w\") as f:\n",
        "   # Convert the list to a string and write it to the file\n",
        "    f.write(str(simulation_mae))"
      ],
      "metadata": {
        "id": "IALzuD5m0Mr0"
      },
      "id": "IALzuD5m0Mr0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simulation_huber = simulation(model = model_huber,\n",
        "                            end = 30,\n",
        "                            patience = 10, #EaryStopping after 10 epochs,\n",
        "                            epochs = 1000,\n",
        "                            loss = \"huber_loss\",\n",
        "                            delta = 0.5)\n",
        "with open(\"simulation_huber_thyroid_.txt\", \"w\") as f:\n",
        "   # Convert the list to a string and write it to the file\n",
        "    f.write(str(simulation_huber))"
      ],
      "metadata": {
        "id": "KjnbTKOR0QI7"
      },
      "id": "KjnbTKOR0QI7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simulation_ce= simulation(model = model_ce,\n",
        "                          end = 30,\n",
        "                          patience = 10, #EaryStopping after 10 epochs,\n",
        "                          epochs = 1000,\n",
        "                          loss = \"binary_crossentropy\")\n",
        "with open(\"simulation_ce_thyroid_.txt\", \"w\") as f:\n",
        "   # Convert the list to a string and write it to the file\n",
        "    f.write(str(simulation_ce))"
      ],
      "metadata": {
        "id": "dffk5V8g0Vd9"
      },
      "id": "dffk5V8g0Vd9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simulation_focal =simulation(model = model_focal,\n",
        "                             end = 30,\n",
        "                             patience = 10, #EaryStopping after 10 epochs,\n",
        "                             epochs = 1000,\n",
        "                             loss =\"focal_loss\")\n",
        "with open(\"simulation_focal_thyroid_.txt\", \"w\") as f:\n",
        "   # Convert the list to a string and write it to the file\n",
        "    f.write(str(simulation_focal))"
      ],
      "metadata": {
        "id": "r3ipEXzI0axQ"
      },
      "id": "r3ipEXzI0axQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in [simulation_mse, simulation_mae, simulation_huber, simulation_ce, simulation_focal]:\n",
        "  print(\"_______________________________________________________\")\n",
        "  print(\"Mean and standard deviation of last epochs\")\n",
        "  print(np.asarray(i[0]).mean())\n",
        "  print(np.asarray(i[0]).std())\n",
        "  print(\"Mean and standard deviation of ROC-AUC\")\n",
        "  print(np.asarray(i[1]).mean())\n",
        "  print(np.asarray(i[1]).std())\n",
        "  print(\"Mean and standard deviation of Recall-Precision-AUC\")\n",
        "  print(np.asarray(i[2]).mean())\n",
        "  print(np.asarray(i[2]).std())\n",
        "\n"
      ],
      "metadata": {
        "id": "u7R3LdnWO7bi"
      },
      "id": "u7R3LdnWO7bi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistische Evaluation\n",
        "\n",
        "Die Ergebnisse aus der Simulation wurden in Textfiles gespeichert. Um die Daten zu laden, wurden diese manuell hier herein kopiert. \n",
        "\n",
        "Quelle:\n",
        "\n",
        "\n",
        "*   simulation_mse_thyroid_.txt\n",
        "\n",
        "*   simulation_mae_thyroid_.txt\n",
        "\n",
        "*   simulation_huber_thyroid_.txt\n",
        "\n",
        "*   simulation_ce_thyroid_.txt\n",
        "\n",
        "*   simulation_focal_thyroid_.txt\n",
        "\n",
        "Um die Resultate der Ausarbeitung zu sehen, den folgenden Block auskommentieren. Ansonsten kann eine neue Simulation verwendet werden (GPU-Unterstützung sollte dafür aktiv sein).\n"
      ],
      "metadata": {
        "id": "AS5tHFlVoXKV"
      },
      "id": "AS5tHFlVoXKV"
    },
    {
      "cell_type": "code",
      "source": [
        "# simulation_mse = [[20, 57, 59, 22, 34, 76, 23, 21, 40, 25, 21, 33, 291, 15, 23, 12, 23, 17, 17, 24, 29, 12, 20, 14, 23, 10, 18, 14, 13, 24, 17], [0.9584652495100257, 0.9482134780642245, 0.9550731192522237, 0.9504748982360922, 0.9504748982360922, 0.9491180461329716, 0.9428614503241368, 0.9481380973918287, 0.9408261721694559, 0.9419568822553898, 0.9388662746871702, 0.9421830242725765, 0.9386401326699835, 0.935549525101764, 0.9299713553444897, 0.9338157696366651, 0.9249208502939845, 0.9293683099653249, 0.9353233830845772, 0.9308005427408412, 0.9289160259309514, 0.9412030755314338, 0.9363787124981154, 0.9326096788783357, 0.9240916628976331, 0.9402985074626866, 0.9314789687924017, 0.9338911503090609, 0.9329865822403136, 0.9292929292929293, 0.9338911503090608], [0.6803622816065465, 0.663316492547374, 0.6888730415670026, 0.6814040351241367, 0.6834342236647633, 0.6826546619267884, 0.6761305895668994, 0.678256333960623, 0.6630007678591426, 0.6629701336462936, 0.6703576011098198, 0.67949454868509, 0.6458677966603843, 0.6410032608543998, 0.6357026214930742, 0.6334190853697573, 0.6293122570155172, 0.6517539583341299, 0.6507019447303317, 0.6291399458411626, 0.6335503762586893, 0.6452104248009196, 0.6395622819907396, 0.637447461518958, 0.6213624373832813, 0.6396836491549408, 0.634711612833963, 0.6410524633749393, 0.6331438351016214, 0.6215217463335303, 0.6258085898635214], [0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.5714285714285715, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.5714285714285715, 0.5714285714285715]]\n",
        "# simulation_mae = [[89, 21, 36, 10, 20, 45, 31, 27, 92, 68, 16, 20, 11, 18, 19, 31, 22, 18, 19, 32, 13, 48, 12, 10, 23, 18, 23, 19, 21, 40, 34], [0.9599728629579376, 0.9604251469923112, 0.9592190562339816, 0.9604251469923112, 0.9605005276647068, 0.9624604251469924, 0.9601990049751243, 0.959897482285542, 0.9497210915121362, 0.9503995175636968, 0.9469320066334991, 0.9454997738579829, 0.9457259158751696, 0.9448967284788181, 0.9458766772199608, 0.9426353083069501, 0.9443690637720489, 0.9459520578923564, 0.9444444444444445, 0.9431629730137193, 0.9454243931855871, 0.9422584049449721, 0.9397708427559174, 0.94451982511684, 0.9411276948590382, 0.9412030755314337, 0.942107643600181, 0.9442183024272577, 0.9364540931705111, 0.9412784562038293, 0.9401477461178953], [0.6678745215784142, 0.6719428380727064, 0.6681782331031876, 0.6703828137591066, 0.6722304268834298, 0.6879151427645401, 0.6850718540525206, 0.6849163848307965, 0.6736557024591323, 0.6843127534757543, 0.679085122721295, 0.6689502025007594, 0.6799215329765188, 0.6783629452337405, 0.6799845470065657, 0.677004407429214, 0.6803941245288192, 0.6815787650831574, 0.6800045064248209, 0.6741273365803461, 0.6817956085614879, 0.675843948511906, 0.6636297071028537, 0.6717767219145767, 0.6754487778463963, 0.6669341759647384, 0.6639329172443394, 0.6780901811049225, 0.6575941582524741, 0.6751936815370715, 0.6625297726519677], [0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287, 0.6285714285714287]]\n",
        "# simulation_huber = [[85, 15, 20, 12, 14, 16, 13, 17, 13, 18, 12, 21, 10, 12, 11, 25, 35, 10, 14, 15, 18, 22, 12, 26, 19, 28, 45, 13, 16, 18, 27], [0.9404504504504504, 0.9411711711711712, 0.941081081081081, 0.9427927927927928, 0.941981981981982, 0.940900900900901, 0.9409909909909909, 0.9423423423423423, 0.9434234234234233, 0.9405405405405406, 0.9418918918918919, 0.9420720720720721, 0.9436036036036036, 0.942072072072072, 0.9406306306306306, 0.9436936936936937, 0.9423423423423424, 0.9408108108108109, 0.9416216216216217, 0.9436936936936936, 0.9422522522522522, 0.9435135135135135, 0.9416216216216217, 0.9428828828828829, 0.941081081081081, 0.9412612612612612, 0.9425225225225224, 0.9403603603603603, 0.9428828828828828, 0.9424324324324324, 0.9424324324324324], [0.40314085664402444, 0.40405685453775664, 0.40306419780742486, 0.41047298536483423, 0.4080883612772183, 0.40261550786900147, 0.402627242408175, 0.4078329362331152, 0.4113737290515094, 0.4029954697071223, 0.40765395463076354, 0.4080996974079129, 0.4128830067085303, 0.40748201752519353, 0.4025218163629189, 0.41363514050712513, 0.40763856263088266, 0.4034278036904077, 0.40567846903441107, 0.41533184306950427, 0.407821443210319, 0.4128714340154459, 0.4040425865477913, 0.41048439953723775, 0.40306419780742486, 0.40342141617034305, 0.4083670960363105, 0.40331209373607296, 0.4117069454052546, 0.40958287973863566, 0.406407302649005], [0.3448275862068965, 0.3448275862068965, 0.3448275862068965, 0.3448275862068965, 0.3448275862068965, 0.3448275862068965, 0.3448275862068965, 0.3448275862068965, 0.3448275862068965, 0.3448275862068965, 0.3448275862068965, 0.3448275862068965, 0.3448275862068965, 0.3448275862068965, 0.3448275862068965, 0.3448275862068965, 0.3448275862068965, 0.3448275862068965, 0.3448275862068965, 0.3448275862068965, 0.3448275862068965, 0.3448275862068965, 0.3448275862068965, 0.3448275862068965, 0.3448275862068965, 0.3448275862068965, 0.3448275862068965, 0.3448275862068965, 0.3448275862068965, 0.3448275862068965, 0.3448275862068965]]\n",
        "# simulation_ce = [[145, 10, 15, 14, 35, 25, 34, 10, 20, 24, 44, 30, 63, 10, 16, 41, 26, 19, 33, 44, 32, 13, 23, 25, 16, 22, 55, 11, 18, 69, 17], [0.9473873873873874, 0.946036036036036, 0.9427027027027026, 0.9467567567567567, 0.9435135135135135, 0.9399099099099099, 0.9374774774774774, 0.936036036036036, 0.940990990990991, 0.9227027027027027, 0.9345045045045046, 0.9351351351351352, 0.9300900900900901, 0.9309909909909909, 0.9312612612612612, 0.9214414414414414, 0.9266666666666667, 0.9272972972972973, 0.9247747747747747, 0.9033333333333334, 0.9193693693693694, 0.9186486486486487, 0.9143243243243243, 0.9146846846846848, 0.9068468468468468, 0.9136936936936937, 0.9281981981981982, 0.9211711711711712, 0.9255855855855857, 0.9181981981981983, 0.9082882882882883], [0.305232232134608, 0.3060093262934467, 0.3067543282698745, 0.32179274126900254, 0.2849302560675358, 0.29850240794925537, 0.2894465541026962, 0.29882737536690873, 0.3024876655313838, 0.28851004053851725, 0.3411958784821764, 0.3314869967298828, 0.2972532460692812, 0.31121481031815756, 0.2936031093953547, 0.3131945755557457, 0.27996722353507775, 0.2962731231384651, 0.3124522041552719, 0.28602341774935497, 0.3039241425375489, 0.2965962165396764, 0.3023627396101569, 0.3052906105842364, 0.2843951143015883, 0.28491183959468747, 0.26348066438113915, 0.3067816610138892, 0.2999366601739338, 0.29335017000454966, 0.293613518788803], [0.4137931034482759, 0.4137931034482759, 0.4137931034482759, 0.4137931034482759, 0.3448275862068965, 0.4137931034482759, 0.3448275862068965, 0.4137931034482759, 0.4137931034482759, 0.3448275862068965, 0.3448275862068965, 0.2758620689655172, 0.2758620689655172, 0.2758620689655172, 0.2758620689655172, 0.2758620689655172, 0.2758620689655172, 0.2758620689655172, 0.2758620689655172, 0.2758620689655172, 0.3448275862068965, 0.2758620689655172, 0.2758620689655172, 0.3448275862068965, 0.3448275862068965, 0.2758620689655172, 0.4137931034482759, 0.4137931034482759, 0.4137931034482759, 0.2758620689655172, 0.2758620689655172]]\n",
        "# simulation_focal = [[74, 10, 10, 21, 154, 47, 24, 37, 25, 16, 17, 11, 12, 22, 24, 21, 18, 29, 27, 19, 17, 18, 24, 12, 30, 28, 23, 19, 11, 22, 13], [0.9942329157881681, 0.9939848691554012, 0.9942329157881682, 0.9944809624209352, 0.9489644053081979, 0.9451196825003101, 0.9371821902517673, 0.9474141138534046, 0.9477241721443632, 0.9492744635991567, 0.9505146967629914, 0.9507007317375666, 0.9466079622969117, 0.9478481954607467, 0.9476621604861714, 0.9502046384720327, 0.9466699739551037, 0.9452437058166936, 0.9481582537517054, 0.9406548431105047, 0.9486543470172392, 0.9492124519409649, 0.9427012278308323, 0.9507627433957584, 0.9445615775765844, 0.945863822398611, 0.945615775765844, 0.9433213444127495, 0.9389185166811361, 0.9474761255115962, 0.9461118690313779], [0.8698570934196981, 0.8661674489131845, 0.8698570934196981, 0.878098589235075, 0.6759791622277223, 0.6790081210966336, 0.6554287150141308, 0.626739849636415, 0.6313268783056842, 0.6442071922515911, 0.605200587829997, 0.6053092100425035, 0.6383547864345533, 0.6204459493622417, 0.6249453083302469, 0.6175003688732906, 0.6195452729096073, 0.6117432165085721, 0.6137142402183011, 0.5965431836180759, 0.6328180280780563, 0.6337953085411974, 0.610753014773887, 0.675614127133818, 0.6030999672537131, 0.6263101907101767, 0.6211278125010302, 0.614412748610864, 0.612225604697942, 0.6387403071531994, 0.61367731614722], [0.8372093023255814, 0.8372093023255814, 0.8372093023255814, 0.8837209302325582, 0.6511627906976744, 0.6976744186046512, 0.6511627906976744, 0.6046511627906977, 0.6046511627906977, 0.6511627906976744, 0.6046511627906977, 0.6046511627906977, 0.6046511627906977, 0.6511627906976744, 0.6046511627906977, 0.6046511627906977, 0.6046511627906977, 0.6046511627906977, 0.6046511627906977, 0.6046511627906977, 0.6511627906976744, 0.6976744186046512, 0.6046511627906977, 0.6511627906976744, 0.6046511627906977, 0.6046511627906977, 0.6046511627906977, 0.6046511627906977, 0.6046511627906977, 0.6046511627906977, 0.6046511627906977]]\n"
      ],
      "metadata": {
        "id": "cDgl_i80w-EZ"
      },
      "id": "cDgl_i80w-EZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Der letzte Index inhät Daten zum erzielten F1-Score. Dieser wird allerdings nicht weiter ausgewertet. Stattdessen wird der AUC für die Precision-Recall Curve weiter betrachtet."
      ],
      "metadata": {
        "id": "_GgkRmWf1zal"
      },
      "id": "_GgkRmWf1zal"
    },
    {
      "cell_type": "code",
      "source": [
        "summary = [\n",
        "    simulation_mse[0],\n",
        "    simulation_mae[0],\n",
        "    simulation_huber[0],\n",
        "    simulation_ce[0],\n",
        "    simulation_focal[0],\n",
        "    simulation_mse[1],\n",
        "    simulation_mae[1],\n",
        "    simulation_huber[1],\n",
        "    simulation_ce[1],\n",
        "    simulation_focal[1],\n",
        "    simulation_mse[2],\n",
        "    simulation_mae[2],\n",
        "    simulation_huber[2],\n",
        "    simulation_ce[2],\n",
        "    simulation_focal[2]   \n",
        "]"
      ],
      "metadata": {
        "id": "QRfTfI8N9khQ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "QRfTfI8N9khQ"
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame(zip(*summary),\n",
        "                       columns=[\n",
        "                           \"last_epochs_mse\",\n",
        "                           \"last_epochs_mae\",\n",
        "                           \"last_epochs_huber\",\n",
        "                           \"last_epochs_ce\",\n",
        "                           \"last_epochs_focal\",\n",
        "                           \"auc_mse\",\n",
        "                           \"auc_mae\",\n",
        "                           \"auc_huber\",\n",
        "                           \"auc_ce\",\n",
        "                           \"auc_focal\",\n",
        "                           \"auprc_mse\",\n",
        "                           \"auprc_mae\",\n",
        "                           \"auprc_huber\",\n",
        "                           \"auprc_ce\",\n",
        "                           \"auprc_focal\"\n",
        "                       ]\n",
        "\n",
        ")\n",
        "\n",
        "results"
      ],
      "metadata": {
        "id": "DW-SZzRz9khQ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "DW-SZzRz9khQ"
    },
    {
      "cell_type": "code",
      "source": [
        "#print boxplot for each variable\n",
        "fig, axs = plt.subplots(ncols=5, nrows=3, figsize=(20, 10))\n",
        "index = 0\n",
        "axs = axs.flatten()\n",
        "for k,v in results.items():\n",
        "    sns.boxplot(y=k, data=results, ax=axs[index])\n",
        "    index += 1\n",
        "save_fig(\"boxplots_results\")\n",
        "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)"
      ],
      "metadata": {
        "id": "DNnhthd89khR"
      },
      "execution_count": null,
      "outputs": [],
      "id": "DNnhthd89khR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kruskal-Wallis-Test"
      ],
      "metadata": {
        "id": "DhBiT7he15_B"
      },
      "id": "DhBiT7he15_B"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Der [Kruskal-Wallis-Test](https:https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kruskal.html//) ist ein nichtparametrischer Test, der zum Vergleich der Mittelwerte von drei oder mehr unabhängigen Gruppen verwendet werden kann. Er ähnelt der einseitigen ANOVA, setzt aber nicht voraus, dass die Daten normal verteilt sind.\n",
        "\n",
        "Zur Durchführung des Kruskal-Wallis-Tests werden die Daten zunächst vom niedrigsten zum höchsten Wert geordnet. Die Ränge werden dann zur Berechnung einer Teststatistik verwendet, die mit einem kritischen Wert verglichen wird, um festzustellen, ob der Unterschied zwischen den Gruppenmitteln statistisch signifikant ist.\n",
        "\n",
        "Der Kruskal-Wallis-Test ist geeignet, wenn die zu vergleichenden Populationen unabhängig sind und die Daten ordinal (d. h., sie können in eine Rangfolge gebracht werden) oder kontinuierlich (aber nicht normalverteilt) sind. Er ist auch geeignet, wenn die Stichprobengröße klein oder ungleich ist.\n",
        "\n"
      ],
      "metadata": {
        "id": "cAJ0U1zb1-eW"
      },
      "id": "cAJ0U1zb1-eW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last-Epochs: Gibt es Unterschiede bezüglich Konvergenz? "
      ],
      "metadata": {
        "id": "TiECRXFK4Zpp"
      },
      "id": "TiECRXFK4Zpp"
    },
    {
      "cell_type": "code",
      "source": [
        "for i in [simulation_mse, simulation_mae, simulation_huber, simulation_ce, simulation_focal]:\n",
        "  print(\"_______________________________________________________\")\n",
        "  print(\"Mean and standard deviation of last epochs\")\n",
        "  print(np.asarray(i[0]).mean())\n",
        "  print(np.asarray(i[0]).std())"
      ],
      "metadata": {
        "id": "nuQTmqfp_YIz"
      },
      "id": "nuQTmqfp_YIz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "'''\n",
        "Null hypothesis that the population median of all of the groups are equal\n",
        "\n",
        "Here: stopped epochs to evaluate the convergence ability \n",
        "'''\n",
        "stats.kruskal(\n",
        "    np.hstack(simulation_mse[0]),\n",
        "    np.hstack(simulation_mae[0]),\n",
        "    np.hstack(simulation_huber[0]),\n",
        "    np.hstack(simulation_ce[0]),\n",
        "    np.hstack(simulation_focal[0]),\n",
        "     )\n",
        "# KruskalResult(statistic=22.23847083667068, pvalue=0.0001796605135019599)\n"
      ],
      "metadata": {
        "id": "f0WyXHEZPkEP"
      },
      "id": "f0WyXHEZPkEP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AUC-ROC Gibt es Unterschiede für die Performance in den Testdaten?"
      ],
      "metadata": {
        "id": "fteyhLbW4f_n"
      },
      "id": "fteyhLbW4f_n"
    },
    {
      "cell_type": "code",
      "source": [
        "for i in [simulation_mse, simulation_mae, simulation_huber, simulation_ce, simulation_focal]:\n",
        "  print(\"_______________________________________________________\")\n",
        "  print(\"Mean and standard deviation of roc_auc\")\n",
        "  print(np.asarray(i[1]).mean())\n",
        "  print(np.asarray(i[1]).std())"
      ],
      "metadata": {
        "id": "sSoLCgzh_coN"
      },
      "id": "sSoLCgzh_coN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "'''\n",
        "Null hypothesis that the population median of all of the groups are equal\n",
        "\n",
        "Here: stopped epochs to evaluate the convergence ability \n",
        "'''\n",
        "stats.kruskal(\n",
        "    np.hstack(simulation_mse[1]),\n",
        "    np.hstack(simulation_mae[1]),\n",
        "    np.hstack(simulation_huber[1]),\n",
        "    np.hstack(simulation_ce[1]),\n",
        "    np.hstack(simulation_focal[1]),\n",
        "     )\n",
        " # KruskalResult(statistic=131.99214948486943, pvalue=1.459883736690209e-27)\n"
      ],
      "metadata": {
        "id": "IAVYjCdx2-lw"
      },
      "id": "IAVYjCdx2-lw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AUC-Precision-Recall-Curve: Gibt es Unterschiede für die Performance in den Testdaten?"
      ],
      "metadata": {
        "id": "CtNMo1Cq4ryp"
      },
      "id": "CtNMo1Cq4ryp"
    },
    {
      "cell_type": "code",
      "source": [
        "for i in [simulation_mse, simulation_mae, simulation_huber, simulation_ce, simulation_focal]:\n",
        "  print(\"_______________________________________________________\")\n",
        "  print(\"Mean and standard deviation of roc_recall_precision\")\n",
        "  print(np.asarray(i[2]).mean())\n",
        "  print(np.asarray(i[2]).std())"
      ],
      "metadata": {
        "id": "h1eQ8WSp_j7p"
      },
      "id": "h1eQ8WSp_j7p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "'''\n",
        "Null hypothesis that the population median of all of the groups are equal\n",
        "\n",
        "Here: stopped epochs to evaluate the convergence ability \n",
        "'''\n",
        "stats.kruskal(\n",
        "    np.hstack(simulation_mse[2]),\n",
        "    np.hstack(simulation_mae[2]),\n",
        "    np.hstack(simulation_huber[2]),\n",
        "    np.hstack(simulation_ce[2]),\n",
        "    np.hstack(simulation_focal[2]),\n",
        "     )\n",
        "# KruskalResult(statistic=131.86117955510082, pvalue=1.557160300415864e-27)\n"
      ],
      "metadata": {
        "id": "yvYPtm1z2_e-"
      },
      "id": "yvYPtm1z2_e-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wenn die Nullhypothese des Kruskal-Wallis-Tests abgelehnt wird, bedeutet dies, dass sich mindestens einer der Gruppenmittelwerte signifikant von den anderen unterscheidet. Der Kruskal-Wallis-Test sagt jedoch nicht aus, welche Gruppe(n) sich unterscheiden. Um festzustellen, welche Gruppe(n) sich unterscheidet/unterscheiden, muss eine zusätzliche Analyse durchgeführt werden, z. B. ein Post-hoc-Test.\n",
        "\n",
        "Es gibt mehrere Post-hoc-Tests, die zum Vergleich der Mittelwerte bestimmter Gruppenpaare verwendet werden können. Zu den gängigen Tests gehören der [Dunn-Test](https://scikit-posthocs.readthedocs.io/en/latest/generated/scikit_posthocs.posthoc_dunn), der [Conover-Test](https://scikit-posthocs.readthedocs.io/en/latest/generated/scikit_posthocs.posthoc_conover) und der [Steel-Dwass-Test](https://scikit-posthocs.readthedocs.io/en/latest/generated/scikit_posthocs.posthoc_dscf). Diese Tests verwenden die Rangdaten aus dem Kruskal-Wallis-Test, um festzustellen, welche Gruppenpaare signifikant unterschiedliche Mittelwerte aufweisen.\n",
        "\n",
        "Alle diese Post-hoc-Tests können verwendet werden, um festzustellen, welche Gruppenpaare nach Durchführung des Kruskal-Wallis-Tests signifikant unterschiedliche Mittelwerte aufweisen.\n",
        "\n"
      ],
      "metadata": {
        "id": "uyzXbYfm2oRg"
      },
      "id": "uyzXbYfm2oRg"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-posthocs"
      ],
      "metadata": {
        "id": "JdIVIJbU7fvV"
      },
      "id": "JdIVIJbU7fvV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Dunn-Test](https://scikit-posthocs.readthedocs.io/en/latest/generated/scikit_posthocs.posthoc_dunn)\n",
        "\n",
        "Der Dunn-Test ist ein Post-hoc-Test, der die Mittelwerte aller Gruppenpaare unter Verwendung der Rangdaten aus dem Kruskal-Wallis-Test vergleicht. Er basiert auf der Differenz der Ränge zwischen den beiden zu vergleichenden Gruppen und passt sich durch Kontrolle der Falschentdeckungsrate an Mehrfachvergleiche an."
      ],
      "metadata": {
        "id": "2khLAXlO6McU"
      },
      "id": "2khLAXlO6McU"
    },
    {
      "cell_type": "code",
      "source": [
        "# last-epochs\n",
        "import scikit_posthocs as sp\n",
        "\n",
        "last_epochs = [np.hstack(simulation_mse[0]),\n",
        "    np.hstack(simulation_mae[0]),\n",
        "    np.hstack(simulation_huber[0]),\n",
        "    np.hstack(simulation_ce[0]),\n",
        "    np.hstack(simulation_focal[0])]\n",
        "\n",
        "sp.posthoc_dunn(last_epochs, p_adjust = 'holm')"
      ],
      "metadata": {
        "id": "2Wrb8pRT6t0T"
      },
      "id": "2Wrb8pRT6t0T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beobachtung: Keine signifikanten Unterschiede bzgl. der Konvergenz zu beobachten.\n",
        "\n"
      ],
      "metadata": {
        "id": "d382uY7b7wfE"
      },
      "id": "d382uY7b7wfE"
    },
    {
      "cell_type": "code",
      "source": [
        "# roc_auc\n",
        "import scikit_posthocs as sp\n",
        "\n",
        "roc_auc = [np.hstack(simulation_mse[1]),\n",
        "    np.hstack(simulation_mae[1]),\n",
        "    np.hstack(simulation_huber[1]),\n",
        "    np.hstack(simulation_ce[1]),\n",
        "    np.hstack(simulation_focal[1])]\n",
        "\n",
        "sp.posthoc_dunn(roc_auc, p_adjust = 'holm')"
      ],
      "metadata": {
        "id": "cTslCFAi6twY"
      },
      "id": "cTslCFAi6twY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# recall_precision_auc\n",
        "import scikit_posthocs as sp\n",
        "\n",
        "recall_precision_auc = [np.hstack(simulation_mse[2]),\n",
        "    np.hstack(simulation_mae[2]),\n",
        "    np.hstack(simulation_huber[2]),\n",
        "    np.hstack(simulation_ce[2]),\n",
        "    np.hstack(simulation_focal[2])]\n",
        "\n",
        "sp.posthoc_dunn(recall_precision_auc, p_adjust = 'holm')"
      ],
      "metadata": {
        "id": "tjqH44_Z6tlf"
      },
      "id": "tjqH44_Z6tlf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Endresultat für Thyroid Disease"
      ],
      "metadata": {
        "id": "rJ2dqOkYBi1i"
      },
      "id": "rJ2dqOkYBi1i"
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Performance                  | Loss       |\n",
        "|------------------------------|------------|\n",
        "| Schnellste Konvergenz?       | -          |\n",
        "| Bester ROC-AUC?              | MAE & FL   |\n",
        "| Bester ROC-Recall-Precision? | MAE & MSE  |\n",
        "\n",
        "Die Nullhypothese des Kruksal-Wallis-Test bzgl. Konvergenz (gestoppte Epochen) konnte nicht verworfen werden.\n",
        "\n",
        "Der MAE & FL haben signifikant die besten Ergebnisse gezeigt mit der AUC und unterscheiden sich nicht untereinander. Diese Unterscheiden sich nur signifikant zu den drei anderen Modellen. Somit wird die Entscheidung getroffen, dass beide Modelle im Schnitt am besten performt haben mit der AUC-Metrik.\n",
        "\n",
        "Der MAE & MSE haben signifikant die besten Ergebnisse gezeigt mit der AUPRC-Metrik und unterscheiden sich nicht untereinander. Diese Unterscheiden sich nur signifikant zu den drei anderen Modellen. Somit wird die Entscheidung getroffen, dass beide Modelle im Schnitt am besten performt haben mit der AUPRC-Metrik.\n",
        "\n",
        "Da der AUC und der AUPRC mit dem MAE im Schnitt am besten gewesen sind, wird nach dieser Simulation das MAE-Modell als finales Modell gewählt."
      ],
      "metadata": {
        "id": "oJxRjhOTAYGP"
      },
      "id": "oJxRjhOTAYGP"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 28.912306,
      "end_time": "2021-07-25T13:21:52.129684",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-07-25T13:21:23.217378",
      "version": "2.3.3"
    },
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}