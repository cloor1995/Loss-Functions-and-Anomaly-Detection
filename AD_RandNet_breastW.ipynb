{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Title: Wisconsin Breast Cancer Database (January 8, 1991)\n",
        "\n",
        "This breast cancer databases was obtained from the University of Wisconsin\n",
        "   Hospitals, Madison from Dr. William H. Wolberg.  If you publish results\n",
        "   when using this database, then please include this information in your\n",
        "   acknowledgements.  Also, please cite one or more of:\n",
        "\n",
        "   1. O. L. Mangasarian and W. H. Wolberg: \"Cancer diagnosis via linear \n",
        "      programming\", SIAM News, Volume 23, Number 5, September 1990, pp 1 & 18.\n",
        "\n",
        "   2. William H. Wolberg and O.L. Mangasarian: \"Multisurface method of \n",
        "      pattern separation for medical diagnosis applied to breast cytology\", \n",
        "      Proceedings of the National Academy of Sciences, U.S.A., Volume 87, \n",
        "      December 1990, pp 9193-9196.\n",
        "\n",
        "   3. O. L. Mangasarian, R. Setiono, and W.H. Wolberg: \"Pattern recognition \n",
        "      via linear programming: Theory and application to medical diagnosis\", \n",
        "      in: \"Large-scale numerical optimization\", Thomas F. Coleman and Yuying\n",
        "      Li, editors, SIAM Publications, Philadelphia 1990, pp 22-30.\n",
        "\n",
        "   4. K. P. Bennett & O. L. Mangasarian: \"Robust linear programming \n",
        "      discrimination of two linearly inseparable sets\", Optimization Methods\n",
        "      and Software 1, 1992, 23-34 (Gordon & Breach Science Publishers).\n",
        "\n",
        "2. Sources:\n",
        "   -- Dr. WIlliam H. Wolberg (physician)\n",
        "      University of Wisconsin Hospitals\n",
        "      Madison, Wisconsin\n",
        "      USA\n",
        "   -- Donor: Olvi Mangasarian (mangasarian@cs.wisc.edu)\n",
        "      Received by David W. Aha (aha@cs.jhu.edu)\n",
        "   -- Date: 15 July 1992\n",
        "\n",
        "3. Past Usage:\n",
        "\n",
        "   Attributes 2 through 10 have been used to represent instances.\n",
        "   Each instance has one of 2 possible classes: benign or malignant.\n",
        "\n",
        "   1. Wolberg,~W.~H., \\& Mangasarian,~O.~L. (1990). Multisurface method of \n",
        "      pattern separation for medical diagnosis applied to breast cytology. In\n",
        "      {\\it Proceedings of the National Academy of Sciences}, {\\it 87},\n",
        "      9193--9196.\n",
        "      -- Size of data set: only 369 instances (at that point in time)\n",
        "      -- Collected classification results: 1 trial only\n",
        "      -- Two pairs of parallel hyperplanes were found to be consistent with\n",
        "         50% of the data\n",
        "         -- Accuracy on remaining 50% of dataset: 93.5%\n",
        "      -- Three pairs of parallel hyperplanes were found to be consistent with\n",
        "         67% of data\n",
        "         -- Accuracy on remaining 33% of dataset: 95.9%\n",
        "\n",
        "   2. Zhang,~J. (1992). Selecting typical instances in instance-based\n",
        "      learning.  In {\\it Proceedings of the Ninth International Machine\n",
        "      Learning Conference} (pp. 470--479).  Aberdeen, Scotland: Morgan\n",
        "      Kaufmann.\n",
        "      -- Size of data set: only 369 instances (at that point in time)\n",
        "      -- Applied 4 instance-based learning algorithms \n",
        "      -- Collected classification results averaged over 10 trials\n",
        "      -- Best accuracy result: \n",
        "         -- 1-nearest neighbor: 93.7%\n",
        "         -- trained on 200 instances, tested on the other 169\n",
        "      -- Also of interest:\n",
        "         -- Using only typical instances: 92.2% (storing only 23.1 instances)\n",
        "         -- trained on 200 instances, tested on the other 169"
      ],
      "metadata": {
        "id": "vekpui3ROUuO"
      },
      "id": "vekpui3ROUuO"
    },
    {
      "cell_type": "markdown",
      "id": "534c9f8f",
      "metadata": {
        "papermill": {
          "duration": 0.034124,
          "end_time": "2021-07-25T13:21:32.083955",
          "exception": false,
          "start_time": "2021-07-25T13:21:32.049831",
          "status": "completed"
        },
        "tags": [],
        "id": "534c9f8f"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyod"
      ],
      "metadata": {
        "id": "gOvrCcCmGBXt"
      },
      "id": "gOvrCcCmGBXt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mlconfig"
      ],
      "metadata": {
        "id": "KdpZU0b3GZH-"
      },
      "id": "KdpZU0b3GZH-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install segmentation_models_3D"
      ],
      "metadata": {
        "id": "4Quy4RAt5Wjh"
      },
      "id": "4Quy4RAt5Wjh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/jonbarron/robust_loss_pytorch\n"
      ],
      "metadata": {
        "id": "Nnp6tRAC6iQp"
      },
      "id": "Nnp6tRAC6iQp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bbfd417",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:32.152997Z",
          "iopub.status.busy": "2021-07-25T13:21:32.151665Z",
          "iopub.status.idle": "2021-07-25T13:21:39.867360Z",
          "shell.execute_reply": "2021-07-25T13:21:39.866498Z",
          "shell.execute_reply.started": "2021-07-25T13:20:27.135966Z"
        },
        "papermill": {
          "duration": 7.748097,
          "end_time": "2021-07-25T13:21:39.867567",
          "exception": false,
          "start_time": "2021-07-25T13:21:32.119470",
          "status": "completed"
        },
        "tags": [],
        "id": "3bbfd417"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import seaborn as sns; sns.set()\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import mlconfig\n",
        "import random as rn\n",
        "import keras\n",
        "import segmentation_models_3D as sm\n",
        "import robust_loss_pytorch \n",
        "import shutil\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "from time import strftime\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, precision_recall_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import normalize\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n",
        "                             roc_curve, recall_score, classification_report, f1_score,\n",
        "                             precision_recall_fscore_support)\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score,cohen_kappa_score,roc_auc_score,f1_score,auc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Achtung: Alle Dateien wurden in einem Ordner auf Drive gespeichert. Um den Code zu replizieren empfehle ich, ebenfalls einen Ordner auf Drive zu erstellen und folgendermaßen das working directory anzupassen:\n",
        "\n",
        "```\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "```\n",
        "\n",
        "```\n",
        "cd /content/drive/MyDrive/<specify here your path>\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "a6NAmi0xmCGJ"
      },
      "id": "a6NAmi0xmCGJ"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "i3W2O7nA8Ywm"
      },
      "id": "i3W2O7nA8Ywm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "assert sys.version_info >= (3, 7) #python 3.7 or higher recommended"
      ],
      "metadata": {
        "id": "f5zzIbWeEQoF"
      },
      "id": "f5zzIbWeEQoF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from packaging import version\n",
        "import sklearn\n",
        "\n",
        "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\") #sklearn 1.01 or higher remmonded"
      ],
      "metadata": {
        "id": "bC4XPHKTEWsb"
      },
      "id": "bC4XPHKTEWsb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\") #tensorflow 2.8.0 or higher remmonded "
      ],
      "metadata": {
        "id": "2cVkUkOaEcrS"
      },
      "id": "2cVkUkOaEcrS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardeinstellungen für Plots anpassen"
      ],
      "metadata": {
        "id": "UBkgZ7vMDfc1"
      },
      "id": "UBkgZ7vMDfc1"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)"
      ],
      "metadata": {
        "id": "DmnRbn6sDanW"
      },
      "id": "DmnRbn6sDanW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Seminar2/ADAPL #specify here your path"
      ],
      "metadata": {
        "id": "LOCKjhUXFDvh"
      },
      "id": "LOCKjhUXFDvh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "aSFiTk8JFGKo"
      },
      "id": "aSFiTk8JFGKo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Erstellen eines Bildordners zum Sammeln aller Bildausgaben\n",
        "\n",
        "Erstellen Sie eine Funktion zum automatischen Speichern eines Plots in einem bestimmten Pfad."
      ],
      "metadata": {
        "id": "ujovvzSuE3Lg"
      },
      "id": "ujovvzSuE3Lg"
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "IMAGES_PATH = Path() / \"images\" / \"breastW\"\n",
        "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ],
      "metadata": {
        "id": "59KLrFMFE034"
      },
      "id": "59KLrFMFE034",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "gAw-T9e69ZVL"
      },
      "id": "gAw-T9e69ZVL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "Vhz7fY5Q0MfF"
      },
      "id": "Vhz7fY5Q0MfF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a6f67029",
      "metadata": {
        "papermill": {
          "duration": 0.027964,
          "end_time": "2021-07-25T13:21:39.924769",
          "exception": false,
          "start_time": "2021-07-25T13:21:39.896805",
          "status": "completed"
        },
        "tags": [],
        "id": "a6f67029"
      },
      "source": [
        "# Data Wrapping "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Relevant Information:\n",
        "\n",
        "   Samples arrive periodically as Dr. Wolberg reports his clinical cases.\n",
        "   The database therefore reflects this chronological grouping of the data.\n",
        "   This grouping information appears immediately below, having been removed\n",
        "   from the data itself:\n",
        "\n",
        "     Group 1: 367 instances (January 1989)\n",
        "     Group 2:  70 instances (October 1989)\n",
        "     Group 3:  31 instances (February 1990)\n",
        "     Group 4:  17 instances (April 1990)\n",
        "     Group 5:  48 instances (August 1990)\n",
        "     Group 6:  49 instances (Updated January 1991)\n",
        "     Group 7:  31 instances (June 1991)\n",
        "     Group 8:  86 instances (November 1991)\n",
        "     -----------------------------------------\n",
        "     Total:   699 points (as of the donated datbase on 15 July 1992)"
      ],
      "metadata": {
        "id": "DTZaV3utOse9"
      },
      "id": "DTZaV3utOse9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of Instances: 699 (as of 15 July 1992)\n",
        "\n",
        "Number of Attributes: 10 plus the class attribute\n",
        "\n",
        "Attribute Information: (class attribute has been moved to last column)\n",
        "\n",
        "   #  Attribute                     Domain\n",
        "   -- -----------------------------------------\n",
        "   1. Sample code number            id number\n",
        "   2. Clump Thickness               1 - 10\n",
        "   3. Uniformity of Cell Size       1 - 10\n",
        "   4. Uniformity of Cell Shape      1 - 10\n",
        "   5. Marginal Adhesion             1 - 10\n",
        "   6. Single Epithelial Cell Size   1 - 10\n",
        "   7. Bare Nuclei                   1 - 10\n",
        "   8. Bland Chromatin               1 - 10\n",
        "   9. Normal Nucleoli               1 - 10\n",
        "  10. Mitoses                       1 - 10\n",
        "  11. Class:                        (2 for benign, 4 for malignant)"
      ],
      "metadata": {
        "id": "GLo7UNfNO78C"
      },
      "id": "GLo7UNfNO78C"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23f8686a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:39.989429Z",
          "iopub.status.busy": "2021-07-25T13:21:39.988272Z",
          "iopub.status.idle": "2021-07-25T13:21:40.061055Z",
          "shell.execute_reply": "2021-07-25T13:21:40.061646Z",
          "shell.execute_reply.started": "2021-07-25T13:20:27.151048Z"
        },
        "papermill": {
          "duration": 0.109014,
          "end_time": "2021-07-25T13:21:40.061847",
          "exception": false,
          "start_time": "2021-07-25T13:21:39.952833",
          "status": "completed"
        },
        "tags": [],
        "id": "23f8686a"
      },
      "outputs": [],
      "source": [
        "PATH = '/content/drive/MyDrive/Seminar2/ADAPL/breastW_data/breast-cancer-wisconsin.txt'\n",
        "df = pd.read_csv(PATH)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e3e32da",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:40.138132Z",
          "iopub.status.busy": "2021-07-25T13:21:40.137413Z",
          "iopub.status.idle": "2021-07-25T13:21:40.233851Z",
          "shell.execute_reply": "2021-07-25T13:21:40.233292Z",
          "shell.execute_reply.started": "2021-07-25T13:20:27.192627Z"
        },
        "papermill": {
          "duration": 0.14124,
          "end_time": "2021-07-25T13:21:40.234017",
          "exception": false,
          "start_time": "2021-07-25T13:21:40.092777",
          "status": "completed"
        },
        "tags": [],
        "id": "1e3e32da"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e023884f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:40.315199Z",
          "iopub.status.busy": "2021-07-25T13:21:40.314023Z",
          "iopub.status.idle": "2021-07-25T13:21:40.318207Z",
          "shell.execute_reply": "2021-07-25T13:21:40.318690Z",
          "shell.execute_reply.started": "2021-07-25T13:20:27.274520Z"
        },
        "papermill": {
          "duration": 0.054244,
          "end_time": "2021-07-25T13:21:40.318870",
          "exception": false,
          "start_time": "2021-07-25T13:21:40.264626",
          "status": "completed"
        },
        "tags": [],
        "id": "e023884f"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4000e47d",
      "metadata": {
        "papermill": {
          "duration": 0.028919,
          "end_time": "2021-07-25T13:21:40.377325",
          "exception": false,
          "start_time": "2021-07-25T13:21:40.348406",
          "status": "completed"
        },
        "tags": [],
        "id": "4000e47d"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a57db549",
      "metadata": {
        "papermill": {
          "duration": 0.028881,
          "end_time": "2021-07-25T13:21:40.435350",
          "exception": false,
          "start_time": "2021-07-25T13:21:40.406469",
          "status": "completed"
        },
        "tags": [],
        "id": "a57db549"
      },
      "source": [
        "Drop unnecessary columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2239e656",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:40.500467Z",
          "iopub.status.busy": "2021-07-25T13:21:40.499799Z",
          "iopub.status.idle": "2021-07-25T13:21:40.502914Z",
          "shell.execute_reply": "2021-07-25T13:21:40.503391Z",
          "shell.execute_reply.started": "2021-07-25T13:20:27.289012Z"
        },
        "papermill": {
          "duration": 0.038906,
          "end_time": "2021-07-25T13:21:40.503565",
          "exception": false,
          "start_time": "2021-07-25T13:21:40.464659",
          "status": "completed"
        },
        "tags": [],
        "id": "2239e656"
      },
      "outputs": [],
      "source": [
        "drop_cols = [\"Sample code number\"]\n",
        "df.drop(drop_cols, axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bf6ce80",
      "metadata": {
        "papermill": {
          "duration": 0.02909,
          "end_time": "2021-07-25T13:21:40.562231",
          "exception": false,
          "start_time": "2021-07-25T13:21:40.533141",
          "status": "completed"
        },
        "tags": [],
        "id": "7bf6ce80"
      },
      "source": [
        "Convert to dummy variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5adfe94d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:40.628625Z",
          "iopub.status.busy": "2021-07-25T13:21:40.627928Z",
          "iopub.status.idle": "2021-07-25T13:21:40.631220Z",
          "shell.execute_reply": "2021-07-25T13:21:40.630570Z",
          "shell.execute_reply.started": "2021-07-25T13:20:27.297185Z"
        },
        "papermill": {
          "duration": 0.039489,
          "end_time": "2021-07-25T13:21:40.631370",
          "exception": false,
          "start_time": "2021-07-25T13:21:40.591881",
          "status": "completed"
        },
        "tags": [],
        "id": "5adfe94d"
      },
      "outputs": [],
      "source": [
        "rep_dict = {2: 0.0, 4: 1.0}\n",
        "df['class outlier'].replace(rep_dict, inplace=True);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceccb042",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:40.695925Z",
          "iopub.status.busy": "2021-07-25T13:21:40.695283Z",
          "iopub.status.idle": "2021-07-25T13:21:40.698996Z",
          "shell.execute_reply": "2021-07-25T13:21:40.699704Z",
          "shell.execute_reply.started": "2021-07-25T13:20:27.310014Z"
        },
        "papermill": {
          "duration": 0.038717,
          "end_time": "2021-07-25T13:21:40.699919",
          "exception": false,
          "start_time": "2021-07-25T13:21:40.661202",
          "status": "completed"
        },
        "tags": [],
        "id": "ceccb042"
      },
      "outputs": [],
      "source": [
        "print(f'Data size is {df.shape}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9m-RJiCMNyfc"
      },
      "id": "9m-RJiCMNyfc"
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "RPpPZ23NK9df"
      },
      "id": "RPpPZ23NK9df",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Missing attribute values: 16\n",
        "\n",
        "There are 16 instances in Groups 1 to 6 that contain a single missing (i.e., unavailable) attribute value, now denoted by \"?\" in column \"Bare Nuclei\"\n"
      ],
      "metadata": {
        "id": "1_cfP_JzOJCD"
      },
      "id": "1_cfP_JzOJCD"
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[df['Bare Nuclei'] == \"?\"]"
      ],
      "metadata": {
        "id": "8oIcT1OdNoM3"
      },
      "id": "8oIcT1OdNoM3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(df.loc[df['Bare Nuclei']==\"?\"].index, inplace=True)"
      ],
      "metadata": {
        "id": "qIQCRaDtPVU_"
      },
      "id": "qIQCRaDtPVU_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Bare Nuclei'] = pd.to_numeric(df['Bare Nuclei'])"
      ],
      "metadata": {
        "id": "joLFde0lM__T"
      },
      "id": "joLFde0lM__T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Data size is {df.shape}.')"
      ],
      "metadata": {
        "id": "Og0tSmuIQSMp"
      },
      "id": "Og0tSmuIQSMp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check for NA-Values \n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "ho1HV5CaW8-T"
      },
      "id": "ho1HV5CaW8-T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7bf1b5a5",
      "metadata": {
        "papermill": {
          "duration": 0.029306,
          "end_time": "2021-07-25T13:21:40.759005",
          "exception": false,
          "start_time": "2021-07-25T13:21:40.729699",
          "status": "completed"
        },
        "tags": [],
        "id": "7bf1b5a5"
      },
      "source": [
        "# EDA and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print boxplot for each variable\n",
        "fig, axs = plt.subplots(ncols=5, nrows=2, figsize=(20, 10))\n",
        "index = 0\n",
        "axs = axs.flatten()\n",
        "for k,v in df.items():\n",
        "    sns.boxplot(y=k, data=df, ax=axs[index])\n",
        "    index += 1\n",
        "save_fig(\"boxplots\")\n",
        "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)"
      ],
      "metadata": {
        "id": "g5gOVHsoXZj2"
      },
      "id": "g5gOVHsoXZj2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k, v in df.items():\n",
        "        q1 = v.quantile(0.25)\n",
        "        q3 = v.quantile(0.75)\n",
        "        irq = q3 - q1\n",
        "        v_col = v[(v <= q1 - 1.5 * irq) | (v >= q3 + 1.5 * irq)]\n",
        "        perc = np.shape(v_col)[0] * 100.0 / np.shape(df)[0]\n",
        "        print(\"Column %s outliers = %.2f%%\" % (k, perc))"
      ],
      "metadata": {
        "id": "kF4jz7nKYmr_"
      },
      "id": "kF4jz7nKYmr_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print distributions\n",
        "fig, axs = plt.subplots(ncols=5, nrows=2, figsize=(20, 10))\n",
        "index = 0\n",
        "axs = axs.flatten()\n",
        "for k,v in df.items():\n",
        "    sns.distplot(v, ax=axs[index])\n",
        "    index += 1\n",
        "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)\n",
        "save_fig(\"distrubutions\")"
      ],
      "metadata": {
        "id": "CoEAggn0Y2Je"
      },
      "id": "CoEAggn0Y2Je",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class distribution:\n",
        " \n",
        "   Benign: 458 (65.5%)\n",
        "   Malignant: 241 (34.5%)"
      ],
      "metadata": {
        "id": "dxjNAQDePJeX"
      },
      "id": "dxjNAQDePJeX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a45194fb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:40.824938Z",
          "iopub.status.busy": "2021-07-25T13:21:40.824315Z",
          "iopub.status.idle": "2021-07-25T13:21:41.055800Z",
          "shell.execute_reply": "2021-07-25T13:21:41.056331Z",
          "shell.execute_reply.started": "2021-07-25T13:20:27.323572Z"
        },
        "papermill": {
          "duration": 0.26791,
          "end_time": "2021-07-25T13:21:41.056508",
          "exception": false,
          "start_time": "2021-07-25T13:21:40.788598",
          "status": "completed"
        },
        "tags": [],
        "id": "a45194fb"
      },
      "outputs": [],
      "source": [
        "df['class outlier'].value_counts().plot(kind='bar', figsize=(8, 4));\n",
        "plt.title('Distrubution of outliers');\n",
        "plt.xlabel('Diagnosis');\n",
        "plt.ylabel('Frequency');\n",
        "plt.xticks([0.0, 1.0], ['Benign', 'Malignant'], rotation=45);\n",
        "save_fig(\"distrubution outliers\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "\n",
        "sns.set(style='whitegrid', context='notebook')\n",
        "\n",
        "def tsne_scatter(features, labels, dimensions=2, save_as='graph.png', RANDOM_SEED = 42):\n",
        "    if dimensions not in (2, 3):\n",
        "        raise ValueError('tsne_scatter can only plot in 2d or 3d')\n",
        "\n",
        "    # t-SNE dimensionality reduction\n",
        "    features_embedded = TSNE(n_components=dimensions, random_state=RANDOM_SEED).fit_transform(features)\n",
        "    \n",
        "    # initialising the plot\n",
        "    fig, ax = plt.subplots(figsize=(8,8))\n",
        "    \n",
        "    # counting dimensions\n",
        "    if dimensions == 3: ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    # plotting data\n",
        "    ax.scatter(\n",
        "        *zip(*features_embedded[np.where(labels==1)]),\n",
        "        marker='o',\n",
        "        color='r',\n",
        "        s=2,\n",
        "        alpha=0.7,\n",
        "        label='Bösartig'\n",
        "    )\n",
        "    ax.scatter(\n",
        "        *zip(*features_embedded[np.where(labels==0)]),\n",
        "        marker='o',\n",
        "        color='g',\n",
        "        s=2,\n",
        "        alpha=0.3,\n",
        "        label='Gutartig'\n",
        "    )\n",
        "\n",
        "    # storing it to be displayed later\n",
        "    sns.set(style='whitegrid', context='notebook')\n",
        "    save_fig(save_as)\n",
        "    plt.show;"
      ],
      "metadata": {
        "id": "ZJLkGL0MyQ91"
      },
      "id": "ZJLkGL0MyQ91",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26491ce4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:41.122609Z",
          "iopub.status.busy": "2021-07-25T13:21:41.121980Z",
          "iopub.status.idle": "2021-07-25T13:21:41.147594Z",
          "shell.execute_reply": "2021-07-25T13:21:41.148118Z",
          "shell.execute_reply.started": "2021-07-25T13:20:27.473227Z"
        },
        "papermill": {
          "duration": 0.060924,
          "end_time": "2021-07-25T13:21:41.148323",
          "exception": false,
          "start_time": "2021-07-25T13:21:41.087399",
          "status": "completed"
        },
        "tags": [],
        "id": "26491ce4"
      },
      "outputs": [],
      "source": [
        "df['anomaly'] = df['class outlier'] == 1.0\n",
        "anomaly = df[df['anomaly'] == True]\n",
        "normal = df[df['anomaly'] == False]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29a4a399",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:41.214548Z",
          "iopub.status.busy": "2021-07-25T13:21:41.213867Z",
          "iopub.status.idle": "2021-07-25T13:21:42.159895Z",
          "shell.execute_reply": "2021-07-25T13:21:42.160398Z",
          "shell.execute_reply.started": "2021-07-25T13:20:27.481699Z"
        },
        "papermill": {
          "duration": 0.980319,
          "end_time": "2021-07-25T13:21:42.160614",
          "exception": false,
          "start_time": "2021-07-25T13:21:41.180295",
          "status": "completed"
        },
        "tags": [],
        "id": "29a4a399"
      },
      "outputs": [],
      "source": [
        "sns.distplot(normal);\n",
        "sns.distplot(anomaly);\n",
        "\n",
        "plt.title('normal vs  anomaly Dist.');\n",
        "plt.ylabel('Dist.');\n",
        "save_fig(\"normal vs anomaly distrubution\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fcb1223",
      "metadata": {
        "papermill": {
          "duration": 0.033415,
          "end_time": "2021-07-25T13:21:42.227161",
          "exception": false,
          "start_time": "2021-07-25T13:21:42.193746",
          "status": "completed"
        },
        "tags": [],
        "id": "4fcb1223"
      },
      "source": [
        "# Normalize The Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = df.iloc[:, 0:11]\n",
        "data"
      ],
      "metadata": {
        "id": "ejQS1dtvS3EV"
      },
      "id": "ejQS1dtvS3EV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0bf83c2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:42.296276Z",
          "iopub.status.busy": "2021-07-25T13:21:42.295602Z",
          "iopub.status.idle": "2021-07-25T13:21:42.303352Z",
          "shell.execute_reply": "2021-07-25T13:21:42.303827Z",
          "shell.execute_reply.started": "2021-07-25T13:20:28.117717Z"
        },
        "papermill": {
          "duration": 0.043478,
          "end_time": "2021-07-25T13:21:42.304045",
          "exception": false,
          "start_time": "2021-07-25T13:21:42.260567",
          "status": "completed"
        },
        "tags": [],
        "id": "f0bf83c2"
      },
      "outputs": [],
      "source": [
        "# The last element contains the labels\n",
        "labels_bool = df.iloc[:,-1]\n",
        "labels  = df.iloc[:,-2]\n",
        "# The other data points are the features\n",
        "data = df.iloc[:, 0:9]\n",
        "\n",
        "data = normalize(data) #normalize data\n",
        "n_features = 9"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tsne_scatter(data, labels, dimensions=2, save_as='tsne_initial_2d')\n",
        "tsne_scatter(data, labels, dimensions=3, save_as='tsne_initial_3d')"
      ],
      "metadata": {
        "id": "UW_vlLmIy_Qq"
      },
      "id": "UW_vlLmIy_Qq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b7a3e8c0",
      "metadata": {
        "papermill": {
          "duration": 0.031913,
          "end_time": "2021-07-25T13:21:42.369025",
          "exception": false,
          "start_time": "2021-07-25T13:21:42.337112",
          "status": "completed"
        },
        "tags": [],
        "id": "b7a3e8c0"
      },
      "source": [
        "# Split the data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generiere Trainings- und Testdaten"
      ],
      "metadata": {
        "id": "kFlKmZZDUk-B"
      },
      "id": "kFlKmZZDUk-B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "968cc560",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:42.436944Z",
          "iopub.status.busy": "2021-07-25T13:21:42.436304Z",
          "iopub.status.idle": "2021-07-25T13:21:42.442700Z",
          "shell.execute_reply": "2021-07-25T13:21:42.443205Z",
          "shell.execute_reply.started": "2021-07-25T13:20:28.126744Z"
        },
        "papermill": {
          "duration": 0.042295,
          "end_time": "2021-07-25T13:21:42.443403",
          "exception": false,
          "start_time": "2021-07-25T13:21:42.401108",
          "status": "completed"
        },
        "tags": [],
        "id": "968cc560"
      },
      "outputs": [],
      "source": [
        "train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "    data, labels_bool,\n",
        "    test_size=0.2,\n",
        "    \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generiere das Validation-Set"
      ],
      "metadata": {
        "id": "EfXD36r91arN"
      },
      "id": "EfXD36r91arN"
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_no_validation, train_data_validation, train_data_no_validation_labels, train_data_validation_labels = train_test_split(\n",
        "    train_data,\n",
        "    train_labels,\n",
        "    test_size = 0.2,\n",
        ")\n",
        "# only train_data_validation and train_data_validation_labels needed"
      ],
      "metadata": {
        "id": "kJ8o_BX00ock"
      },
      "id": "kJ8o_BX00ock",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cef295c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:42.510580Z",
          "iopub.status.busy": "2021-07-25T13:21:42.509856Z",
          "iopub.status.idle": "2021-07-25T13:21:42.515158Z",
          "shell.execute_reply": "2021-07-25T13:21:42.515651Z",
          "shell.execute_reply.started": "2021-07-25T13:20:28.142992Z"
        },
        "papermill": {
          "duration": 0.040807,
          "end_time": "2021-07-25T13:21:42.515830",
          "exception": false,
          "start_time": "2021-07-25T13:21:42.475023",
          "status": "completed"
        },
        "tags": [],
        "id": "5cef295c"
      },
      "outputs": [],
      "source": [
        "anomalous_train_data = train_data[train_labels] #training data with outlier-label == True\n",
        "anomalous_test_data = test_data[test_labels] #test data with outlier-label == True\n",
        "\n",
        "normal_train_data = train_data[~train_labels] #training data with outlier-label == False\n",
        "normal_test_data = test_data[~test_labels] #test data with outlier-label == False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for k in [train_data, train_labels, train_data_validation, train_data_validation_labels]: \n",
        " print(k.shape)"
      ],
      "metadata": {
        "id": "EGsGaiP01R7w"
      },
      "id": "EGsGaiP01R7w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in [train_data, test_data, train_labels, test_labels]: \n",
        " print(k.shape)"
      ],
      "metadata": {
        "id": "vZhjp7VNZIMr"
      },
      "id": "vZhjp7VNZIMr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in [anomalous_train_data, anomalous_test_data, normal_train_data, normal_test_data]: \n",
        " print(k.shape)"
      ],
      "metadata": {
        "id": "rOIRuAXJngZm"
      },
      "id": "rOIRuAXJngZm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Anomaly Share for training data equals ={:10.4f}\".format(len(anomalous_train_data)/len(train_data)))\n",
        "print(\"Anomaly Share for test data equals ={:10.4f}\".format(len(anomalous_test_data)/len(test_data)))"
      ],
      "metadata": {
        "id": "b9uGwVVTnr6j"
      },
      "id": "b9uGwVVTnr6j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "41902a32",
      "metadata": {
        "papermill": {
          "duration": 0.032604,
          "end_time": "2021-07-25T13:21:42.580702",
          "exception": false,
          "start_time": "2021-07-25T13:21:42.548098",
          "status": "completed"
        },
        "tags": [],
        "id": "41902a32"
      },
      "source": [
        "# Train the model with train & validation data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Randomized autoencoders\n",
        "\n",
        "Die Implementierung für die RandAE-Klasse stammt aus folgendem [Repository](https://github.com/danieltsoukup/autoencoders):"
      ],
      "metadata": {
        "id": "PKnXqpbJfU8j"
      },
      "id": "PKnXqpbJfU8j"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aktivierungsfunktionen\n",
        "\n",
        "*   Hidden Layer: Relu\n",
        "*   Output Layer: Sigmoid\n",
        "\n",
        "Es wird ein Drop_ratio von 0.5 festgelegt. Ein Drop_ratio von 0.0 entspricht einer *fully connected architecture*\n",
        "\n"
      ],
      "metadata": {
        "id": "AYCNoY07UZF3"
      },
      "id": "AYCNoY07UZF3"
    },
    {
      "cell_type": "code",
      "source": [
        "class RandAE(tf.keras.Sequential):\n",
        "    def __init__(self, input_dim, hidden_dims, drop_ratio=0.5, **kwargs):\n",
        "        super(RandAE, self).__init__(**kwargs)\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.drop_ratio = drop_ratio\n",
        "        \n",
        "        self.layer_masks = dict()\n",
        "        \n",
        "        self.build_model()\n",
        "                \n",
        "    def build_model(self) -> None:\n",
        "        \"\"\"\n",
        "        Adds the layers and records masks.\n",
        "        \"\"\"\n",
        "        \n",
        "        self.add(layers.Input(self.input_dim, name=\"input\"))\n",
        "        \n",
        "        for i, dim in enumerate(self.hidden_dims):\n",
        "            layer_name = f\"hidden_{i}\"\n",
        "            layer = layers.Dense(dim, \n",
        "                                 activation=\"relu\" if i > 0 else \"sigmoid\", \n",
        "                                 name=layer_name)\n",
        "            self.add(layer)\n",
        "            \n",
        "            # add layer mask\n",
        "            self.layer_masks[layer_name] = self.get_mask(layer)\n",
        "        \n",
        "        layer_name = \"output\"\n",
        "        output_layer = layers.Dense(self.input_dim, activation=\"sigmoid\", name=layer_name)\n",
        "        self.add(output_layer)\n",
        "        self.layer_masks[layer_name] = self.get_mask(output_layer)\n",
        "            \n",
        "    def get_mask(self, layer) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Build mask for a layer.\n",
        "        \"\"\"\n",
        "        \n",
        "        shape = layer.input_shape[1], layer.output_shape[1]\n",
        "        \n",
        "        return np.random.choice([0., 1.], size=shape, p=[self.drop_ratio, 1-self.drop_ratio])\n",
        "        \n",
        "    def load_masks(self, mask_pickle_path) -> None:\n",
        "        \"\"\"\n",
        "        Load the masks from a pickled dictionary.\n",
        "        \"\"\"\n",
        "        \n",
        "        with open(mask_pickle_path, 'rb') as handle:\n",
        "            self.layer_masks = pickle.load(handle)    \n",
        "            \n",
        "    def get_encoder(self) -> keras.Sequential:\n",
        "        \"\"\"\n",
        "        Get the encoder from the full model.\n",
        "        \"\"\"\n",
        "        \n",
        "        n_layers = (len(self.hidden_dims)+1)//2\n",
        "        encoder_layers = [layers.Input(self.input_dim)] + self.layers[:n_layers]\n",
        "\n",
        "        return keras.Sequential(encoder_layers)\n",
        "        \n",
        "    \n",
        "    def mask_weights(self) -> None:\n",
        "        \"\"\"\n",
        "        Apply the masks to each layer in the encoder and decoder.\n",
        "        \"\"\"\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            layer_name = layer.name\n",
        "            if layer_name in self.layer_masks:\n",
        "                masked_w = layer.weights[0].numpy()*self.layer_masks[layer_name]\n",
        "                b = layer.weights[1].numpy()\n",
        "                layer.set_weights((masked_w, b))        \n",
        "\n",
        "    def call(self, data, training=True) -> tf.Tensor:\n",
        "        \n",
        "        # mask the weights before original forward pass\n",
        "        self.mask_weights()\n",
        "        \n",
        "        return super().call(data)"
      ],
      "metadata": {
        "id": "SvaT6tdyfYp3"
      },
      "id": "SvaT6tdyfYp3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_mse = RandAE(9,[24,12,6,3,6,12,24])\n",
        "model_mae = RandAE(9,[24,12,6,3,6,12,24])\n",
        "model_ce = RandAE(9,[24,12,6,3,6,12,24])\n",
        "model_focal = RandAE(9,[24,12,6,3,6,12,24])\n",
        "model_huber = RandAE(9,[24,12,6,3,6,12,24])"
      ],
      "metadata": {
        "id": "AGxkc_txfkAN"
      },
      "id": "AGxkc_txfkAN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "73sIBiifn4rk"
      },
      "id": "73sIBiifn4rk"
    },
    {
      "cell_type": "code",
      "source": [
        "#load checkpoints\n",
        "# checkpoint_mse = tf.keras.callbacks.ModelCheckpoint(\"checkpoint_mse\",\n",
        "#                                                    save_weights_only=True) #generate checkpoints\n",
        "\n",
        "\n",
        "#define early stopping\n",
        "early_stopping_mse = tf.keras.callbacks.EarlyStopping(patience=20,\n",
        "                                                     restore_best_weights=True) \n",
        "\n",
        "\n",
        "\n",
        "                                                                                                                                                       \n",
        "model_mse.compile(loss=\"mean_squared_error\", optimizer=\"adam\", run_eagerly=True)\n",
        "\n",
        "\n",
        "history_mse = model_mse.fit(train_data,\n",
        "                    train_data,\n",
        "                    batch_size=16,\n",
        "                    epochs = 1000,\n",
        "                    validation_data=(train_data_validation, train_data_validation),\n",
        "                    callbacks=[\n",
        "                        # checkpoint_mse,\n",
        "                        early_stopping_mse,\n",
        "                        ],\n",
        "                    verbose = 0,\n",
        "                    validation_batch_size = 16,\n",
        "                    shuffle=True)\n",
        "model_mse.summary()"
      ],
      "metadata": {
        "id": "cES2dR_t2ms8"
      },
      "id": "cES2dR_t2ms8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint_mae = tf.keras.callbacks.ModelCheckpoint(\"checkpoint_mae\",\n",
        "#                                                    save_weights_only=True) #generate checkpoints\n",
        "early_stopping_mae = tf.keras.callbacks.EarlyStopping(patience=20,\n",
        "                                                     restore_best_weights=True) #define early stopping                                                  \n",
        "model_mae.compile(loss=\"mean_absolute_error\", optimizer=\"adam\", run_eagerly=True)\n",
        "history_mae = model_mae.fit(train_data,\n",
        "                    train_data,\n",
        "                    batch_size=16,\n",
        "                    epochs = 1000,\n",
        "                    validation_data=(train_data_validation, train_data_validation),\n",
        "                    callbacks=[\n",
        "                        # checkpoint_mae,\n",
        "                        early_stopping_mae],\n",
        "                    verbose = 0,\n",
        "                    validation_batch_size = 16,\n",
        "                    shuffle=True)\n",
        "model_mae.summary()"
      ],
      "metadata": {
        "id": "Z4g9XkAMOxNy"
      },
      "id": "Z4g9XkAMOxNy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# checkpoint_ce = tf.keras.callbacks.ModelCheckpoint(\"checkpoint_ce\",\n",
        "#                                                    save_weights_only=True) #generate checkpoints\n",
        "early_stopping_ce = tf.keras.callbacks.EarlyStopping(patience=20,\n",
        "                                                     restore_best_weights=True) #define early stopping  \n",
        "\n",
        "model_ce.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",  run_eagerly=True)\n",
        "\n",
        "history_ce = model_ce.fit(train_data,\n",
        "                    train_data,\n",
        "                    batch_size=16,\n",
        "                    epochs = 1000,\n",
        "                    validation_data=(train_data_validation, train_data_validation),\n",
        "                    callbacks=[\n",
        "                        # checkpoint_ce,\n",
        "                        early_stopping_ce],\n",
        "                    verbose = 0,\n",
        "                    validation_batch_size = 16,\n",
        "                    shuffle=True)\n",
        "model_ce.summary()"
      ],
      "metadata": {
        "id": "-7dHzwjCOxwn"
      },
      "id": "-7dHzwjCOxwn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Der Focal Loss wird mit γ = 2 initalisiert.\n",
        "\n",
        "[Quelle](https://github.com/qubvel/segmentation_models/blob/master/segmentation_models/losses.py)"
      ],
      "metadata": {
        "id": "j0zI_qD6bgCB"
      },
      "id": "j0zI_qD6bgCB"
    },
    {
      "cell_type": "code",
      "source": [
        "focal_loss = sm.losses.BinaryFocalLoss()\n",
        "# checkpoint_focal = tf.keras.callbacks.ModelCheckpoint(\"checkpoint_focal\",\n",
        "#                                                    save_weights_only=True) #generate checkpoints\n",
        "early_stopping_focal = tf.keras.callbacks.EarlyStopping(patience=20,\n",
        "                                                     restore_best_weights=True) #define early stopping  \n",
        "\n",
        "model_focal.compile(loss=focal_loss, optimizer=\"adam\",  run_eagerly=True)\n",
        "\n",
        "\n",
        "history_focal = model_focal.fit(train_data,\n",
        "                                train_data,\n",
        "                                batch_size=16,\n",
        "                                epochs = 1000,\n",
        "                    validation_data=(train_data_validation, train_data_validation),\n",
        "                    callbacks=[\n",
        "                        # checkpoint_focal,\n",
        "                        early_stopping_focal],\n",
        "                    verbose = 0,\n",
        "                    validation_batch_size = 16,\n",
        "                    shuffle=True)\n",
        "model_focal.summary()"
      ],
      "metadata": {
        "id": "hxnX31oHOyML"
      },
      "id": "hxnX31oHOyML",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Huber wird mit δ = 0,5 initalisiert"
      ],
      "metadata": {
        "id": "JYwQPlllYwT5"
      },
      "id": "JYwQPlllYwT5"
    },
    {
      "cell_type": "code",
      "source": [
        "huber = tf.keras.losses.Huber(delta= 0.5, reduction=\"auto\", name=\"huber_loss\")\n",
        "# checkpoint_huber = tf.keras.callbacks.ModelCheckpoint(\"checkpoint_huber\",\n",
        "#                                                    save_weights_only=True) #generate checkpoints\n",
        "early_stopping_huber = tf.keras.callbacks.EarlyStopping(patience=20,\n",
        "                                                     restore_best_weights=True) #define early stopping  \n",
        "\n",
        "model_huber.compile(loss=huber, optimizer=\"adam\",  run_eagerly=True) \n",
        "\n",
        "history_huber = model_huber.fit(train_data,\n",
        "                    train_data,\n",
        "                    batch_size=16,\n",
        "                    epochs = 1000,\n",
        "                    validation_data=(train_data_validation, train_data_validation),\n",
        "                    callbacks=[\n",
        "                        #checkpoint_huber,\n",
        "                        early_stopping_huber],\n",
        "                    verbose = 0,\n",
        "                    validation_batch_size = 16,\n",
        "                    shuffle=True)\n",
        "model_huber.summary()"
      ],
      "metadata": {
        "id": "Vq_hbVqLOyhb"
      },
      "id": "Vq_hbVqLOyhb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotte den Trainingsverlauf für jedes Modell"
      ],
      "metadata": {
        "id": "-bv_TOU8V-8E"
      },
      "id": "-bv_TOU8V-8E"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_performance():\n",
        "    '''\n",
        "    Plot the training performance of each model \n",
        "    '''\n",
        "    figure, axis = plt.subplots(ncols=1,nrows=5, figsize=(10, 20))\n",
        "    # first row: complicated architecture\n",
        "    axis[0].plot(history_mse.history[\"loss\"], label=\"Training Loss\")\n",
        "    axis[0].plot(history_mse.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "    axis[0].set_title('Performance Training with model_mse')\n",
        "    axis[0].set(xlabel='Epochs', ylabel='Loss')\n",
        "    axis[0].legend()\n",
        "    \n",
        "    axis[1].plot(history_mae.history[\"loss\"], label=\"Training Loss\")\n",
        "    axis[1].plot(history_mae.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "    axis[1].set_title('Performance Training with model_mae')\n",
        "    axis[1].set(xlabel='Epochs', ylabel='Loss')\n",
        "    axis[1].legend()\n",
        "    \n",
        "    axis[2].plot(history_huber.history[\"loss\"], label=\"Training Loss\")\n",
        "    axis[2].plot(history_huber.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "    axis[2].set_title('Performance Training with model_huber')\n",
        "    axis[2].set(xlabel='Epochs', ylabel='Loss')\n",
        "    axis[2].legend()\n",
        "    \n",
        "    #second row: simple architectures \n",
        "    axis[3].plot(history_ce.history[\"loss\"], label=\"Training Loss\")\n",
        "    axis[3].plot(history_ce.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "    axis[3].set_title('Performance Training by model_ce')\n",
        "    axis[3].set(xlabel='Epochs', ylabel='Loss')\n",
        "    axis[3].legend()\n",
        "    \n",
        "    axis[4].plot(history_focal.history[\"loss\"], label=\"Training Loss\")\n",
        "    axis[4].plot(history_focal.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "    axis[4].set_title('Performance Training by model_focal')\n",
        "    axis[4].set(xlabel='Epochs', ylabel='Loss')\n",
        "    axis[4].legend()\n",
        "\n",
        "    save_fig(\"Performances for training\")\n",
        "    plt.show()\n",
        "\n",
        "plot_training_performance()"
      ],
      "metadata": {
        "id": "uDAa0dQZ3eBt"
      },
      "id": "uDAa0dQZ3eBt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Diese Funktion extrahiert die letzte ausgeführte Epoche für ein Model"
      ],
      "metadata": {
        "id": "vuHx_Q-XWG__"
      },
      "id": "vuHx_Q-XWG__"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_last_epoch(history, max = \"max\") -> int:\n",
        "  epochs = np.array([])\n",
        "  stopped_epoch = np.array([])\n",
        "  for key, value in enumerate(history.history[\"loss\"]):\n",
        "    epochs = np.append(epochs,key)\n",
        "  stopped_epoch = np.append(stopped_epoch,int(np.max(epochs)))\n",
        "  if max == \"max\":\n",
        "    return int(np.max(epochs))\n",
        "  else:\n",
        "    return epochs\n"
      ],
      "metadata": {
        "id": "ORFJfp_z2g-_"
      },
      "id": "ORFJfp_z2g-_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for history in [\n",
        "history_mse,\n",
        "    history_mae,\n",
        "    history_ce,\n",
        "    history_focal,\n",
        "    history_huber ]:\n",
        "    print(get_last_epoch(history))"
      ],
      "metadata": {
        "id": "8E7sn_dipW0a"
      },
      "id": "8E7sn_dipW0a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotte den TNSE-Graph für den latenten Raum in einem Modell"
      ],
      "metadata": {
        "id": "N4DXnaMkWe86"
      },
      "id": "N4DXnaMkWe86"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_tnse_performance_bottleneck_layer():\n",
        "    '''\n",
        "    Plot the training performance of each model \n",
        "    '''\n",
        "    figure, axis = plt.subplots(ncols=1,nrows=5, figsize=(10, 20))\n",
        "    # first row: complicated architecture\n",
        "    encoder = model_mse.get_encoder()\n",
        "    vis_data_latent = encoder.predict(train_data)\n",
        "    tsne = TSNE(n_components=2)\n",
        "    tsne_data = tsne.fit_transform(vis_data_latent)\n",
        "    axis[0].scatter(tsne_data[train_labels == 0, 0], \n",
        "            tsne_data[train_labels == 0, 1], c=\"grey\", alpha=0.1, label=\"inlier\")\n",
        "    axis[0].scatter(tsne_data[train_labels == 1, 0], \n",
        "            tsne_data[train_labels == 1, 1], c=\"crimson\", alpha=1, label=\"outlier\")\n",
        "    axis[0].set_title('t-SNE for outliers/inliers on the latent manifold for model_mse')\n",
        "    axis[0].legend()\n",
        "    \n",
        "    encoder = model_mae.get_encoder()\n",
        "    vis_data_latent = encoder.predict(train_data)\n",
        "    tsne = TSNE(n_components=2)\n",
        "    tsne_data = tsne.fit_transform(vis_data_latent)\n",
        "    axis[1].scatter(tsne_data[train_labels == 0, 0], \n",
        "            tsne_data[train_labels == 0, 1], c=\"grey\", alpha=0.1, label=\"inlier\")\n",
        "    axis[1].scatter(tsne_data[train_labels == 1, 0], \n",
        "            tsne_data[train_labels == 1, 1], c=\"crimson\", alpha=1, label=\"outlier\")\n",
        "    axis[1].set_title('t-SNE for outliers/inliers on the latent manifold for model_mae')\n",
        "    axis[1].legend()\n",
        "    \n",
        "    encoder = model_huber.get_encoder()\n",
        "    vis_data_latent = encoder.predict(train_data)\n",
        "    tsne = TSNE(n_components=2)\n",
        "    tsne_data = tsne.fit_transform(vis_data_latent)\n",
        "    axis[2].scatter(tsne_data[train_labels == 0, 0], \n",
        "            tsne_data[train_labels == 0, 1], c=\"grey\", alpha=0.1, label=\"inlier\")\n",
        "    axis[2].scatter(tsne_data[train_labels == 1, 0], \n",
        "            tsne_data[train_labels == 1, 1], c=\"crimson\", alpha=1, label=\"outlier\")\n",
        "    axis[2].set_title('t-SNE for outliers/inliers on the latent manifold for model_huber')\n",
        "    axis[2].legend()\n",
        "    \n",
        "    encoder = model_ce.get_encoder()\n",
        "    vis_data_latent = encoder.predict(train_data)\n",
        "    tsne = TSNE(n_components=2)\n",
        "    tsne_data = tsne.fit_transform(vis_data_latent)\n",
        "    axis[3].scatter(tsne_data[train_labels == 0, 0], \n",
        "            tsne_data[train_labels == 0, 1], c=\"grey\", alpha=0.1, label=\"inlier\")\n",
        "    axis[3].scatter(tsne_data[train_labels == 1, 0], \n",
        "            tsne_data[train_labels == 1, 1], c=\"crimson\", alpha=1, label=\"outlier\")\n",
        "    axis[3].set_title('t-SNE for outliers/inliers on the latent manifold for model_ce')\n",
        "    axis[3].legend()\n",
        "    \n",
        "    encoder = model_focal.get_encoder()\n",
        "    vis_data_latent = encoder.predict(train_data)\n",
        "    tsne = TSNE(n_components=2)\n",
        "    tsne_data = tsne.fit_transform(vis_data_latent)\n",
        "    axis[4].scatter(tsne_data[train_labels == 0, 0], \n",
        "            tsne_data[train_labels == 0, 1], c=\"grey\", alpha=0.1, label=\"inlier\")\n",
        "    axis[4].scatter(tsne_data[train_labels == 1, 0], \n",
        "            tsne_data[train_labels == 1, 1], c=\"crimson\", alpha=1, label=\"outlier\")\n",
        "    axis[4].set_title('t-SNE for outliers/inliers on the latent manifold for model_focal')\n",
        "    axis[4].legend()\n",
        "\n",
        "    save_fig(\"TNSE-Distrubution in bottleneck-layer for all models\")\n",
        "    plt.show()\n",
        "\n",
        "plot_tnse_performance_bottleneck_layer()"
      ],
      "metadata": {
        "id": "5nHCyBoqZz_L"
      },
      "id": "5nHCyBoqZz_L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation on test data"
      ],
      "metadata": {
        "id": "84g4adTVfcmC"
      },
      "id": "84g4adTVfcmC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generiere den Rekonstruktionsfehler"
      ],
      "metadata": {
        "id": "0xn6JL8yjgU8"
      },
      "id": "0xn6JL8yjgU8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Um den Rekonstruktionsfehler zu berechnen, wird als Metrik der MSE verwendet."
      ],
      "metadata": {
        "id": "_izo1C8vjVCt"
      },
      "id": "_izo1C8vjVCt"
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_error_df(model, metric:str = \"mean_squared_error\", data = test_data):\n",
        "  if model in [model_mse,model_mae,model_ce,model_focal,model_huber]:\n",
        "    print(\"Calculate reconstruction error for model: \", model)\n",
        "    if metric == \"mean_squared_error\":\n",
        "      mse = np.mean(np.power(data - model.predict(data), 2), axis=1)\n",
        "      return(pd.DataFrame({'reconstruction_error': mse,\n",
        "                              'anomaly': test_labels}))\n",
        "    else:\n",
        "      print(\"No further metric functions are implemented yet\")\n",
        "      return\n",
        "  else:\n",
        "    raise(\"Choosen model is not supported yet. Please choose an model in [model_mse,model_mae,model_ce,model_focal,model_huber]\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L0PLrhpKbBXz"
      },
      "id": "L0PLrhpKbBXz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse_error_df = gen_error_df(model_mse)\n",
        "mae_error_df = gen_error_df(model_mae)\n",
        "ce_error_df = gen_error_df(model_ce)\n",
        "focal_error_df = gen_error_df(model_focal)\n",
        "huber_error_df = gen_error_df(model_huber)"
      ],
      "metadata": {
        "id": "Uq-KLMRCBXBF"
      },
      "id": "Uq-KLMRCBXBF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ROC und AUC"
      ],
      "metadata": {
        "id": "BkPb4nFSj6hZ"
      },
      "id": "BkPb4nFSj6hZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualisiere das Ergebnis für die Testdaten für alle Modelle"
      ],
      "metadata": {
        "id": "aBEolzL2kucr"
      },
      "id": "aBEolzL2kucr"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_test_performance_roc(error_df = [mse_error_df,\n",
        "                                          mae_error_df,\n",
        "                                          huber_error_df,\n",
        "                                          ce_error_df,\n",
        "                                          focal_error_df]):\n",
        "    '''\n",
        "    Plot the ROC curve performance of each model \n",
        "    '''\n",
        "    figure, axis = plt.subplots(ncols=1,nrows=5, figsize=(10, 20))\n",
        "    # first row: complicated architecture\n",
        "    fpr, tpr, thresholds = roc_curve(error_df[0].anomaly, error_df[0].reconstruction_error)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    axis[0].plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\n",
        "    axis[0].plot([0,1],[0,1],'r--')\n",
        "    axis[0].legend(loc='lower right')\n",
        "    axis[0].set_title('Receiver Operating Characteristic for Model_MSE')\n",
        "    axis[0].set_xlim([-0.001, 1])\n",
        "    axis[0].set_ylim([0, 1.001])\n",
        "    axis[0].set(xlabel='False Positive Rate', ylabel='True Positive Rate')\n",
        "    axis[0].legend()\n",
        "    \n",
        "    fpr, tpr, thresholds = roc_curve(error_df[1].anomaly, error_df[1].reconstruction_error)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    axis[1].plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\n",
        "    axis[1].plot([0,1],[0,1],'r--')\n",
        "    axis[1].legend(loc='lower right')\n",
        "    axis[1].set_title('Receiver Operating Characteristic for Model_MAE')\n",
        "    axis[1].set_xlim([-0.001, 1])\n",
        "    axis[1].set_ylim([0, 1.001])\n",
        "    axis[1].set(xlabel='False Positive Rate', ylabel='True Positive Rate')\n",
        "    axis[1].legend()\n",
        "    \n",
        "    fpr, tpr, thresholds = roc_curve(error_df[2].anomaly, error_df[2].reconstruction_error)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    axis[2].plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\n",
        "    axis[2].plot([0,1],[0,1],'r--')\n",
        "    axis[2].legend(loc='lower right')\n",
        "    axis[2].set_title('Receiver Operating Characteristic for Model_Huber')\n",
        "    axis[2].set_xlim([-0.001, 1])\n",
        "    axis[2].set_ylim([0, 1.001])\n",
        "    axis[2].set(xlabel='False Positive Rate', ylabel='True Positive Rate')\n",
        "    axis[2].legend()\n",
        "    \n",
        "    #second row: simple architectures \n",
        "    fpr, tpr, thresholds = roc_curve(error_df[3].anomaly, error_df[3].reconstruction_error)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    axis[3].plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\n",
        "    axis[3].plot([0,1],[0,1],'r--')\n",
        "    axis[3].legend(loc='lower right')\n",
        "    axis[3].set_title('Receiver Operating Characteristic for Model_CE')\n",
        "    axis[3].set_xlim([-0.001, 1])\n",
        "    axis[3].set_ylim([0, 1.001])\n",
        "    axis[3].set(xlabel='False Positive Rate', ylabel='True Positive Rate')\n",
        "    axis[3].legend()\n",
        "    \n",
        "    fpr, tpr, thresholds = roc_curve(error_df[4].anomaly, error_df[4].reconstruction_error)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    axis[4].plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\n",
        "    axis[4].plot([0,1],[0,1],'r--')\n",
        "    axis[4].legend(loc='lower right')\n",
        "    axis[4].set_title('Receiver Operating Characteristic for Model_Focal')\n",
        "    axis[4].set_xlim([-0.001, 1])\n",
        "    axis[4].set_ylim([0, 1.001])\n",
        "    axis[4].set(xlabel='False Positive Rate', ylabel='True Positive Rate')\n",
        "    axis[4].legend()\n",
        "\n",
        "\n",
        "    save_fig(\"Roc-Curves for test data\")\n",
        "    plt.show()\n",
        "\n",
        "plot_test_performance_roc()"
      ],
      "metadata": {
        "id": "dbo0o_rvwZ-T"
      },
      "id": "dbo0o_rvwZ-T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Precision, Recall und F1-Score\n",
        "\n",
        "Da diese Metriken davon abhängig sind, welche Threshold man für die Klassifikation wählt, wird die Threshold wie folgt gewählt, um den F1-Score zu maximieren:\n",
        "\n",
        "\n",
        "*   Bestimme die Threshold, für die die Precision und der Recall gleich sind\n",
        "*   Da es sich bei dem F1-Score um ein harmonisches Mittel handelt, wird so der F1-Score maximiert\n",
        "\n"
      ],
      "metadata": {
        "id": "fda09jLOkFj3"
      },
      "id": "fda09jLOkFj3"
    },
    {
      "cell_type": "code",
      "source": [
        "prec, recall, thresholds = precision_recall_curve(focal_error_df[\"anomaly\"], focal_error_df[\"reconstruction_error\"])\n",
        "\n",
        "best_idx = np.argmin(np.abs(prec-recall)[:int(len(np.abs(prec-recall))*-0.1)]) #to resolve an bug, the last four obsvervations have to be excluded\n",
        "best_prec, best_recall = prec[best_idx], recall[best_idx]\n",
        "print(f\"Best precision {np.round(best_prec, 2)}, \"\\\n",
        "      f\"recall: {np.round(best_recall, 2)} at {np.round(thresholds[best_idx], 2)} threshold.\")"
      ],
      "metadata": {
        "id": "Qncty3ipkCyc"
      },
      "id": "Qncty3ipkCyc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "auc_score = auc(recall, prec)\n",
        "auc_score"
      ],
      "metadata": {
        "id": "X18MiBKgtuoj"
      },
      "id": "X18MiBKgtuoj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualisiere das Ergebnis für alle Ergebnisse"
      ],
      "metadata": {
        "id": "cHwMS5yjkqVZ"
      },
      "id": "cHwMS5yjkqVZ"
    },
    {
      "cell_type": "code",
      "source": [
        "## Recall Vs Precision\n",
        "def plot_test_performance_recall_vs_precision(error_df = [mse_error_df,\n",
        "                                          mae_error_df,\n",
        "                                          huber_error_df,\n",
        "                                          ce_error_df,\n",
        "                                          focal_error_df]):\n",
        "    figure, axis = plt.subplots(ncols=2,nrows=5, figsize=(30, 30))\n",
        "    # first row: complicated architecture\n",
        "    precision, recall, thresholds = precision_recall_curve(error_df[0].anomaly, error_df[0].reconstruction_error)\n",
        "    best_idx = np.argmin(np.abs(precision-recall)[:-4])\n",
        "    best_prec, best_recall = precision[best_idx], recall[best_idx]\n",
        "    precision_recall_auc = auc(recall, precision)\n",
        "    axis[0,0].plot(recall, precision, label='AUC = %0.4f'% precision_recall_auc)\n",
        "    axis[0,0].legend(loc='lower right')\n",
        "    axis[0,0].set_title('Recall vs Precision for Model_MSE')\n",
        "    axis[0,0].set(xlabel='Recall', ylabel='Precision')\n",
        "    axis[0,1].set_title('Determine the threshold for Model_MSE')\n",
        "    axis[0,1].plot(thresholds, precision[:-1], label=\"precision\")\n",
        "    axis[0,1].plot(thresholds, recall[:-1], label=\"recall\")\n",
        "    axis[0,1].axvline(thresholds[best_idx], 0, 1, c=\"grey\", linestyle=\"--\")\n",
        "    axis[0,1].set_xlabel(\"thresholds\")\n",
        "    axis[0,1].legend()\n",
        "\n",
        "    precision, recall, thresholds = precision_recall_curve(error_df[1].anomaly, error_df[1].reconstruction_error)\n",
        "    best_idx = np.argmin(np.abs(precision-recall)[:-4])\n",
        "    best_prec, best_recall = precision[best_idx], recall[best_idx]\n",
        "    precision_recall_auc = auc(recall, precision)\n",
        "    axis[1,0].plot(recall, precision, label='AUC = %0.4f'% precision_recall_auc)\n",
        "    axis[1,0].legend(loc='lower right')\n",
        "    axis[1,0].set_title('Recall vs Precision for Model_MAE')\n",
        "    axis[1,0].set(xlabel='Recall', ylabel='Precision')\n",
        "    axis[1,1].set_title('Determine the threshold for Model_MAE')\n",
        "    axis[1,1].plot(thresholds, precision[:-1], label=\"precision\")\n",
        "    axis[1,1].plot(thresholds, recall[:-1], label=\"recall\")\n",
        "    axis[1,1].axvline(thresholds[best_idx], 0, 1, c=\"grey\", linestyle=\"--\")\n",
        "    axis[1,1].set_xlabel(\"thresholds\")\n",
        "    axis[1,1].legend()\n",
        "\n",
        "    precision, recall, thresholds = precision_recall_curve(error_df[2].anomaly, error_df[2].reconstruction_error)\n",
        "    best_idx = np.argmin(np.abs(precision-recall)[:-4])\n",
        "    best_prec, best_recall = precision[best_idx], recall[best_idx]\n",
        "    precision_recall_auc = auc(recall, precision)\n",
        "    axis[2,0].plot(recall, precision, label='AUC = %0.4f'% precision_recall_auc)\n",
        "    axis[2,0].legend(loc='lower right')\n",
        "    axis[2,0].set_title('Recall vs Precision for Model_Huber')\n",
        "    axis[2,0].set(xlabel='Recall', ylabel='Precision')\n",
        "    axis[2,1].set_title('Determine the threshold for Model_Huber')\n",
        "    axis[2,1].plot(thresholds, precision[:-1], label=\"precision\")\n",
        "    axis[2,1].plot(thresholds, recall[:-1], label=\"recall\")\n",
        "    axis[2,1].axvline(thresholds[best_idx], 0, 1, c=\"grey\", linestyle=\"--\")\n",
        "    axis[2,1].set_xlabel(\"thresholds\")\n",
        "    axis[2,1].legend()\n",
        "\n",
        "    precision, recall, thresholds = precision_recall_curve(error_df[3].anomaly, error_df[3].reconstruction_error)\n",
        "    best_idx = np.argmin(np.abs(precision-recall)[:-4])\n",
        "    best_prec, best_recall = precision[best_idx], recall[best_idx]\n",
        "    precision_recall_auc = auc(recall, precision)\n",
        "    axis[3,0].plot(recall, precision, label='AUC = %0.4f'% precision_recall_auc)\n",
        "    axis[3,0].legend(loc='lower right')\n",
        "    axis[3,0].set_title('Recall vs Precision for Model_CE')\n",
        "    axis[3,0].set(xlabel='Recall', ylabel='Precision')\n",
        "    axis[3,1].set_title('Determine the threshold for Model_CE')\n",
        "    axis[3,1].plot(thresholds, precision[:-1], label=\"precision\")\n",
        "    axis[3,1].plot(thresholds, recall[:-1], label=\"recall\")\n",
        "    axis[3,1].axvline(thresholds[best_idx], 0, 1, c=\"grey\", linestyle=\"--\")\n",
        "    axis[3,1].set_xlabel(\"thresholds\")\n",
        "    axis[3,1].legend()\n",
        "\n",
        "    precision, recall, thresholds = precision_recall_curve(error_df[4].anomaly, error_df[4].reconstruction_error)\n",
        "    best_idx = np.argmin(np.abs(precision-recall)[:-4])\n",
        "    best_prec, best_recall = precision[best_idx], recall[best_idx]\n",
        "    precision_recall_auc = auc(recall, precision)\n",
        "    axis[4,0].plot(recall, precision, label='AUC = %0.4f'% precision_recall_auc)\n",
        "    axis[4,0].legend(loc='lower right')\n",
        "    axis[4,0].set_title('Recall vs Precision for Model_FL')\n",
        "    axis[4,0].set(xlabel='Recall', ylabel='Precision')\n",
        "    axis[4,1].set_title('Determine the threshold for Model_Focal')\n",
        "    axis[4,1].plot(thresholds, precision[:-1], label=\"precision\")\n",
        "    axis[4,1].plot(thresholds, recall[:-1], label=\"recall\")\n",
        "    axis[4,1].axvline(thresholds[best_idx], 0, 1, c=\"grey\", linestyle=\"--\")\n",
        "    axis[4,1].set_xlabel(\"thresholds\")\n",
        "    axis[4,1].legend()\n",
        "\n",
        "    save_fig(\"Recall vs Precision curves for test data\")\n",
        "    sns.despine()\n",
        "    plt.show()\n",
        "\n",
        "plot_test_performance_recall_vs_precision()"
      ],
      "metadata": {
        "id": "fUY3nZZYg3ty"
      },
      "id": "fUY3nZZYg3ty",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_precision_threshold(error_df):\n",
        "  '''\n",
        "  Return an array, which includes the thresholds with the second highest precision score for each model\n",
        "  '''\n",
        "  prec, recall, threshold = precision_recall_curve(error_df[\"anomaly\"], error_df[\"reconstruction_error\"])\n",
        "  best_idx = np.argmin(np.abs(prec-recall)[:-4])\n",
        "  best_prec, best_recall = prec[best_idx], recall[best_idx]\n",
        "\n",
        "  return threshold[best_idx]\n",
        "\n",
        "for i in [mse_error_df, mae_error_df, huber_error_df, ce_error_df, focal_error_df]:   \n",
        "  print(get_precision_threshold(i))"
      ],
      "metadata": {
        "id": "pQXAUcPkbAd-"
      },
      "id": "pQXAUcPkbAd-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_thresholds_data(index:int,save_name:str,error_df = [mse_error_df,\n",
        "                                          mae_error_df,\n",
        "                                          huber_error_df,\n",
        "                                          ce_error_df,\n",
        "                                          focal_error_df]):\n",
        "  if index > len(error_df) - 1 or index < 0:\n",
        "    raise(\"Index out of range implemented. Please correct your statement\")\n",
        "  else:\n",
        "    groups = error_df[index].groupby('anomaly')\n",
        "    threshold = get_precision_threshold(error_df[index])\n",
        "    fig, ax = plt.subplots()\n",
        "    for name, group in groups:\n",
        "      ax.plot(group.index, group.reconstruction_error, marker='o', ms=3.5, linestyle='',\n",
        "              label= \"Anomaly\" if name == 1 else \"Normal\")\n",
        "    ax.hlines(threshold, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", zorder=100, label='Threshold')\n",
        "    ax.legend()\n",
        "    plt.title(\"Reconstruction error visualisation to identify anomalies\")\n",
        "    plt.ylabel(\"Reconstruction error\")\n",
        "    plt.xlabel(\"Data point index\")\n",
        "    save_fig(save_name)\n",
        "    plt.show();\n"
      ],
      "metadata": {
        "id": "7irw2FnvhxlO"
      },
      "id": "7irw2FnvhxlO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_thresholds_data(0, \"Reconstruction error visualisation to identify anomalies with model_mse\")\n",
        "plot_thresholds_data(1, \"Reconstruction error visualisation to identify anomalies with model_mae\")\n",
        "plot_thresholds_data(2, \"Reconstruction error visualisation to identify anomalies with model_huber\")\n",
        "plot_thresholds_data(3, \"Reconstruction error visualisation to identify anomalies with model_ce\")\n",
        "plot_thresholds_data(4, \"Reconstruction error visualisation to identify anomalies with model_focal\")"
      ],
      "metadata": {
        "id": "fNyor5Gpl986"
      },
      "id": "fNyor5Gpl986",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Predict class based on error threshold\n",
        "\n",
        "# Weitermachen\n",
        "def plot_cm_matrix(index:int,save_name:str,error_df = [mse_error_df,\n",
        "                                          mae_error_df,\n",
        "                                          huber_error_df,\n",
        "                                          ce_error_df,\n",
        "                                          focal_error_df],\n",
        "                   LABELS = [\"Non Fraud\", \"Fraud\"]):\n",
        "  if index > len(error_df) - 1 or index < 0:\n",
        "    raise(\"Index out of range implemented. Please correct your statement\")\n",
        "  else:\n",
        "    error_df = error_df[index]\n",
        "    threshold = get_precision_threshold(error_df)\n",
        "    y_pred = [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\n",
        "    conf_matrix = confusion_matrix(error_df.anomaly, y_pred,labels=[0,1])\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    sns.heatmap(conf_matrix,xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\",cmap='Blues');\n",
        "    plt.title(\"Confusion matrix\")\n",
        "    plt.ylabel('True class')\n",
        "    plt.xlabel('Predicted class')\n",
        "    save_fig(save_name)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "5TALJXevjfyZ"
      },
      "id": "5TALJXevjfyZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cm_matrix(0, \"CM-matrix visualisation to identify anomalies with model_mse\")\n",
        "plot_cm_matrix(1, \"CM-matrix visualisation to identify anomalies with model_mae\")\n",
        "plot_cm_matrix(2, \"CM-matrix visualisation to identify anomalies with model_huber\")\n",
        "plot_cm_matrix(3, \"CM-matrix visualisation to identify anomalies with model_ce\")\n",
        "plot_cm_matrix(4, \"CM-matrix visualisation to identify anomalies with model_focal\")"
      ],
      "metadata": {
        "id": "pEr2o-V2mij-"
      },
      "id": "pEr2o-V2mij-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_f1_scores(index, error_df =  \n",
        "                        [mse_error_df,\n",
        "                         mae_error_df,\n",
        "                         huber_error_df,\n",
        "                         ce_error_df,\n",
        "                         focal_error_df]\n",
        "                        ):\n",
        "  if index > len(error_df) - 1 or index < 0:\n",
        "    raise(\"Index out of range implemented. Please correct your statement\")\n",
        "  error_df = error_df[index]\n",
        "  threshold = get_precision_threshold(error_df)\n",
        "  y_pred =  [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\n",
        "  print('F1_Score with threshold:', threshold)\n",
        "  return(f1_score(error_df.anomaly, y_pred))\n",
        "calculate_f1_scores(0)"
      ],
      "metadata": {
        "id": "z6SVA051j1ei"
      },
      "id": "z6SVA051j1ei",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "error_df = [mse_error_df,\n",
        "             mae_error_df,\n",
        "             huber_error_df,\n",
        "             ce_error_df,\n",
        "             focal_error_df]\n",
        "for i in error_df:\n",
        "  threshold = get_precision_threshold(i) #specify here the index \n",
        "  y_pred =  [1 if e > threshold else 0 for e in i.reconstruction_error.values]\n",
        "\n",
        "  cm1 = confusion_matrix(i.anomaly, y_pred,labels=[1,0])\n",
        "  print('Confusion Matrix Val: \\n', cm1)\n",
        "\n",
        "\n",
        "  total1=sum(sum(cm1))\n",
        "  #####from confusion matrix calculate accuracy\n",
        "\n",
        "  accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
        "  print ('Accuracy Val: ', accuracy1)\n",
        "\n",
        "\n",
        "  sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
        "  print('Sensitivity Val: ', sensitivity1 )\n",
        "\n",
        "\n",
        "  specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
        "  print('Specificity Val: ', specificity1)\n",
        "\n",
        "  KappaValue=cohen_kappa_score(i.anomaly, y_pred)\n",
        "  print(\"Kappa Value :\",KappaValue)\n",
        "  AUC=roc_auc_score(i.anomaly, y_pred)\n",
        "\n",
        "  print(\"AUC         :\",AUC)\n",
        "\n",
        "  print(\"F1-Score Val  : \",f1_score(i.anomaly, y_pred))\n",
        "\n",
        "  print(\"_______________________________________\")"
      ],
      "metadata": {
        "id": "Z_rMWW0NcR0r"
      },
      "id": "Z_rMWW0NcR0r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simulation"
      ],
      "metadata": {
        "id": "4tHAq-c3oQn6"
      },
      "id": "4tHAq-c3oQn6"
    },
    {
      "cell_type": "code",
      "source": [
        "def simulation(\n",
        "    loss,\n",
        "    model,\n",
        "    patience,\n",
        "    epochs,\n",
        "    n:int = 0,\n",
        "    end:int = 30,\n",
        "    optimizer=\"adam\",\n",
        "    test_size = 0.2,\n",
        "    batch_size=16,\n",
        "    validation_batch_size = 16,\n",
        "    delta = 0.5\n",
        "    ):\n",
        "  \n",
        "  stopped_epochs = []\n",
        "  auc_values_roc = []\n",
        "  auc_values_recall_prec = []\n",
        "  f1_values = []\n",
        "\n",
        "  if loss == \"focal_loss\":\n",
        "    loss = sm.losses.BinaryFocalLoss()\n",
        "    print(\"Start the Simulation:\")\n",
        "    while(True):\n",
        "      print(\"Lossfunction: \", loss),\n",
        "      print(\"Iteration: \",n)\n",
        "      #Step 1: Splitt data\n",
        "      train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "        data,\n",
        "        labels_bool,\n",
        "        test_size=test_size\n",
        "        )\n",
        "      train_data_no_validation, train_data_validation, train_data_no_validation_labels, train_data_validation_labels = train_test_split(\n",
        "        train_data,\n",
        "        train_labels,\n",
        "        test_size = test_size\n",
        "        )\n",
        "      \n",
        "      #Step2: Train data\n",
        "      model = model\n",
        "      early_stopping = tf.keras.callbacks.EarlyStopping(patience=patience,\n",
        "                                                      restore_best_weights=True) #generate callback\n",
        "      model.compile(loss=loss, optimizer=optimizer, run_eagerly=True)\n",
        "      history = model.fit(train_data,\n",
        "                      train_data,\n",
        "                      batch_size=batch_size,\n",
        "                      epochs = epochs,\n",
        "                      validation_data=(train_data_validation, train_data_validation),\n",
        "                      callbacks=[\n",
        "                          early_stopping,\n",
        "                          ],\n",
        "                      verbose = 0,\n",
        "                      validation_batch_size = validation_batch_size,\n",
        "                      shuffle=True)\n",
        "      stopped_epochs.append(get_last_epoch(history)) #append stopped epoch \n",
        "\n",
        "      #Step3: Evaluate test data with AUC\n",
        "      error_df = gen_error_df(model)\n",
        "      fpr, tpr, thresholds = roc_curve(error_df.anomaly, error_df.reconstruction_error)\n",
        "      auc_values_roc.append(auc(fpr, tpr))\n",
        "\n",
        "      #Step4: Evaluate test data with AUC\n",
        "      prec, recall, thresholds = precision_recall_curve(error_df[\"anomaly\"], error_df[\"reconstruction_error\"])\n",
        "      auc_values_recall_prec.append(auc(recall, prec))\n",
        "      best_idx = np.argmin(np.abs(prec-recall)[:-4])\n",
        "      best_prec, best_recall = prec[best_idx], recall[best_idx]\n",
        "      threshold = thresholds[best_idx]\n",
        "      y_pred =  [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\n",
        "      f1_values.append(f1_score(error_df.anomaly, y_pred))\n",
        "\n",
        "      if (n >= end):\n",
        "        print(\"Final Output:\")\n",
        "        print(\"Return stopped epochs per iteration, \")\n",
        "        print(\"Return auc values per iteration, \")\n",
        "        print(\"Return precision scores with best thresholds per iteration, \")\n",
        "        print(\"Return recall scores with best thresholds per iteration, \")\n",
        "        print(\"Return f1 scores with best thresholds per iteration, \")\n",
        "        return(stopped_epochs,auc_values_roc,auc_values_recall_prec,f1_values)\n",
        "        break\n",
        "      n+=1\n",
        "  \n",
        "  elif loss == \"huber_loss\":\n",
        "    loss = tf.keras.losses.Huber(delta=delta, reduction=\"auto\", name=\"huber_loss\")\n",
        "    print(\"Start the Simulation:\")\n",
        "    while(True):\n",
        "      print(\"Lossfunction: \", loss),\n",
        "      print(\"Iteration: \",n)\n",
        "      #Step 1: Splitt data\n",
        "      train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "        data,\n",
        "        labels_bool,\n",
        "        test_size=test_size\n",
        "        )\n",
        "      train_data_no_validation, train_data_validation, train_data_no_validation_labels, train_data_validation_labels = train_test_split(\n",
        "        train_data,\n",
        "        train_labels,\n",
        "        test_size = test_size\n",
        "        )\n",
        "      \n",
        "      #Step2: Train data\n",
        "      model = model\n",
        "      early_stopping = tf.keras.callbacks.EarlyStopping(patience=patience,\n",
        "                                                      restore_best_weights=True) #generate callback\n",
        "      model.compile(loss=loss, optimizer=optimizer, run_eagerly=True)\n",
        "      history = model.fit(train_data,\n",
        "                      train_data,\n",
        "                      batch_size=batch_size,\n",
        "                      epochs = epochs,\n",
        "                      validation_data=(train_data_validation, train_data_validation),\n",
        "                      callbacks=[\n",
        "                          early_stopping,\n",
        "                          ],\n",
        "                      verbose = 0,\n",
        "                      validation_batch_size = validation_batch_size,\n",
        "                      shuffle=True)\n",
        "      stopped_epochs.append(get_last_epoch(history)) #append stopped epoch \n",
        "\n",
        "      #Step3: Evaluate test data with AUC\n",
        "      error_df = gen_error_df(model)\n",
        "      fpr, tpr, thresholds = roc_curve(error_df.anomaly, error_df.reconstruction_error)\n",
        "      auc_values_roc.append(auc(fpr, tpr))\n",
        "\n",
        "      #Step4: Evaluate test data with AUC\n",
        "      prec, recall, thresholds = precision_recall_curve(error_df[\"anomaly\"], error_df[\"reconstruction_error\"])\n",
        "      auc_values_recall_prec.append(auc(recall, prec))\n",
        "      best_idx = np.argmin(np.abs(prec-recall)[:-4])\n",
        "      best_prec, best_recall = prec[best_idx], recall[best_idx]\n",
        "      threshold = thresholds[best_idx]\n",
        "      y_pred =  [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\n",
        "      f1_values.append(f1_score(error_df.anomaly, y_pred))\n",
        "\n",
        "      if (n >= end):\n",
        "        print(\"Final Output:\")\n",
        "        print(\"Return stopped epochs per iteration, \")\n",
        "        print(\"Return auc values per iteration, \")\n",
        "        print(\"Return precision scores with best thresholds per iteration, \")\n",
        "        print(\"Return recall scores with best thresholds per iteration, \")\n",
        "        print(\"Return f1 scores with best thresholds per iteration, \")\n",
        "        return(stopped_epochs,auc_values_roc,auc_values_recall_prec,f1_values)\n",
        "        break\n",
        "      n+=1\n",
        "      \n",
        "  else:\n",
        "    print(\"Start the Simulation:\")\n",
        "    while(True):\n",
        "      print(\"Lossfunction: \", loss),\n",
        "      print(\"Iteration: \",n)\n",
        "      #Step 1: Splitt data\n",
        "      train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "        data,\n",
        "        labels_bool,\n",
        "        test_size=test_size\n",
        "        )\n",
        "      train_data_no_validation, train_data_validation, train_data_no_validation_labels, train_data_validation_labels = train_test_split(\n",
        "        train_data,\n",
        "        train_labels,\n",
        "        test_size = test_size\n",
        "        )\n",
        "      #Step2: Train data\n",
        "      model = model\n",
        "      early_stopping = tf.keras.callbacks.EarlyStopping(patience=patience,\n",
        "                                                      restore_best_weights=True) #generate callback\n",
        "      model.compile(loss=loss, optimizer=optimizer, run_eagerly=True)\n",
        "      history = model.fit(train_data,\n",
        "                      train_data,\n",
        "                      batch_size=batch_size,\n",
        "                      epochs = epochs,\n",
        "                      validation_data=(train_data_validation, train_data_validation),\n",
        "                      callbacks=[\n",
        "                          early_stopping,\n",
        "                          ],\n",
        "                      verbose = 0,\n",
        "                      validation_batch_size = validation_batch_size,\n",
        "                      shuffle=True)\n",
        "      stopped_epochs.append(get_last_epoch(history)) #append stopped epoch\n",
        "      \n",
        "      #Step3: Evaluate test data with AUC\n",
        "      error_df = gen_error_df(model)\n",
        "      fpr, tpr, thresholds = roc_curve(error_df.anomaly, error_df.reconstruction_error)\n",
        "      auc_values_roc.append(auc(fpr, tpr))\n",
        "\n",
        "      #Step4: Evaluate test data with AUC\n",
        "      prec, recall, thresholds = precision_recall_curve(error_df[\"anomaly\"], error_df[\"reconstruction_error\"])\n",
        "      auc_values_recall_prec.append(auc(recall, prec))\n",
        "      best_idx = np.argmin(np.abs(prec-recall)[:-4])\n",
        "      best_prec, best_recall = prec[best_idx], recall[best_idx]\n",
        "      threshold = thresholds[best_idx]\n",
        "      y_pred =  [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\n",
        "      f1_values.append(f1_score(error_df.anomaly, y_pred))\n",
        "\n",
        "      if (n >= end):\n",
        "        print(\"Final Output:\")\n",
        "        print(\"Return stopped epochs per iteration, \")\n",
        "        print(\"Return auc values per iteration, \")\n",
        "        print(\"Return precision scores with best thresholds per iteration, \")\n",
        "        print(\"Return recall scores with best thresholds per iteration, \")\n",
        "        print(\"Return f1 scores with best thresholds per iteration, \")\n",
        "        return(stopped_epochs,auc_values_roc,auc_values_recall_prec,f1_values)\n",
        "        break\n",
        "      n+=1"
      ],
      "metadata": {
        "id": "nxHH6UsB2uzQ"
      },
      "id": "nxHH6UsB2uzQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simulation_mse = simulation(model = model_mse,\n",
        "                            end = 30,\n",
        "                            patience = 20, #EaryStopping after 20 epochs,\n",
        "                            epochs = 1000,\n",
        "                            loss = \"mean_squared_error\")\n",
        "with open(\"simulation_mse_breastW_.txt\", \"w\") as f:\n",
        "   # Convert the list to a string and write it to the file\n",
        "    f.write(str(simulation_mse))"
      ],
      "metadata": {
        "id": "fzwsv1odlPh6"
      },
      "id": "fzwsv1odlPh6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simulation_mae = simulation(model = model_mae,\n",
        "                            end = 30,\n",
        "                            patience = 20, #EaryStopping after 20 epochs,\n",
        "                            epochs = 1000,\n",
        "                            loss = \"mean_absolute_error\")\n",
        "with open(\"simulation_mae_breastW_.txt\", \"w\") as f:\n",
        "   # Convert the list to a string and write it to the file\n",
        "    f.write(str(simulation_mae))"
      ],
      "metadata": {
        "id": "IALzuD5m0Mr0"
      },
      "id": "IALzuD5m0Mr0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simulation_huber = simulation(model = model_huber,\n",
        "                            end = 30,\n",
        "                            patience = 20, #EaryStopping after 20 epochs,\n",
        "                            epochs = 1000,\n",
        "                            loss = \"huber_loss\",\n",
        "                            delta = 0.5)\n",
        "with open(\"simulation_huber_breastW_.txt\", \"w\") as f:\n",
        "   # Convert the list to a string and write it to the file\n",
        "    f.write(str(simulation_huber))"
      ],
      "metadata": {
        "id": "KjnbTKOR0QI7"
      },
      "id": "KjnbTKOR0QI7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simulation_ce= simulation(model = model_ce,\n",
        "                          end = 30,\n",
        "                          patience = 20, #EaryStopping after 20 epochs,\n",
        "                          epochs = 1000,\n",
        "                          loss = \"binary_crossentropy\")\n",
        "with open(\"simulation_ce_breastW_.txt\", \"w\") as f:\n",
        "   # Convert the list to a string and write it to the file\n",
        "    f.write(str(simulation_ce))"
      ],
      "metadata": {
        "id": "dffk5V8g0Vd9"
      },
      "id": "dffk5V8g0Vd9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simulation_focal =simulation(model = model_focal,\n",
        "                             end = 30,\n",
        "                             patience = 20, #EaryStopping after 20 epochs,\n",
        "                             epochs = 1000,\n",
        "                             loss =\"focal_loss\")\n",
        "with open(\"simulation_focal_breastW_.txt\", \"w\") as f:\n",
        "   # Convert the list to a string and write it to the file\n",
        "    f.write(str(simulation_focal))"
      ],
      "metadata": {
        "id": "r3ipEXzI0axQ"
      },
      "id": "r3ipEXzI0axQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in [simulation_mse, simulation_mae, simulation_huber, simulation_ce, simulation_focal]:\n",
        "  print(\"_______________________________________________________\")\n",
        "  print(\"Mean and standard deviation of last epochs\")\n",
        "  print(np.asarray(i[0]).mean())\n",
        "  print(np.asarray(i[0]).std())\n",
        "  print(\"Mean and standard deviation of ROC-AUC\")\n",
        "  print(np.asarray(i[1]).mean())\n",
        "  print(np.asarray(i[1]).std())\n",
        "  print(\"Mean and standard deviation of Recall-Precision-AUC\")\n",
        "  print(np.asarray(i[2]).mean())\n",
        "  print(np.asarray(i[2]).std())\n",
        "\n"
      ],
      "metadata": {
        "id": "u7R3LdnWO7bi"
      },
      "id": "u7R3LdnWO7bi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistische Evaluation\n",
        "\n",
        "Die Ergebnisse aus der Simulation wurden in Textfiles gespeichert. Um die Daten zu laden, wurden diese manuell hier herein kopiert. \n",
        "\n",
        "Quelle:\n",
        "\n",
        "\n",
        "*   simulation_mse_breastW_.txt\n",
        "\n",
        "*   simulation_mae_breastW_.txt\n",
        "\n",
        "*   simulation_huber_breastW_.txt\n",
        "\n",
        "*   simulation_ce_breastW_.txt\n",
        "\n",
        "*   simulation_focal_breastW_.txt\n",
        "\n",
        "Um die Resultate der Ausarbeitung zu sehen, den folgenden Block auskommentieren. Ansonsten kann eine neue Simulation verwendet werden (GPU-Unterstützung sollte dafür aktiv sein)."
      ],
      "metadata": {
        "id": "AS5tHFlVoXKV"
      },
      "id": "AS5tHFlVoXKV"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# simulation_mse = [[83, 215, 57, 338, 44, 115, 71, 98, 41, 34, 97, 47, 28, 50, 47, 87, 92, 40, 34, 73, 77, 29, 26, 80, 63, 39, 60, 50, 22, 35, 32], [0.7263576779026217, 0.7146535580524345, 0.7205056179775281, 0.7315074906367041, 0.7256554307116105, 0.7357209737827715, 0.7312734082397004, 0.723314606741573, 0.7233146067415731, 0.7308052434456929, 0.7305711610486891, 0.7350187265917604, 0.7326779026217228, 0.7322097378277154, 0.7308052434456928, 0.7317415730337079, 0.7359550561797752, 0.7359550561797753, 0.7305711610486891, 0.7296348314606741, 0.7350187265917603, 0.7333801498127341, 0.7347846441947566, 0.7378277153558053, 0.7357209737827716, 0.7336142322097378, 0.7422752808988764, 0.7354868913857677, 0.7432116104868913, 0.7350187265917603, 0.7411048689138577], [0.48176857922048094, 0.4670740675305246, 0.4775567095162599, 0.4740908307338667, 0.4717914709485081, 0.476076902544321, 0.4748747898065486, 0.4698974893962126, 0.46709910802737914, 0.47229246844955336, 0.47117515637650004, 0.47749521431167735, 0.472484780642743, 0.47422774372666937, 0.4728186625328763, 0.47333476032415756, 0.47660213797134116, 0.47974132090976895, 0.47343942129970523, 0.4704574952104742, 0.47337968765219246, 0.47748451066693526, 0.47676490235837227, 0.4824133913904798, 0.48030668774202157, 0.4843403914346208, 0.4844864309003886, 0.48003156565189886, 0.48711794904534494, 0.4800151318751841, 0.4842461099644249], [0.46315789473684216, 0.46315789473684216, 0.4842105263157895, 0.5052631578947369, 0.5052631578947369, 0.5052631578947369, 0.46315789473684216, 0.4842105263157895, 0.4842105263157895, 0.4842105263157895, 0.4842105263157895, 0.46315789473684216, 0.46315789473684216, 0.46315789473684216, 0.4842105263157895, 0.46315789473684216, 0.4842105263157895, 0.4842105263157895, 0.4842105263157895, 0.4842105263157895, 0.4842105263157895, 0.4842105263157895, 0.4842105263157895, 0.4842105263157895, 0.4842105263157895, 0.4842105263157895, 0.4842105263157895, 0.4842105263157895, 0.5052631578947369, 0.4842105263157895, 0.4842105263157895]]\n",
        "# simulation_mae = [[128, 26, 70, 140, 22, 58, 101, 51, 36, 68, 58, 38, 24, 43, 20, 30, 43, 52, 62, 45, 36, 26, 37, 52, 39, 21, 23, 24, 22, 56, 50], [0.7078651685393258, 0.7116104868913857, 0.7158239700374532, 0.7202715355805244, 0.7186329588014981, 0.7109082397003745, 0.7286985018726592, 0.7272940074906367, 0.7249531835205992, 0.7219101123595506, 0.7256554307116105, 0.7258895131086143, 0.7244850187265918, 0.7205056179775281, 0.7244850187265918, 0.7305711610486891, 0.716994382022472, 0.7265917602996255, 0.7212078651685394, 0.7258895131086143, 0.7216760299625469, 0.7289325842696629, 0.7263576779026217, 0.7207397003745318, 0.7216760299625468, 0.7202715355805244, 0.722378277153558, 0.726123595505618, 0.7221441947565543, 0.7289325842696629, 0.7219101123595506], [0.45901317444488926, 0.4619196223201732, 0.4627237874503759, 0.4666265842335252, 0.4650070262117747, 0.45851249836724484, 0.47635534214147834, 0.4750241210448929, 0.4716508800866052, 0.46903469394525377, 0.4723533092192342, 0.47283404692277986, 0.471353590662186, 0.46799211503789206, 0.4712292213811213, 0.4747044461547346, 0.4645095370792167, 0.47353115856730893, 0.4678296348758231, 0.4738615875491491, 0.46922193610331026, 0.4748045446342627, 0.4733699413791063, 0.46876850377452717, 0.4690500513836704, 0.4680798457542617, 0.46983641508631485, 0.47286838051314706, 0.4686185712180406, 0.47458842935518186, 0.4684168894820359], [0.5319148936170214, 0.5473684210526316, 0.0, 0.0, 0.0, 0.0, 0.5473684210526316, 0.5473684210526316, 0.5473684210526316, 0.0, 0.5473684210526316, 0.5473684210526316, 0.0, 0.0, 0.5473684210526316, 0.5473684210526316, 0.0, 0.5473684210526316, 0.0, 0.5473684210526316, 0.0, 0.5473684210526316, 0.5473684210526316, 0.5473684210526316, 0.0, 0.0, 0.5473684210526316, 0.5473684210526316, 0.0, 0.5473684210526316, 0.0]]\n",
        "# simulation_huber = [[22, 38, 104, 47, 85, 49, 22, 55, 33, 23, 35, 39, 32, 36, 84, 66, 69, 46, 45, 63, 32, 36, 30, 36, 27, 39, 22, 41, 46, 26, 48], [0.7055243445692884, 0.6914794007490637, 0.7010767790262172, 0.7057584269662921, 0.7106741573033708, 0.7043539325842697, 0.7080992509363295, 0.696629213483146, 0.7062265917602997, 0.7029494382022472, 0.7045880149812734, 0.6952247191011236, 0.7038857677902621, 0.7062265917602997, 0.7010767790262172, 0.7071629213483146, 0.7073970037453183, 0.7055243445692884, 0.7137172284644194, 0.7057584269662921, 0.7027153558052435, 0.7092696629213483, 0.7066947565543071, 0.7052902621722846, 0.7045880149812734, 0.7085674157303371, 0.7020131086142322, 0.7052902621722846, 0.7055243445692884, 0.7024812734082397, 0.7071629213483146], [0.48511685602285015, 0.4480950747034451, 0.4555284598253575, 0.45877104863888, 0.461021367751396, 0.47958902705348505, 0.46354960643998144, 0.4528752617026056, 0.45890730822589154, 0.4565499774702527, 0.46173628429241165, 0.45211201088517294, 0.4572496369132849, 0.4583010441593189, 0.453886154777847, 0.4618191677894363, 0.4632236357758274, 0.4595699686124987, 0.4857400122070571, 0.45789591490167236, 0.45765476878348255, 0.48556623361873247, 0.4592732580467916, 0.46305588212561893, 0.45744858299427427, 0.46016559096892384, 0.4556737282740109, 0.4597732182194534, 0.45909577716143274, 0.4539226879092227, 0.4596671143555374], [0.5473684210526316, 0.5263157894736842, 0.5052631578947369, 0.5263157894736842, 0.5473684210526316, 0.5052631578947369, 0.5263157894736842, 0.5263157894736842, 0.5052631578947369, 0.5052631578947369, 0.4842105263157895, 0.4842105263157895, 0.5263157894736842, 0.5263157894736842, 0.5263157894736842, 0.5052631578947369, 0.5263157894736842, 0.5263157894736842, 0.5052631578947369, 0.5263157894736842, 0.5263157894736842, 0.5263157894736842, 0.5052631578947369, 0.5263157894736842, 0.5263157894736842, 0.5473684210526316, 0.5052631578947369, 0.5052631578947369, 0.5052631578947369, 0.5052631578947369, 0.4842105263157895]]\n",
        "# simulation_ce = [[485, 189, 78, 62, 22, 27, 68, 39, 39, 108, 80, 54, 91, 56, 36, 32, 60, 33, 27, 58, 51, 81, 34, 41, 51, 92, 107, 32, 30, 21, 26], [0.7120300751879699, 0.7255639097744362, 0.7313283208020049, 0.7233082706766917, 0.7436090225563909, 0.7413533834586467, 0.712531328320802, 0.7208020050125313, 0.7360902255639098, 0.7255639097744361, 0.7263157894736842, 0.7345864661654136, 0.7343358395989975, 0.7378446115288221, 0.7328320802005012, 0.7340852130325815, 0.7413533834586467, 0.7418546365914787, 0.7466165413533834, 0.737593984962406, 0.7491228070175439, 0.7483709273182957, 0.7448621553884711, 0.7335839598997493, 0.7451127819548873, 0.756641604010025, 0.7546365914786968, 0.7526315789473683, 0.7493734335839598, 0.7506265664160401, 0.7421052631578947], [0.47619047619047616, 0.5238095238095238, 0.5, 0.5, 0.5238095238095238, 0.5238095238095238, 0.5, 0.5238095238095238, 0.5238095238095238, 0.5, 0.47619047619047616, 0.5238095238095238, 0.5, 0.5238095238095238, 0.5, 0.5, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5], [0.47619047619047616, 0.5238095238095238, 0.5, 0.5, 0.5238095238095238, 0.5238095238095238, 0.5, 0.5238095238095238, 0.5238095238095238, 0.5, 0.47619047619047616, 0.5238095238095238, 0.5, 0.5238095238095238, 0.5, 0.5, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5], [0.4819277108433735, 0.5060240963855421, 0.5060240963855421, 0.5060240963855421, 0.5301204819277109, 0.5301204819277109, 0.4819277108433735, 0.5060240963855421, 0.5060240963855421, 0.4819277108433735, 0.4819277108433735, 0.5301204819277109, 0.5060240963855421, 0.5060240963855421, 0.5060240963855421, 0.5060240963855421, 0.5301204819277109, 0.5301204819277109, 0.5301204819277109, 0.5060240963855421, 0.5301204819277109, 0.5301204819277109, 0.5301204819277109, 0.5301204819277109, 0.5301204819277109, 0.5301204819277109, 0.5301204819277109, 0.5301204819277109, 0.5060240963855421, 0.5301204819277109, 0.5060240963855421]]\n",
        "# simulation_focal = [[32, 26, 44, 35, 71, 21, 46, 74, 48, 34, 24, 39, 30, 27, 20, 44, 20, 25, 22, 38, 20, 26, 81, 31, 20, 27, 20, 41, 29, 30, 23], [0.4740168539325842, 0.4691011235955056, 0.4676966292134831, 0.46980337078651685, 0.47425093632958804, 0.47191011235955055, 0.4679307116104869, 0.4662921348314606, 0.46956928838951306, 0.4700374531835205, 0.4777621722846442, 0.46652621722846443, 0.46722846441947563, 0.4735486891385768, 0.47425093632958804, 0.4726123595505618, 0.4719101123595505, 0.4665262172284643, 0.46863295880149813, 0.4726123595505618, 0.4691011235955056, 0.46816479400749056, 0.47659176029962547, 0.47659176029962547, 0.47354868913857684, 0.4698033707865169, 0.46863295880149813, 0.47448501872659177, 0.4737827715355805, 0.4691011235955056, 0.4669943820224719], [0.31915164371855187, 0.31658236479639157, 0.3159374131222968, 0.3168479031200666, 0.31908087560415554, 0.31769398481776634, 0.31632000546305017, 0.3153905225025381, 0.31718954444088937, 0.3172862206179785, 0.3212815320978825, 0.3156302684987513, 0.3157176067165215, 0.3189039476561454, 0.31910487418090017, 0.31852958677993504, 0.3180342331985332, 0.3156481398496002, 0.31683704307058663, 0.3184592858328014, 0.31677252895686214, 0.3162018731406295, 0.320709574079507, 0.32025286140578596, 0.31846127498792043, 0.3175745337847127, 0.3167964246710562, 0.3196675170673019, 0.3189920353355094, 0.31743411013544826, 0.31570847905756383], [0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105]]\n"
      ],
      "metadata": {
        "id": "cDgl_i80w-EZ"
      },
      "id": "cDgl_i80w-EZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = [\n",
        "    simulation_mse[0],\n",
        "    simulation_mae[0],\n",
        "    simulation_huber[0],\n",
        "    simulation_ce[0],\n",
        "    simulation_focal[0],\n",
        "    simulation_mse[1],\n",
        "    simulation_mae[1],\n",
        "    simulation_huber[1],\n",
        "    simulation_ce[1],\n",
        "    simulation_focal[1],\n",
        "    simulation_mse[2],\n",
        "    simulation_mae[2],\n",
        "    simulation_huber[2],\n",
        "    simulation_ce[2],\n",
        "    simulation_focal[2]   \n",
        "]"
      ],
      "metadata": {
        "id": "3wIl_qAEtB8a"
      },
      "execution_count": null,
      "outputs": [],
      "id": "3wIl_qAEtB8a"
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame(zip(*summary),\n",
        "                       columns=[\n",
        "                           \"last_epochs_mse\",\n",
        "                           \"last_epochs_mae\",\n",
        "                           \"last_epochs_huber\",\n",
        "                           \"last_epochs_ce\",\n",
        "                           \"last_epochs_focal\",\n",
        "                           \"auc_mse\",\n",
        "                           \"auc_mae\",\n",
        "                           \"auc_huber\",\n",
        "                           \"auc_ce\",\n",
        "                           \"auc_focal\",\n",
        "                           \"auprc_mse\",\n",
        "                           \"auprc_mae\",\n",
        "                           \"auprc_huber\",\n",
        "                           \"auprc_ce\",\n",
        "                           \"auprc_focal\"\n",
        "                       ]\n",
        "\n",
        ")\n",
        "\n",
        "results"
      ],
      "metadata": {
        "id": "mDjVmp6QtB8b"
      },
      "execution_count": null,
      "outputs": [],
      "id": "mDjVmp6QtB8b"
    },
    {
      "cell_type": "code",
      "source": [
        "#print boxplot for each variable\n",
        "fig, axs = plt.subplots(ncols=5, nrows=3, figsize=(20, 10))\n",
        "index = 0\n",
        "axs = axs.flatten()\n",
        "for k,v in results.items():\n",
        "    sns.boxplot(y=k, data=results, ax=axs[index])\n",
        "    index += 1\n",
        "save_fig(\"boxplots_results\")\n",
        "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)"
      ],
      "metadata": {
        "id": "16FvdHuetB8b"
      },
      "execution_count": null,
      "outputs": [],
      "id": "16FvdHuetB8b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kruskal-Wallis-Test"
      ],
      "metadata": {
        "id": "DhBiT7he15_B"
      },
      "id": "DhBiT7he15_B"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Der [Kruskal-Wallis-Test](https:https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kruskal.html//) ist ein nichtparametrischer Test, der zum Vergleich der Mittelwerte von drei oder mehr unabhängigen Gruppen verwendet werden kann. Er ähnelt der einseitigen ANOVA, setzt aber nicht voraus, dass die Daten normal verteilt sind.\n",
        "\n",
        "Zur Durchführung des Kruskal-Wallis-Tests werden die Daten zunächst vom niedrigsten zum höchsten Wert geordnet. Die Ränge werden dann zur Berechnung einer Teststatistik verwendet, die mit einem kritischen Wert verglichen wird, um festzustellen, ob der Unterschied zwischen den Gruppenmitteln statistisch signifikant ist.\n",
        "\n",
        "Der Kruskal-Wallis-Test ist geeignet, wenn die zu vergleichenden Populationen unabhängig sind und die Daten ordinal (d. h., sie können in eine Rangfolge gebracht werden) oder kontinuierlich (aber nicht normalverteilt) sind. Er ist auch geeignet, wenn die Stichprobengröße klein oder ungleich ist.\n",
        "\n"
      ],
      "metadata": {
        "id": "cAJ0U1zb1-eW"
      },
      "id": "cAJ0U1zb1-eW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last-Epochs: Gibt es Unterschiede bezüglich Konvergenz? "
      ],
      "metadata": {
        "id": "TiECRXFK4Zpp"
      },
      "id": "TiECRXFK4Zpp"
    },
    {
      "cell_type": "code",
      "source": [
        "for i in [simulation_mse, simulation_mae, simulation_huber, simulation_ce, simulation_focal]:\n",
        "  print(\"_______________________________________________________\")\n",
        "  print(\"Mean and standard deviation of last epochs\")\n",
        "  print(np.asarray(i[0]).mean())\n",
        "  print(np.asarray(i[0]).std())"
      ],
      "metadata": {
        "id": "nuQTmqfp_YIz"
      },
      "id": "nuQTmqfp_YIz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "'''\n",
        "Null hypothesis that the population median of all of the groups are equal\n",
        "\n",
        "Here: stopped epochs to evaluate the convergence ability \n",
        "'''\n",
        "stats.kruskal(\n",
        "    np.hstack(simulation_mse[0]),\n",
        "    np.hstack(simulation_mae[0]),\n",
        "    np.hstack(simulation_huber[0]),\n",
        "    np.hstack(simulation_ce[0]),\n",
        "    np.hstack(simulation_focal[0]),\n",
        "     )\n",
        "# KruskalResult(statistic=22.23847083667068, pvalue=0.0001796605135019599)\n"
      ],
      "metadata": {
        "id": "f0WyXHEZPkEP"
      },
      "id": "f0WyXHEZPkEP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AUC: Gibt es Unterschiede für die Performance in den Testdaten?"
      ],
      "metadata": {
        "id": "fteyhLbW4f_n"
      },
      "id": "fteyhLbW4f_n"
    },
    {
      "cell_type": "code",
      "source": [
        "for i in [simulation_mse, simulation_mae, simulation_huber, simulation_ce, simulation_focal]:\n",
        "  print(\"_______________________________________________________\")\n",
        "  print(\"Mean and standard deviation of roc_auc\")\n",
        "  print(np.asarray(i[1]).mean())\n",
        "  print(np.asarray(i[1]).std())"
      ],
      "metadata": {
        "id": "sSoLCgzh_coN"
      },
      "id": "sSoLCgzh_coN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "'''\n",
        "Null hypothesis that the population median of all of the groups are equal\n",
        "\n",
        "Here: stopped epochs to evaluate the convergence ability \n",
        "'''\n",
        "stats.kruskal(\n",
        "    np.hstack(simulation_mse[1]),\n",
        "    np.hstack(simulation_mae[1]),\n",
        "    np.hstack(simulation_huber[1]),\n",
        "    np.hstack(simulation_ce[1]),\n",
        "    np.hstack(simulation_focal[1]),\n",
        "     )\n",
        " # KruskalResult(statistic=131.99214948486943, pvalue=1.459883736690209e-27)\n"
      ],
      "metadata": {
        "id": "IAVYjCdx2-lw"
      },
      "id": "IAVYjCdx2-lw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AUPRC: Gibt es Unterschiede für die Performance in den Testdaten?"
      ],
      "metadata": {
        "id": "CtNMo1Cq4ryp"
      },
      "id": "CtNMo1Cq4ryp"
    },
    {
      "cell_type": "code",
      "source": [
        "for i in [simulation_mse, simulation_mae, simulation_huber, simulation_ce, simulation_focal]:\n",
        "  print(\"_______________________________________________________\")\n",
        "  print(\"Mean and standard deviation of roc_recall_precision\")\n",
        "  print(np.asarray(i[2]).mean())\n",
        "  print(np.asarray(i[2]).std())"
      ],
      "metadata": {
        "id": "h1eQ8WSp_j7p"
      },
      "id": "h1eQ8WSp_j7p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "'''\n",
        "Null hypothesis that the population median of all of the groups are equal\n",
        "\n",
        "Here: stopped epochs to evaluate the convergence ability \n",
        "'''\n",
        "stats.kruskal(\n",
        "    np.hstack(simulation_mse[2]),\n",
        "    np.hstack(simulation_mae[2]),\n",
        "    np.hstack(simulation_huber[2]),\n",
        "    np.hstack(simulation_ce[2]),\n",
        "    np.hstack(simulation_focal[2]),\n",
        "     )\n",
        "# KruskalResult(statistic=131.86117955510082, pvalue=1.557160300415864e-27)\n"
      ],
      "metadata": {
        "id": "yvYPtm1z2_e-"
      },
      "id": "yvYPtm1z2_e-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wenn die Nullhypothese des Kruskal-Wallis-Tests abgelehnt wird, bedeutet dies, dass sich mindestens einer der Gruppenmittelwerte signifikant von den anderen unterscheidet. Der Kruskal-Wallis-Test sagt jedoch nicht aus, welche Gruppe(n) sich unterscheiden. Um festzustellen, welche Gruppe(n) sich unterscheidet/unterscheiden, muss eine zusätzliche Analyse durchgeführt werden, z. B. ein Post-hoc-Test.\n",
        "\n",
        "Es gibt mehrere Post-hoc-Tests, die zum Vergleich der Mittelwerte bestimmter Gruppenpaare verwendet werden können. Zu den gängigen Tests gehören der [Dunn-Test](https://scikit-posthocs.readthedocs.io/en/latest/generated/scikit_posthocs.posthoc_dunn), der [Conover-Test](https://scikit-posthocs.readthedocs.io/en/latest/generated/scikit_posthocs.posthoc_conover) und der [Steel-Dwass-Test](https://scikit-posthocs.readthedocs.io/en/latest/generated/scikit_posthocs.posthoc_dscf). Diese Tests verwenden die Rangdaten aus dem Kruskal-Wallis-Test, um festzustellen, welche Gruppenpaare signifikant unterschiedliche Mittelwerte aufweisen.\n",
        "\n",
        "Alle diese Post-hoc-Tests können verwendet werden, um festzustellen, welche Gruppenpaare nach Durchführung des Kruskal-Wallis-Tests signifikant unterschiedliche Mittelwerte aufweisen.\n",
        "\n"
      ],
      "metadata": {
        "id": "uyzXbYfm2oRg"
      },
      "id": "uyzXbYfm2oRg"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-posthocs"
      ],
      "metadata": {
        "id": "JdIVIJbU7fvV"
      },
      "id": "JdIVIJbU7fvV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Dunn-Test](https://scikit-posthocs.readthedocs.io/en/latest/generated/scikit_posthocs.posthoc_dunn)\n",
        "\n",
        "Der Dunn-Test ist ein Post-hoc-Test, der die Mittelwerte aller Gruppenpaare unter Verwendung der Rangdaten aus dem Kruskal-Wallis-Test vergleicht. Er basiert auf der Differenz der Ränge zwischen den beiden zu vergleichenden Gruppen und passt sich durch Kontrolle der Falschentdeckungsrate an Mehrfachvergleiche an."
      ],
      "metadata": {
        "id": "2khLAXlO6McU"
      },
      "id": "2khLAXlO6McU"
    },
    {
      "cell_type": "code",
      "source": [
        "# last-epochs\n",
        "import scikit_posthocs as sp\n",
        "\n",
        "last_epochs = [np.hstack(simulation_mse[0]),\n",
        "    np.hstack(simulation_mae[0]),\n",
        "    np.hstack(simulation_huber[0]),\n",
        "    np.hstack(simulation_ce[0]),\n",
        "    np.hstack(simulation_focal[0])]\n",
        "\n",
        "sp.posthoc_dunn(last_epochs, p_adjust = 'holm')"
      ],
      "metadata": {
        "id": "2Wrb8pRT6t0T"
      },
      "id": "2Wrb8pRT6t0T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beobachtung:\n",
        "\n",
        "\n",
        "*   Der Focal-Loss entscheidet sich signifikant bzgl. Konvergenz von MSE und der CE \n",
        "*   Ansonsten wurde keine signifikanten Ergebnisse erzielt\n",
        "\n"
      ],
      "metadata": {
        "id": "d382uY7b7wfE"
      },
      "id": "d382uY7b7wfE"
    },
    {
      "cell_type": "code",
      "source": [
        "# roc_auc\n",
        "import scikit_posthocs as sp\n",
        "\n",
        "roc_auc = [np.hstack(simulation_mse[1]),\n",
        "    np.hstack(simulation_mae[1]),\n",
        "    np.hstack(simulation_huber[1]),\n",
        "    np.hstack(simulation_ce[1]),\n",
        "    np.hstack(simulation_focal[1])]\n",
        "\n",
        "sp.posthoc_dunn(roc_auc, p_adjust = 'holm')"
      ],
      "metadata": {
        "id": "cTslCFAi6twY"
      },
      "id": "cTslCFAi6twY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# recall_precision_auc\n",
        "import scikit_posthocs as sp\n",
        "\n",
        "recall_precision_auc = [np.hstack(simulation_mse[2]),\n",
        "    np.hstack(simulation_mae[2]),\n",
        "    np.hstack(simulation_huber[2]),\n",
        "    np.hstack(simulation_ce[2]),\n",
        "    np.hstack(simulation_focal[2])]\n",
        "\n",
        "sp.posthoc_dunn(recall_precision_auc, p_adjust = 'holm')"
      ],
      "metadata": {
        "id": "tjqH44_Z6tlf"
      },
      "id": "tjqH44_Z6tlf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Endresultat für breastW"
      ],
      "metadata": {
        "id": "rJ2dqOkYBi1i"
      },
      "id": "rJ2dqOkYBi1i"
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Performance                  | Loss       |\n",
        "|------------------------------|------------|\n",
        "| Schnellste Konvergenz?       | FL (gegenüber MSE und CE)         |\n",
        "| Bester ROC-AUC?              | MSE & CE   |\n",
        "| Bester ROC-Recall-Precision? | CE         |\n",
        "\n",
        "Anhand dieser Simulation hat die CE die beste Performance gezeigt mit AUPRC und AUC. Der MSE ist in der Tabelle gelistet, da dieser sich mit der AUC-Metrik sich nicht signifikant zum CE-Modell unterscheidet. Der CE erzielt im Schnitt  die langsamste Konvergenz.\n",
        "\n",
        "Aufgrund der Performance mit der AUPRC-Metrik hat die CE die beste Performance insgesamt gezeigt und wird als finalles Modell in dieser Simulation ausgewählt.\n",
        "\n",
        "Der Focal-Loss hat zwar die schnellste Konvergenz aufgezeigt, performt im Gegensatz zu den anderen Modellen signifikant am schlechtesten.\n",
        "\n"
      ],
      "metadata": {
        "id": "oJxRjhOTAYGP"
      },
      "id": "oJxRjhOTAYGP"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 28.912306,
      "end_time": "2021-07-25T13:21:52.129684",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-07-25T13:21:23.217378",
      "version": "2.3.3"
    },
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}