{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Title: Wisconsin Breast Cancer Database (January 8, 1991)\n",
        "\n",
        "This breast cancer databases was obtained from the University of Wisconsin\n",
        "   Hospitals, Madison from Dr. William H. Wolberg.  If you publish results\n",
        "   when using this database, then please include this information in your\n",
        "   acknowledgements.  Also, please cite one or more of:\n",
        "\n",
        "   1. O. L. Mangasarian and W. H. Wolberg: \"Cancer diagnosis via linear \n",
        "      programming\", SIAM News, Volume 23, Number 5, September 1990, pp 1 & 18.\n",
        "\n",
        "   2. William H. Wolberg and O.L. Mangasarian: \"Multisurface method of \n",
        "      pattern separation for medical diagnosis applied to breast cytology\", \n",
        "      Proceedings of the National Academy of Sciences, U.S.A., Volume 87, \n",
        "      December 1990, pp 9193-9196.\n",
        "\n",
        "   3. O. L. Mangasarian, R. Setiono, and W.H. Wolberg: \"Pattern recognition \n",
        "      via linear programming: Theory and application to medical diagnosis\", \n",
        "      in: \"Large-scale numerical optimization\", Thomas F. Coleman and Yuying\n",
        "      Li, editors, SIAM Publications, Philadelphia 1990, pp 22-30.\n",
        "\n",
        "   4. K. P. Bennett & O. L. Mangasarian: \"Robust linear programming \n",
        "      discrimination of two linearly inseparable sets\", Optimization Methods\n",
        "      and Software 1, 1992, 23-34 (Gordon & Breach Science Publishers).\n",
        "\n",
        "2. Sources:\n",
        "   -- Dr. WIlliam H. Wolberg (physician)\n",
        "      University of Wisconsin Hospitals\n",
        "      Madison, Wisconsin\n",
        "      USA\n",
        "   -- Donor: Olvi Mangasarian (mangasarian@cs.wisc.edu)\n",
        "      Received by David W. Aha (aha@cs.jhu.edu)\n",
        "   -- Date: 15 July 1992\n",
        "\n",
        "3. Past Usage:\n",
        "\n",
        "   Attributes 2 through 10 have been used to represent instances.\n",
        "   Each instance has one of 2 possible classes: benign or malignant.\n",
        "\n",
        "   1. Wolberg,~W.~H., \\& Mangasarian,~O.~L. (1990). Multisurface method of \n",
        "      pattern separation for medical diagnosis applied to breast cytology. In\n",
        "      {\\it Proceedings of the National Academy of Sciences}, {\\it 87},\n",
        "      9193--9196.\n",
        "      -- Size of data set: only 369 instances (at that point in time)\n",
        "      -- Collected classification results: 1 trial only\n",
        "      -- Two pairs of parallel hyperplanes were found to be consistent with\n",
        "         50% of the data\n",
        "         -- Accuracy on remaining 50% of dataset: 93.5%\n",
        "      -- Three pairs of parallel hyperplanes were found to be consistent with\n",
        "         67% of data\n",
        "         -- Accuracy on remaining 33% of dataset: 95.9%\n",
        "\n",
        "   2. Zhang,~J. (1992). Selecting typical instances in instance-based\n",
        "      learning.  In {\\it Proceedings of the Ninth International Machine\n",
        "      Learning Conference} (pp. 470--479).  Aberdeen, Scotland: Morgan\n",
        "      Kaufmann.\n",
        "      -- Size of data set: only 369 instances (at that point in time)\n",
        "      -- Applied 4 instance-based learning algorithms \n",
        "      -- Collected classification results averaged over 10 trials\n",
        "      -- Best accuracy result: \n",
        "         -- 1-nearest neighbor: 93.7%\n",
        "         -- trained on 200 instances, tested on the other 169\n",
        "      -- Also of interest:\n",
        "         -- Using only typical instances: 92.2% (storing only 23.1 instances)\n",
        "         -- trained on 200 instances, tested on the other 169"
      ],
      "metadata": {
        "id": "vekpui3ROUuO"
      },
      "id": "vekpui3ROUuO"
    },
    {
      "cell_type": "markdown",
      "id": "534c9f8f",
      "metadata": {
        "papermill": {
          "duration": 0.034124,
          "end_time": "2021-07-25T13:21:32.083955",
          "exception": false,
          "start_time": "2021-07-25T13:21:32.049831",
          "status": "completed"
        },
        "tags": [],
        "id": "534c9f8f"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyod"
      ],
      "metadata": {
        "id": "gOvrCcCmGBXt"
      },
      "id": "gOvrCcCmGBXt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mlconfig"
      ],
      "metadata": {
        "id": "KdpZU0b3GZH-"
      },
      "id": "KdpZU0b3GZH-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install segmentation_models_3D"
      ],
      "metadata": {
        "id": "4Quy4RAt5Wjh"
      },
      "id": "4Quy4RAt5Wjh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/jonbarron/robust_loss_pytorch\n"
      ],
      "metadata": {
        "id": "Nnp6tRAC6iQp"
      },
      "id": "Nnp6tRAC6iQp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bbfd417",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:32.152997Z",
          "iopub.status.busy": "2021-07-25T13:21:32.151665Z",
          "iopub.status.idle": "2021-07-25T13:21:39.867360Z",
          "shell.execute_reply": "2021-07-25T13:21:39.866498Z",
          "shell.execute_reply.started": "2021-07-25T13:20:27.135966Z"
        },
        "papermill": {
          "duration": 7.748097,
          "end_time": "2021-07-25T13:21:39.867567",
          "exception": false,
          "start_time": "2021-07-25T13:21:32.119470",
          "status": "completed"
        },
        "tags": [],
        "id": "3bbfd417"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import seaborn as sns; sns.set()\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import mlconfig\n",
        "import random as rn\n",
        "import keras\n",
        "import segmentation_models_3D as sm\n",
        "import robust_loss_pytorch \n",
        "import shutil\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "from time import strftime\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, precision_recall_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import normalize\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n",
        "                             roc_curve, recall_score, classification_report, f1_score,\n",
        "                             precision_recall_fscore_support)\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score,cohen_kappa_score,roc_auc_score,f1_score,auc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Achtung: Alle Dateien wurden in einem Ordner auf Drive gespeichert. Um den Code zu replizieren empfehle ich, ebenfalls einen Ordner auf Drive zu erstellen und folgenderma√üen das working directory anzupassen:\n",
        "\n",
        "```\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "```\n",
        "\n",
        "```\n",
        "cd /content/drive/MyDrive/<specify here your path>\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "a6NAmi0xmCGJ"
      },
      "id": "a6NAmi0xmCGJ"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "i3W2O7nA8Ywm"
      },
      "id": "i3W2O7nA8Ywm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "assert sys.version_info >= (3, 7) #python 3.7 or higher recommended"
      ],
      "metadata": {
        "id": "f5zzIbWeEQoF"
      },
      "id": "f5zzIbWeEQoF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from packaging import version\n",
        "import sklearn\n",
        "\n",
        "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\") #sklearn 1.01 or higher remmonded"
      ],
      "metadata": {
        "id": "bC4XPHKTEWsb"
      },
      "id": "bC4XPHKTEWsb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\") #tensorflow 2.8.0 or higher remmonded "
      ],
      "metadata": {
        "id": "2cVkUkOaEcrS"
      },
      "id": "2cVkUkOaEcrS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardeinstellungen f√ºr Plots anpassen"
      ],
      "metadata": {
        "id": "UBkgZ7vMDfc1"
      },
      "id": "UBkgZ7vMDfc1"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)"
      ],
      "metadata": {
        "id": "DmnRbn6sDanW"
      },
      "id": "DmnRbn6sDanW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Seminar2/ADAPL #specify here your path"
      ],
      "metadata": {
        "id": "LOCKjhUXFDvh"
      },
      "id": "LOCKjhUXFDvh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "aSFiTk8JFGKo"
      },
      "id": "aSFiTk8JFGKo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Erstellen eines Bildordners zum Sammeln aller Bildausgaben\n",
        "\n",
        "Erstellen Sie eine Funktion zum automatischen Speichern eines Plots in einem bestimmten Pfad."
      ],
      "metadata": {
        "id": "ujovvzSuE3Lg"
      },
      "id": "ujovvzSuE3Lg"
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "IMAGES_PATH = Path() / \"images\" / \"breastW\"\n",
        "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ],
      "metadata": {
        "id": "59KLrFMFE034"
      },
      "id": "59KLrFMFE034",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "gAw-T9e69ZVL"
      },
      "id": "gAw-T9e69ZVL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "Vhz7fY5Q0MfF"
      },
      "id": "Vhz7fY5Q0MfF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a6f67029",
      "metadata": {
        "papermill": {
          "duration": 0.027964,
          "end_time": "2021-07-25T13:21:39.924769",
          "exception": false,
          "start_time": "2021-07-25T13:21:39.896805",
          "status": "completed"
        },
        "tags": [],
        "id": "a6f67029"
      },
      "source": [
        "# Data Wrapping "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Relevant Information:\n",
        "\n",
        "   Samples arrive periodically as Dr. Wolberg reports his clinical cases.\n",
        "   The database therefore reflects this chronological grouping of the data.\n",
        "   This grouping information appears immediately below, having been removed\n",
        "   from the data itself:\n",
        "\n",
        "     Group 1: 367 instances (January 1989)\n",
        "     Group 2:  70 instances (October 1989)\n",
        "     Group 3:  31 instances (February 1990)\n",
        "     Group 4:  17 instances (April 1990)\n",
        "     Group 5:  48 instances (August 1990)\n",
        "     Group 6:  49 instances (Updated January 1991)\n",
        "     Group 7:  31 instances (June 1991)\n",
        "     Group 8:  86 instances (November 1991)\n",
        "     -----------------------------------------\n",
        "     Total:   699 points (as of the donated datbase on 15 July 1992)"
      ],
      "metadata": {
        "id": "DTZaV3utOse9"
      },
      "id": "DTZaV3utOse9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of Instances: 699 (as of 15 July 1992)\n",
        "\n",
        "Number of Attributes: 10 plus the class attribute\n",
        "\n",
        "Attribute Information: (class attribute has been moved to last column)\n",
        "\n",
        "   #  Attribute                     Domain\n",
        "   -- -----------------------------------------\n",
        "   1. Sample code number            id number\n",
        "   2. Clump Thickness               1 - 10\n",
        "   3. Uniformity of Cell Size       1 - 10\n",
        "   4. Uniformity of Cell Shape      1 - 10\n",
        "   5. Marginal Adhesion             1 - 10\n",
        "   6. Single Epithelial Cell Size   1 - 10\n",
        "   7. Bare Nuclei                   1 - 10\n",
        "   8. Bland Chromatin               1 - 10\n",
        "   9. Normal Nucleoli               1 - 10\n",
        "  10. Mitoses                       1 - 10\n",
        "  11. Class:                        (2 for benign, 4 for malignant)"
      ],
      "metadata": {
        "id": "GLo7UNfNO78C"
      },
      "id": "GLo7UNfNO78C"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23f8686a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:39.989429Z",
          "iopub.status.busy": "2021-07-25T13:21:39.988272Z",
          "iopub.status.idle": "2021-07-25T13:21:40.061055Z",
          "shell.execute_reply": "2021-07-25T13:21:40.061646Z",
          "shell.execute_reply.started": "2021-07-25T13:20:27.151048Z"
        },
        "papermill": {
          "duration": 0.109014,
          "end_time": "2021-07-25T13:21:40.061847",
          "exception": false,
          "start_time": "2021-07-25T13:21:39.952833",
          "status": "completed"
        },
        "tags": [],
        "id": "23f8686a"
      },
      "outputs": [],
      "source": [
        "PATH = '/content/drive/MyDrive/Seminar2/ADAPL/breastW_data/breast-cancer-wisconsin.txt'\n",
        "df = pd.read_csv(PATH)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e3e32da",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:40.138132Z",
          "iopub.status.busy": "2021-07-25T13:21:40.137413Z",
          "iopub.status.idle": "2021-07-25T13:21:40.233851Z",
          "shell.execute_reply": "2021-07-25T13:21:40.233292Z",
          "shell.execute_reply.started": "2021-07-25T13:20:27.192627Z"
        },
        "papermill": {
          "duration": 0.14124,
          "end_time": "2021-07-25T13:21:40.234017",
          "exception": false,
          "start_time": "2021-07-25T13:21:40.092777",
          "status": "completed"
        },
        "tags": [],
        "id": "1e3e32da"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e023884f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:40.315199Z",
          "iopub.status.busy": "2021-07-25T13:21:40.314023Z",
          "iopub.status.idle": "2021-07-25T13:21:40.318207Z",
          "shell.execute_reply": "2021-07-25T13:21:40.318690Z",
          "shell.execute_reply.started": "2021-07-25T13:20:27.274520Z"
        },
        "papermill": {
          "duration": 0.054244,
          "end_time": "2021-07-25T13:21:40.318870",
          "exception": false,
          "start_time": "2021-07-25T13:21:40.264626",
          "status": "completed"
        },
        "tags": [],
        "id": "e023884f"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4000e47d",
      "metadata": {
        "papermill": {
          "duration": 0.028919,
          "end_time": "2021-07-25T13:21:40.377325",
          "exception": false,
          "start_time": "2021-07-25T13:21:40.348406",
          "status": "completed"
        },
        "tags": [],
        "id": "4000e47d"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a57db549",
      "metadata": {
        "papermill": {
          "duration": 0.028881,
          "end_time": "2021-07-25T13:21:40.435350",
          "exception": false,
          "start_time": "2021-07-25T13:21:40.406469",
          "status": "completed"
        },
        "tags": [],
        "id": "a57db549"
      },
      "source": [
        "Drop unnecessary columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2239e656",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:40.500467Z",
          "iopub.status.busy": "2021-07-25T13:21:40.499799Z",
          "iopub.status.idle": "2021-07-25T13:21:40.502914Z",
          "shell.execute_reply": "2021-07-25T13:21:40.503391Z",
          "shell.execute_reply.started": "2021-07-25T13:20:27.289012Z"
        },
        "papermill": {
          "duration": 0.038906,
          "end_time": "2021-07-25T13:21:40.503565",
          "exception": false,
          "start_time": "2021-07-25T13:21:40.464659",
          "status": "completed"
        },
        "tags": [],
        "id": "2239e656"
      },
      "outputs": [],
      "source": [
        "drop_cols = [\"Sample code number\"]\n",
        "df.drop(drop_cols, axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bf6ce80",
      "metadata": {
        "papermill": {
          "duration": 0.02909,
          "end_time": "2021-07-25T13:21:40.562231",
          "exception": false,
          "start_time": "2021-07-25T13:21:40.533141",
          "status": "completed"
        },
        "tags": [],
        "id": "7bf6ce80"
      },
      "source": [
        "Convert to dummy variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5adfe94d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:40.628625Z",
          "iopub.status.busy": "2021-07-25T13:21:40.627928Z",
          "iopub.status.idle": "2021-07-25T13:21:40.631220Z",
          "shell.execute_reply": "2021-07-25T13:21:40.630570Z",
          "shell.execute_reply.started": "2021-07-25T13:20:27.297185Z"
        },
        "papermill": {
          "duration": 0.039489,
          "end_time": "2021-07-25T13:21:40.631370",
          "exception": false,
          "start_time": "2021-07-25T13:21:40.591881",
          "status": "completed"
        },
        "tags": [],
        "id": "5adfe94d"
      },
      "outputs": [],
      "source": [
        "rep_dict = {2: 0.0, 4: 1.0}\n",
        "df['class outlier'].replace(rep_dict, inplace=True);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceccb042",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:40.695925Z",
          "iopub.status.busy": "2021-07-25T13:21:40.695283Z",
          "iopub.status.idle": "2021-07-25T13:21:40.698996Z",
          "shell.execute_reply": "2021-07-25T13:21:40.699704Z",
          "shell.execute_reply.started": "2021-07-25T13:20:27.310014Z"
        },
        "papermill": {
          "duration": 0.038717,
          "end_time": "2021-07-25T13:21:40.699919",
          "exception": false,
          "start_time": "2021-07-25T13:21:40.661202",
          "status": "completed"
        },
        "tags": [],
        "id": "ceccb042"
      },
      "outputs": [],
      "source": [
        "print(f'Data size is {df.shape}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9m-RJiCMNyfc"
      },
      "id": "9m-RJiCMNyfc"
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "RPpPZ23NK9df"
      },
      "id": "RPpPZ23NK9df",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Missing attribute values: 16\n",
        "\n",
        "There are 16 instances in Groups 1 to 6 that contain a single missing (i.e., unavailable) attribute value, now denoted by \"?\" in column \"Bare Nuclei\"\n"
      ],
      "metadata": {
        "id": "1_cfP_JzOJCD"
      },
      "id": "1_cfP_JzOJCD"
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[df['Bare Nuclei'] == \"?\"]"
      ],
      "metadata": {
        "id": "8oIcT1OdNoM3"
      },
      "id": "8oIcT1OdNoM3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(df.loc[df['Bare Nuclei']==\"?\"].index, inplace=True)"
      ],
      "metadata": {
        "id": "qIQCRaDtPVU_"
      },
      "id": "qIQCRaDtPVU_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Bare Nuclei'] = pd.to_numeric(df['Bare Nuclei'])"
      ],
      "metadata": {
        "id": "joLFde0lM__T"
      },
      "id": "joLFde0lM__T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Data size is {df.shape}.')"
      ],
      "metadata": {
        "id": "Og0tSmuIQSMp"
      },
      "id": "Og0tSmuIQSMp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check for NA-Values \n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "ho1HV5CaW8-T"
      },
      "id": "ho1HV5CaW8-T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7bf1b5a5",
      "metadata": {
        "papermill": {
          "duration": 0.029306,
          "end_time": "2021-07-25T13:21:40.759005",
          "exception": false,
          "start_time": "2021-07-25T13:21:40.729699",
          "status": "completed"
        },
        "tags": [],
        "id": "7bf1b5a5"
      },
      "source": [
        "# EDA and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print boxplot for each variable\n",
        "fig, axs = plt.subplots(ncols=5, nrows=2, figsize=(20, 10))\n",
        "index = 0\n",
        "axs = axs.flatten()\n",
        "for k,v in df.items():\n",
        "    sns.boxplot(y=k, data=df, ax=axs[index])\n",
        "    index += 1\n",
        "save_fig(\"boxplots\")\n",
        "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)"
      ],
      "metadata": {
        "id": "g5gOVHsoXZj2"
      },
      "id": "g5gOVHsoXZj2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k, v in df.items():\n",
        "        q1 = v.quantile(0.25)\n",
        "        q3 = v.quantile(0.75)\n",
        "        irq = q3 - q1\n",
        "        v_col = v[(v <= q1 - 1.5 * irq) | (v >= q3 + 1.5 * irq)]\n",
        "        perc = np.shape(v_col)[0] * 100.0 / np.shape(df)[0]\n",
        "        print(\"Column %s outliers = %.2f%%\" % (k, perc))"
      ],
      "metadata": {
        "id": "kF4jz7nKYmr_"
      },
      "id": "kF4jz7nKYmr_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print distributions\n",
        "fig, axs = plt.subplots(ncols=5, nrows=2, figsize=(20, 10))\n",
        "index = 0\n",
        "axs = axs.flatten()\n",
        "for k,v in df.items():\n",
        "    sns.distplot(v, ax=axs[index])\n",
        "    index += 1\n",
        "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)\n",
        "save_fig(\"distrubutions\")"
      ],
      "metadata": {
        "id": "CoEAggn0Y2Je"
      },
      "id": "CoEAggn0Y2Je",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class distribution:\n",
        " \n",
        "   Benign: 458 (65.5%)\n",
        "   Malignant: 241 (34.5%)"
      ],
      "metadata": {
        "id": "dxjNAQDePJeX"
      },
      "id": "dxjNAQDePJeX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a45194fb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:40.824938Z",
          "iopub.status.busy": "2021-07-25T13:21:40.824315Z",
          "iopub.status.idle": "2021-07-25T13:21:41.055800Z",
          "shell.execute_reply": "2021-07-25T13:21:41.056331Z",
          "shell.execute_reply.started": "2021-07-25T13:20:27.323572Z"
        },
        "papermill": {
          "duration": 0.26791,
          "end_time": "2021-07-25T13:21:41.056508",
          "exception": false,
          "start_time": "2021-07-25T13:21:40.788598",
          "status": "completed"
        },
        "tags": [],
        "id": "a45194fb"
      },
      "outputs": [],
      "source": [
        "df['class outlier'].value_counts().plot(kind='bar', figsize=(8, 4));\n",
        "plt.title('Distrubution of outliers');\n",
        "plt.xlabel('Diagnosis');\n",
        "plt.ylabel('Frequency');\n",
        "plt.xticks([0.0, 1.0], ['Benign', 'Malignant'], rotation=45);\n",
        "save_fig(\"distrubution outliers\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "\n",
        "sns.set(style='whitegrid', context='notebook')\n",
        "\n",
        "def tsne_scatter(features, labels, dimensions=2, save_as='graph.png', RANDOM_SEED = 42):\n",
        "    if dimensions not in (2, 3):\n",
        "        raise ValueError('tsne_scatter can only plot in 2d or 3d')\n",
        "\n",
        "    # t-SNE dimensionality reduction\n",
        "    features_embedded = TSNE(n_components=dimensions, random_state=RANDOM_SEED).fit_transform(features)\n",
        "    \n",
        "    # initialising the plot\n",
        "    fig, ax = plt.subplots(figsize=(8,8))\n",
        "    \n",
        "    # counting dimensions\n",
        "    if dimensions == 3: ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    # plotting data\n",
        "    ax.scatter(\n",
        "        *zip(*features_embedded[np.where(labels==1)]),\n",
        "        marker='o',\n",
        "        color='r',\n",
        "        s=2,\n",
        "        alpha=0.7,\n",
        "        label='B√∂sartig'\n",
        "    )\n",
        "    ax.scatter(\n",
        "        *zip(*features_embedded[np.where(labels==0)]),\n",
        "        marker='o',\n",
        "        color='g',\n",
        "        s=2,\n",
        "        alpha=0.3,\n",
        "        label='Gutartig'\n",
        "    )\n",
        "\n",
        "    # storing it to be displayed later\n",
        "    sns.set(style='whitegrid', context='notebook')\n",
        "    save_fig(save_as)\n",
        "    plt.show;"
      ],
      "metadata": {
        "id": "ZJLkGL0MyQ91"
      },
      "id": "ZJLkGL0MyQ91",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26491ce4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:41.122609Z",
          "iopub.status.busy": "2021-07-25T13:21:41.121980Z",
          "iopub.status.idle": "2021-07-25T13:21:41.147594Z",
          "shell.execute_reply": "2021-07-25T13:21:41.148118Z",
          "shell.execute_reply.started": "2021-07-25T13:20:27.473227Z"
        },
        "papermill": {
          "duration": 0.060924,
          "end_time": "2021-07-25T13:21:41.148323",
          "exception": false,
          "start_time": "2021-07-25T13:21:41.087399",
          "status": "completed"
        },
        "tags": [],
        "id": "26491ce4"
      },
      "outputs": [],
      "source": [
        "df['anomaly'] = df['class outlier'] == 1.0\n",
        "anomaly = df[df['anomaly'] == True]\n",
        "normal = df[df['anomaly'] == False]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29a4a399",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:41.214548Z",
          "iopub.status.busy": "2021-07-25T13:21:41.213867Z",
          "iopub.status.idle": "2021-07-25T13:21:42.159895Z",
          "shell.execute_reply": "2021-07-25T13:21:42.160398Z",
          "shell.execute_reply.started": "2021-07-25T13:20:27.481699Z"
        },
        "papermill": {
          "duration": 0.980319,
          "end_time": "2021-07-25T13:21:42.160614",
          "exception": false,
          "start_time": "2021-07-25T13:21:41.180295",
          "status": "completed"
        },
        "tags": [],
        "id": "29a4a399"
      },
      "outputs": [],
      "source": [
        "sns.distplot(normal);\n",
        "sns.distplot(anomaly);\n",
        "\n",
        "plt.title('normal vs  anomaly Dist.');\n",
        "plt.ylabel('Dist.');\n",
        "save_fig(\"normal vs anomaly distrubution\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fcb1223",
      "metadata": {
        "papermill": {
          "duration": 0.033415,
          "end_time": "2021-07-25T13:21:42.227161",
          "exception": false,
          "start_time": "2021-07-25T13:21:42.193746",
          "status": "completed"
        },
        "tags": [],
        "id": "4fcb1223"
      },
      "source": [
        "# Normalize The Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = df.iloc[:, 0:11]\n",
        "data"
      ],
      "metadata": {
        "id": "ejQS1dtvS3EV"
      },
      "id": "ejQS1dtvS3EV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0bf83c2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:42.296276Z",
          "iopub.status.busy": "2021-07-25T13:21:42.295602Z",
          "iopub.status.idle": "2021-07-25T13:21:42.303352Z",
          "shell.execute_reply": "2021-07-25T13:21:42.303827Z",
          "shell.execute_reply.started": "2021-07-25T13:20:28.117717Z"
        },
        "papermill": {
          "duration": 0.043478,
          "end_time": "2021-07-25T13:21:42.304045",
          "exception": false,
          "start_time": "2021-07-25T13:21:42.260567",
          "status": "completed"
        },
        "tags": [],
        "id": "f0bf83c2"
      },
      "outputs": [],
      "source": [
        "# The last element contains the labels\n",
        "labels_bool = df.iloc[:,-1]\n",
        "labels  = df.iloc[:,-2]\n",
        "# The other data points are the features\n",
        "data = df.iloc[:, 0:9]\n",
        "\n",
        "data = normalize(data) #normalize data\n",
        "n_features = 9"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tsne_scatter(data, labels, dimensions=2, save_as='tsne_initial_2d')\n",
        "tsne_scatter(data, labels, dimensions=3, save_as='tsne_initial_3d')"
      ],
      "metadata": {
        "id": "UW_vlLmIy_Qq"
      },
      "id": "UW_vlLmIy_Qq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b7a3e8c0",
      "metadata": {
        "papermill": {
          "duration": 0.031913,
          "end_time": "2021-07-25T13:21:42.369025",
          "exception": false,
          "start_time": "2021-07-25T13:21:42.337112",
          "status": "completed"
        },
        "tags": [],
        "id": "b7a3e8c0"
      },
      "source": [
        "# Split the data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generiere Trainings- und Testdaten"
      ],
      "metadata": {
        "id": "kFlKmZZDUk-B"
      },
      "id": "kFlKmZZDUk-B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "968cc560",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:42.436944Z",
          "iopub.status.busy": "2021-07-25T13:21:42.436304Z",
          "iopub.status.idle": "2021-07-25T13:21:42.442700Z",
          "shell.execute_reply": "2021-07-25T13:21:42.443205Z",
          "shell.execute_reply.started": "2021-07-25T13:20:28.126744Z"
        },
        "papermill": {
          "duration": 0.042295,
          "end_time": "2021-07-25T13:21:42.443403",
          "exception": false,
          "start_time": "2021-07-25T13:21:42.401108",
          "status": "completed"
        },
        "tags": [],
        "id": "968cc560"
      },
      "outputs": [],
      "source": [
        "train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "    data, labels_bool,\n",
        "    test_size=0.2,\n",
        "    \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generiere das Validation-Set"
      ],
      "metadata": {
        "id": "EfXD36r91arN"
      },
      "id": "EfXD36r91arN"
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_no_validation, train_data_validation, train_data_no_validation_labels, train_data_validation_labels = train_test_split(\n",
        "    train_data,\n",
        "    train_labels,\n",
        "    test_size = 0.2,\n",
        ")\n",
        "# only train_data_validation and train_data_validation_labels needed"
      ],
      "metadata": {
        "id": "kJ8o_BX00ock"
      },
      "id": "kJ8o_BX00ock",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cef295c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T13:21:42.510580Z",
          "iopub.status.busy": "2021-07-25T13:21:42.509856Z",
          "iopub.status.idle": "2021-07-25T13:21:42.515158Z",
          "shell.execute_reply": "2021-07-25T13:21:42.515651Z",
          "shell.execute_reply.started": "2021-07-25T13:20:28.142992Z"
        },
        "papermill": {
          "duration": 0.040807,
          "end_time": "2021-07-25T13:21:42.515830",
          "exception": false,
          "start_time": "2021-07-25T13:21:42.475023",
          "status": "completed"
        },
        "tags": [],
        "id": "5cef295c"
      },
      "outputs": [],
      "source": [
        "anomalous_train_data = train_data[train_labels] #training data with outlier-label == True\n",
        "anomalous_test_data = test_data[test_labels] #test data with outlier-label == True\n",
        "\n",
        "normal_train_data = train_data[~train_labels] #training data with outlier-label == False\n",
        "normal_test_data = test_data[~test_labels] #test data with outlier-label == False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for k in [train_data, train_labels, train_data_validation, train_data_validation_labels]: \n",
        " print(k.shape)"
      ],
      "metadata": {
        "id": "EGsGaiP01R7w"
      },
      "id": "EGsGaiP01R7w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in [train_data, test_data, train_labels, test_labels]: \n",
        " print(k.shape)"
      ],
      "metadata": {
        "id": "vZhjp7VNZIMr"
      },
      "id": "vZhjp7VNZIMr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in [anomalous_train_data, anomalous_test_data, normal_train_data, normal_test_data]: \n",
        " print(k.shape)"
      ],
      "metadata": {
        "id": "rOIRuAXJngZm"
      },
      "id": "rOIRuAXJngZm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Anomaly Share for training data equals ={:10.4f}\".format(len(anomalous_train_data)/len(train_data)))\n",
        "print(\"Anomaly Share for test data equals ={:10.4f}\".format(len(anomalous_test_data)/len(test_data)))"
      ],
      "metadata": {
        "id": "b9uGwVVTnr6j"
      },
      "id": "b9uGwVVTnr6j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "41902a32",
      "metadata": {
        "papermill": {
          "duration": 0.032604,
          "end_time": "2021-07-25T13:21:42.580702",
          "exception": false,
          "start_time": "2021-07-25T13:21:42.548098",
          "status": "completed"
        },
        "tags": [],
        "id": "41902a32"
      },
      "source": [
        "# Train the model with train & validation data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Randomized autoencoders\n",
        "\n",
        "Die Implementierung f√ºr die RandAE-Klasse stammt aus folgendem [Repository](https://github.com/danieltsoukup/autoencoders):"
      ],
      "metadata": {
        "id": "PKnXqpbJfU8j"
      },
      "id": "PKnXqpbJfU8j"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aktivierungsfunktionen\n",
        "\n",
        "*   Hidden Layer: Relu\n",
        "*   Output Layer: Sigmoid\n",
        "\n",
        "Es wird ein Drop_ratio von 0.5 festgelegt. Ein Drop_ratio von 0.0 entspricht einer *fully connected architecture*\n",
        "\n"
      ],
      "metadata": {
        "id": "AYCNoY07UZF3"
      },
      "id": "AYCNoY07UZF3"
    },
    {
      "cell_type": "code",
      "source": [
        "class RandAE(tf.keras.Sequential):\n",
        "    def __init__(self, input_dim, hidden_dims, drop_ratio=0.5, **kwargs):\n",
        "        super(RandAE, self).__init__(**kwargs)\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.drop_ratio = drop_ratio\n",
        "        \n",
        "        self.layer_masks = dict()\n",
        "        \n",
        "        self.build_model()\n",
        "                \n",
        "    def build_model(self) -> None:\n",
        "        \"\"\"\n",
        "        Adds the layers and records masks.\n",
        "        \"\"\"\n",
        "        \n",
        "        self.add(layers.Input(self.input_dim, name=\"input\"))\n",
        "        \n",
        "        for i, dim in enumerate(self.hidden_dims):\n",
        "            layer_name = f\"hidden_{i}\"\n",
        "            layer = layers.Dense(dim, \n",
        "                                 activation=\"relu\" if i > 0 else \"sigmoid\", \n",
        "                                 name=layer_name)\n",
        "            self.add(layer)\n",
        "            \n",
        "            # add layer mask\n",
        "            self.layer_masks[layer_name] = self.get_mask(layer)\n",
        "        \n",
        "        layer_name = \"output\"\n",
        "        output_layer = layers.Dense(self.input_dim, activation=\"sigmoid\", name=layer_name)\n",
        "        self.add(output_layer)\n",
        "        self.layer_masks[layer_name] = self.get_mask(output_layer)\n",
        "            \n",
        "    def get_mask(self, layer) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Build mask for a layer.\n",
        "        \"\"\"\n",
        "        \n",
        "        shape = layer.input_shape[1], layer.output_shape[1]\n",
        "        \n",
        "        return np.random.choice([0., 1.], size=shape, p=[self.drop_ratio, 1-self.drop_ratio])\n",
        "        \n",
        "    def load_masks(self, mask_pickle_path) -> None:\n",
        "        \"\"\"\n",
        "        Load the masks from a pickled dictionary.\n",
        "        \"\"\"\n",
        "        \n",
        "        with open(mask_pickle_path, 'rb') as handle:\n",
        "            self.layer_masks = pickle.load(handle)    \n",
        "            \n",
        "    def get_encoder(self) -> keras.Sequential:\n",
        "        \"\"\"\n",
        "        Get the encoder from the full model.\n",
        "        \"\"\"\n",
        "        \n",
        "        n_layers = (len(self.hidden_dims)+1)//2\n",
        "        encoder_layers = [layers.Input(self.input_dim)] + self.layers[:n_layers]\n",
        "\n",
        "        return keras.Sequential(encoder_layers)\n",
        "        \n",
        "    \n",
        "    def mask_weights(self) -> None:\n",
        "        \"\"\"\n",
        "        Apply the masks to each layer in the encoder and decoder.\n",
        "        \"\"\"\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            layer_name = layer.name\n",
        "            if layer_name in self.layer_masks:\n",
        "                masked_w = layer.weights[0].numpy()*self.layer_masks[layer_name]\n",
        "                b = layer.weights[1].numpy()\n",
        "                layer.set_weights((masked_w, b))        \n",
        "\n",
        "    def call(self, data, training=True) -> tf.Tensor:\n",
        "        \n",
        "        # mask the weights before original forward pass\n",
        "        self.mask_weights()\n",
        "        \n",
        "        return super().call(data)"
      ],
      "metadata": {
        "id": "SvaT6tdyfYp3"
      },
      "id": "SvaT6tdyfYp3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_mse = RandAE(9,[24,12,6,3,6,12,24])\n",
        "model_mae = RandAE(9,[24,12,6,3,6,12,24])\n",
        "model_ce = RandAE(9,[24,12,6,3,6,12,24])\n",
        "model_focal = RandAE(9,[24,12,6,3,6,12,24])\n",
        "model_huber = RandAE(9,[24,12,6,3,6,12,24])"
      ],
      "metadata": {
        "id": "AGxkc_txfkAN"
      },
      "id": "AGxkc_txfkAN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "73sIBiifn4rk"
      },
      "id": "73sIBiifn4rk"
    },
    {
      "cell_type": "code",
      "source": [
        "#load checkpoints\n",
        "# checkpoint_mse = tf.keras.callbacks.ModelCheckpoint(\"checkpoint_mse\",\n",
        "#                                                    save_weights_only=True) #generate checkpoints\n",
        "\n",
        "\n",
        "#define early stopping\n",
        "early_stopping_mse = tf.keras.callbacks.EarlyStopping(patience=20,\n",
        "                                                     restore_best_weights=True) \n",
        "\n",
        "\n",
        "\n",
        "                                                                                                                                                       \n",
        "model_mse.compile(loss=\"mean_squared_error\", optimizer=\"adam\", run_eagerly=True)\n",
        "\n",
        "\n",
        "history_mse = model_mse.fit(train_data,\n",
        "                    train_data,\n",
        "                    batch_size=16,\n",
        "                    epochs = 1000,\n",
        "                    validation_data=(train_data_validation, train_data_validation),\n",
        "                    callbacks=[\n",
        "                        # checkpoint_mse,\n",
        "                        early_stopping_mse,\n",
        "                        ],\n",
        "                    verbose = 0,\n",
        "                    validation_batch_size = 16,\n",
        "                    shuffle=True)\n",
        "model_mse.summary()"
      ],
      "metadata": {
        "id": "cES2dR_t2ms8"
      },
      "id": "cES2dR_t2ms8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint_mae = tf.keras.callbacks.ModelCheckpoint(\"checkpoint_mae\",\n",
        "#                                                    save_weights_only=True) #generate checkpoints\n",
        "early_stopping_mae = tf.keras.callbacks.EarlyStopping(patience=20,\n",
        "                                                     restore_best_weights=True) #define early stopping                                                  \n",
        "model_mae.compile(loss=\"mean_absolute_error\", optimizer=\"adam\", run_eagerly=True)\n",
        "history_mae = model_mae.fit(train_data,\n",
        "                    train_data,\n",
        "                    batch_size=16,\n",
        "                    epochs = 1000,\n",
        "                    validation_data=(train_data_validation, train_data_validation),\n",
        "                    callbacks=[\n",
        "                        # checkpoint_mae,\n",
        "                        early_stopping_mae],\n",
        "                    verbose = 0,\n",
        "                    validation_batch_size = 16,\n",
        "                    shuffle=True)\n",
        "model_mae.summary()"
      ],
      "metadata": {
        "id": "Z4g9XkAMOxNy"
      },
      "id": "Z4g9XkAMOxNy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# checkpoint_ce = tf.keras.callbacks.ModelCheckpoint(\"checkpoint_ce\",\n",
        "#                                                    save_weights_only=True) #generate checkpoints\n",
        "early_stopping_ce = tf.keras.callbacks.EarlyStopping(patience=20,\n",
        "                                                     restore_best_weights=True) #define early stopping  \n",
        "\n",
        "model_ce.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",  run_eagerly=True)\n",
        "\n",
        "history_ce = model_ce.fit(train_data,\n",
        "                    train_data,\n",
        "                    batch_size=16,\n",
        "                    epochs = 1000,\n",
        "                    validation_data=(train_data_validation, train_data_validation),\n",
        "                    callbacks=[\n",
        "                        # checkpoint_ce,\n",
        "                        early_stopping_ce],\n",
        "                    verbose = 0,\n",
        "                    validation_batch_size = 16,\n",
        "                    shuffle=True)\n",
        "model_ce.summary()"
      ],
      "metadata": {
        "id": "-7dHzwjCOxwn"
      },
      "id": "-7dHzwjCOxwn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Der Focal Loss wird mit Œ≥ = 2 initalisiert.\n",
        "\n",
        "[Quelle](https://github.com/qubvel/segmentation_models/blob/master/segmentation_models/losses.py)"
      ],
      "metadata": {
        "id": "j0zI_qD6bgCB"
      },
      "id": "j0zI_qD6bgCB"
    },
    {
      "cell_type": "code",
      "source": [
        "focal_loss = sm.losses.BinaryFocalLoss()\n",
        "# checkpoint_focal = tf.keras.callbacks.ModelCheckpoint(\"checkpoint_focal\",\n",
        "#                                                    save_weights_only=True) #generate checkpoints\n",
        "early_stopping_focal = tf.keras.callbacks.EarlyStopping(patience=20,\n",
        "                                                     restore_best_weights=True) #define early stopping  \n",
        "\n",
        "model_focal.compile(loss=focal_loss, optimizer=\"adam\",  run_eagerly=True)\n",
        "\n",
        "\n",
        "history_focal = model_focal.fit(train_data,\n",
        "                                train_data,\n",
        "                                batch_size=16,\n",
        "                                epochs = 1000,\n",
        "                    validation_data=(train_data_validation, train_data_validation),\n",
        "                    callbacks=[\n",
        "                        # checkpoint_focal,\n",
        "                        early_stopping_focal],\n",
        "                    verbose = 0,\n",
        "                    validation_batch_size = 16,\n",
        "                    shuffle=True)\n",
        "model_focal.summary()"
      ],
      "metadata": {
        "id": "hxnX31oHOyML"
      },
      "id": "hxnX31oHOyML",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Huber wird mit Œ¥ = 0,5 initalisiert"
      ],
      "metadata": {
        "id": "JYwQPlllYwT5"
      },
      "id": "JYwQPlllYwT5"
    },
    {
      "cell_type": "code",
      "source": [
        "huber = tf.keras.losses.Huber(delta= 0.5, reduction=\"auto\", name=\"huber_loss\")\n",
        "# checkpoint_huber = tf.keras.callbacks.ModelCheckpoint(\"checkpoint_huber\",\n",
        "#                                                    save_weights_only=True) #generate checkpoints\n",
        "early_stopping_huber = tf.keras.callbacks.EarlyStopping(patience=20,\n",
        "                                                     restore_best_weights=True) #define early stopping  \n",
        "\n",
        "model_huber.compile(loss=huber, optimizer=\"adam\",  run_eagerly=True) \n",
        "\n",
        "history_huber = model_huber.fit(train_data,\n",
        "                    train_data,\n",
        "                    batch_size=16,\n",
        "                    epochs = 1000,\n",
        "                    validation_data=(train_data_validation, train_data_validation),\n",
        "                    callbacks=[\n",
        "                        #checkpoint_huber,\n",
        "                        early_stopping_huber],\n",
        "                    verbose = 0,\n",
        "                    validation_batch_size = 16,\n",
        "                    shuffle=True)\n",
        "model_huber.summary()"
      ],
      "metadata": {
        "id": "Vq_hbVqLOyhb"
      },
      "id": "Vq_hbVqLOyhb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotte den Trainingsverlauf f√ºr jedes Modell"
      ],
      "metadata": {
        "id": "-bv_TOU8V-8E"
      },
      "id": "-bv_TOU8V-8E"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_performance():\n",
        "    '''\n",
        "    Plot the training performance of each model \n",
        "    '''\n",
        "    figure, axis = plt.subplots(ncols=1,nrows=5, figsize=(10, 20))\n",
        "    # first row: complicated architecture\n",
        "    axis[0].plot(history_mse.history[\"loss\"], label=\"Training Loss\")\n",
        "    axis[0].plot(history_mse.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "    axis[0].set_title('Performance Training with model_mse')\n",
        "    axis[0].set(xlabel='Epochs', ylabel='Loss')\n",
        "    axis[0].legend()\n",
        "    \n",
        "    axis[1].plot(history_mae.history[\"loss\"], label=\"Training Loss\")\n",
        "    axis[1].plot(history_mae.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "    axis[1].set_title('Performance Training with model_mae')\n",
        "    axis[1].set(xlabel='Epochs', ylabel='Loss')\n",
        "    axis[1].legend()\n",
        "    \n",
        "    axis[2].plot(history_huber.history[\"loss\"], label=\"Training Loss\")\n",
        "    axis[2].plot(history_huber.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "    axis[2].set_title('Performance Training with model_huber')\n",
        "    axis[2].set(xlabel='Epochs', ylabel='Loss')\n",
        "    axis[2].legend()\n",
        "    \n",
        "    #second row: simple architectures \n",
        "    axis[3].plot(history_ce.history[\"loss\"], label=\"Training Loss\")\n",
        "    axis[3].plot(history_ce.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "    axis[3].set_title('Performance Training by model_ce')\n",
        "    axis[3].set(xlabel='Epochs', ylabel='Loss')\n",
        "    axis[3].legend()\n",
        "    \n",
        "    axis[4].plot(history_focal.history[\"loss\"], label=\"Training Loss\")\n",
        "    axis[4].plot(history_focal.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "    axis[4].set_title('Performance Training by model_focal')\n",
        "    axis[4].set(xlabel='Epochs', ylabel='Loss')\n",
        "    axis[4].legend()\n",
        "\n",
        "    save_fig(\"Performances for training\")\n",
        "    plt.show()\n",
        "\n",
        "plot_training_performance()"
      ],
      "metadata": {
        "id": "uDAa0dQZ3eBt"
      },
      "id": "uDAa0dQZ3eBt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Diese Funktion extrahiert die letzte ausgef√ºhrte Epoche f√ºr ein Model"
      ],
      "metadata": {
        "id": "vuHx_Q-XWG__"
      },
      "id": "vuHx_Q-XWG__"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_last_epoch(history, max = \"max\") -> int:\n",
        "  epochs = np.array([])\n",
        "  stopped_epoch = np.array([])\n",
        "  for key, value in enumerate(history.history[\"loss\"]):\n",
        "    epochs = np.append(epochs,key)\n",
        "  stopped_epoch = np.append(stopped_epoch,int(np.max(epochs)))\n",
        "  if max == \"max\":\n",
        "    return int(np.max(epochs))\n",
        "  else:\n",
        "    return epochs\n"
      ],
      "metadata": {
        "id": "ORFJfp_z2g-_"
      },
      "id": "ORFJfp_z2g-_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for history in [\n",
        "history_mse,\n",
        "    history_mae,\n",
        "    history_ce,\n",
        "    history_focal,\n",
        "    history_huber ]:\n",
        "    print(get_last_epoch(history))"
      ],
      "metadata": {
        "id": "8E7sn_dipW0a"
      },
      "id": "8E7sn_dipW0a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotte den TNSE-Graph f√ºr den latenten Raum in einem Modell"
      ],
      "metadata": {
        "id": "N4DXnaMkWe86"
      },
      "id": "N4DXnaMkWe86"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_tnse_performance_bottleneck_layer():\n",
        "    '''\n",
        "    Plot the training performance of each model \n",
        "    '''\n",
        "    figure, axis = plt.subplots(ncols=1,nrows=5, figsize=(10, 20))\n",
        "    # first row: complicated architecture\n",
        "    encoder = model_mse.get_encoder()\n",
        "    vis_data_latent = encoder.predict(train_data)\n",
        "    tsne = TSNE(n_components=2)\n",
        "    tsne_data = tsne.fit_transform(vis_data_latent)\n",
        "    axis[0].scatter(tsne_data[train_labels == 0, 0], \n",
        "            tsne_data[train_labels == 0, 1], c=\"grey\", alpha=0.1, label=\"inlier\")\n",
        "    axis[0].scatter(tsne_data[train_labels == 1, 0], \n",
        "            tsne_data[train_labels == 1, 1], c=\"crimson\", alpha=1, label=\"outlier\")\n",
        "    axis[0].set_title('t-SNE for outliers/inliers on the latent manifold for model_mse')\n",
        "    axis[0].legend()\n",
        "    \n",
        "    encoder = model_mae.get_encoder()\n",
        "    vis_data_latent = encoder.predict(train_data)\n",
        "    tsne = TSNE(n_components=2)\n",
        "    tsne_data = tsne.fit_transform(vis_data_latent)\n",
        "    axis[1].scatter(tsne_data[train_labels == 0, 0], \n",
        "            tsne_data[train_labels == 0, 1], c=\"grey\", alpha=0.1, label=\"inlier\")\n",
        "    axis[1].scatter(tsne_data[train_labels == 1, 0], \n",
        "            tsne_data[train_labels == 1, 1], c=\"crimson\", alpha=1, label=\"outlier\")\n",
        "    axis[1].set_title('t-SNE for outliers/inliers on the latent manifold for model_mae')\n",
        "    axis[1].legend()\n",
        "    \n",
        "    encoder = model_huber.get_encoder()\n",
        "    vis_data_latent = encoder.predict(train_data)\n",
        "    tsne = TSNE(n_components=2)\n",
        "    tsne_data = tsne.fit_transform(vis_data_latent)\n",
        "    axis[2].scatter(tsne_data[train_labels == 0, 0], \n",
        "            tsne_data[train_labels == 0, 1], c=\"grey\", alpha=0.1, label=\"inlier\")\n",
        "    axis[2].scatter(tsne_data[train_labels == 1, 0], \n",
        "            tsne_data[train_labels == 1, 1], c=\"crimson\", alpha=1, label=\"outlier\")\n",
        "    axis[2].set_title('t-SNE for outliers/inliers on the latent manifold for model_huber')\n",
        "    axis[2].legend()\n",
        "    \n",
        "    encoder = model_ce.get_encoder()\n",
        "    vis_data_latent = encoder.predict(train_data)\n",
        "    tsne = TSNE(n_components=2)\n",
        "    tsne_data = tsne.fit_transform(vis_data_latent)\n",
        "    axis[3].scatter(tsne_data[train_labels == 0, 0], \n",
        "            tsne_data[train_labels == 0, 1], c=\"grey\", alpha=0.1, label=\"inlier\")\n",
        "    axis[3].scatter(tsne_data[train_labels == 1, 0], \n",
        "            tsne_data[train_labels == 1, 1], c=\"crimson\", alpha=1, label=\"outlier\")\n",
        "    axis[3].set_title('t-SNE for outliers/inliers on the latent manifold for model_ce')\n",
        "    axis[3].legend()\n",
        "    \n",
        "    encoder = model_focal.get_encoder()\n",
        "    vis_data_latent = encoder.predict(train_data)\n",
        "    tsne = TSNE(n_components=2)\n",
        "    tsne_data = tsne.fit_transform(vis_data_latent)\n",
        "    axis[4].scatter(tsne_data[train_labels == 0, 0], \n",
        "            tsne_data[train_labels == 0, 1], c=\"grey\", alpha=0.1, label=\"inlier\")\n",
        "    axis[4].scatter(tsne_data[train_labels == 1, 0], \n",
        "            tsne_data[train_labels == 1, 1], c=\"crimson\", alpha=1, label=\"outlier\")\n",
        "    axis[4].set_title('t-SNE for outliers/inliers on the latent manifold for model_focal')\n",
        "    axis[4].legend()\n",
        "\n",
        "    save_fig(\"TNSE-Distrubution in bottleneck-layer for all models\")\n",
        "    plt.show()\n",
        "\n",
        "plot_tnse_performance_bottleneck_layer()"
      ],
      "metadata": {
        "id": "5nHCyBoqZz_L"
      },
      "id": "5nHCyBoqZz_L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation on test data"
      ],
      "metadata": {
        "id": "84g4adTVfcmC"
      },
      "id": "84g4adTVfcmC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generiere den Rekonstruktionsfehler"
      ],
      "metadata": {
        "id": "0xn6JL8yjgU8"
      },
      "id": "0xn6JL8yjgU8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Um den Rekonstruktionsfehler zu berechnen, wird als Metrik der MSE verwendet."
      ],
      "metadata": {
        "id": "_izo1C8vjVCt"
      },
      "id": "_izo1C8vjVCt"
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_error_df(model, metric:str = \"mean_squared_error\", data = test_data):\n",
        "  if model in [model_mse,model_mae,model_ce,model_focal,model_huber]:\n",
        "    print(\"Calculate reconstruction error for model: \", model)\n",
        "    if metric == \"mean_squared_error\":\n",
        "      mse = np.mean(np.power(data - model.predict(data), 2), axis=1)\n",
        "      return(pd.DataFrame({'reconstruction_error': mse,\n",
        "                              'anomaly': test_labels}))\n",
        "    else:\n",
        "      print(\"No further metric functions are implemented yet\")\n",
        "      return\n",
        "  else:\n",
        "    raise(\"Choosen model is not supported yet. Please choose an model in [model_mse,model_mae,model_ce,model_focal,model_huber]\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L0PLrhpKbBXz"
      },
      "id": "L0PLrhpKbBXz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse_error_df = gen_error_df(model_mse)\n",
        "mae_error_df = gen_error_df(model_mae)\n",
        "ce_error_df = gen_error_df(model_ce)\n",
        "focal_error_df = gen_error_df(model_focal)\n",
        "huber_error_df = gen_error_df(model_huber)"
      ],
      "metadata": {
        "id": "Uq-KLMRCBXBF"
      },
      "id": "Uq-KLMRCBXBF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ROC und AUC"
      ],
      "metadata": {
        "id": "BkPb4nFSj6hZ"
      },
      "id": "BkPb4nFSj6hZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualisiere das Ergebnis f√ºr die Testdaten f√ºr alle Modelle"
      ],
      "metadata": {
        "id": "aBEolzL2kucr"
      },
      "id": "aBEolzL2kucr"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_test_performance_roc(error_df = [mse_error_df,\n",
        "                                          mae_error_df,\n",
        "                                          huber_error_df,\n",
        "                                          ce_error_df,\n",
        "                                          focal_error_df]):\n",
        "    '''\n",
        "    Plot the ROC curve performance of each model \n",
        "    '''\n",
        "    figure, axis = plt.subplots(ncols=1,nrows=5, figsize=(10, 20))\n",
        "    # first row: complicated architecture\n",
        "    fpr, tpr, thresholds = roc_curve(error_df[0].anomaly, error_df[0].reconstruction_error)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    axis[0].plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\n",
        "    axis[0].plot([0,1],[0,1],'r--')\n",
        "    axis[0].legend(loc='lower right')\n",
        "    axis[0].set_title('Receiver Operating Characteristic for Model_MSE')\n",
        "    axis[0].set_xlim([-0.001, 1])\n",
        "    axis[0].set_ylim([0, 1.001])\n",
        "    axis[0].set(xlabel='False Positive Rate', ylabel='True Positive Rate')\n",
        "    axis[0].legend()\n",
        "    \n",
        "    fpr, tpr, thresholds = roc_curve(error_df[1].anomaly, error_df[1].reconstruction_error)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    axis[1].plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\n",
        "    axis[1].plot([0,1],[0,1],'r--')\n",
        "    axis[1].legend(loc='lower right')\n",
        "    axis[1].set_title('Receiver Operating Characteristic for Model_MAE')\n",
        "    axis[1].set_xlim([-0.001, 1])\n",
        "    axis[1].set_ylim([0, 1.001])\n",
        "    axis[1].set(xlabel='False Positive Rate', ylabel='True Positive Rate')\n",
        "    axis[1].legend()\n",
        "    \n",
        "    fpr, tpr, thresholds = roc_curve(error_df[2].anomaly, error_df[2].reconstruction_error)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    axis[2].plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\n",
        "    axis[2].plot([0,1],[0,1],'r--')\n",
        "    axis[2].legend(loc='lower right')\n",
        "    axis[2].set_title('Receiver Operating Characteristic for Model_Huber')\n",
        "    axis[2].set_xlim([-0.001, 1])\n",
        "    axis[2].set_ylim([0, 1.001])\n",
        "    axis[2].set(xlabel='False Positive Rate', ylabel='True Positive Rate')\n",
        "    axis[2].legend()\n",
        "    \n",
        "    #second row: simple architectures \n",
        "    fpr, tpr, thresholds = roc_curve(error_df[3].anomaly, error_df[3].reconstruction_error)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    axis[3].plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\n",
        "    axis[3].plot([0,1],[0,1],'r--')\n",
        "    axis[3].legend(loc='lower right')\n",
        "    axis[3].set_title('Receiver Operating Characteristic for Model_CE')\n",
        "    axis[3].set_xlim([-0.001, 1])\n",
        "    axis[3].set_ylim([0, 1.001])\n",
        "    axis[3].set(xlabel='False Positive Rate', ylabel='True Positive Rate')\n",
        "    axis[3].legend()\n",
        "    \n",
        "    fpr, tpr, thresholds = roc_curve(error_df[4].anomaly, error_df[4].reconstruction_error)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    axis[4].plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\n",
        "    axis[4].plot([0,1],[0,1],'r--')\n",
        "    axis[4].legend(loc='lower right')\n",
        "    axis[4].set_title('Receiver Operating Characteristic for Model_Focal')\n",
        "    axis[4].set_xlim([-0.001, 1])\n",
        "    axis[4].set_ylim([0, 1.001])\n",
        "    axis[4].set(xlabel='False Positive Rate', ylabel='True Positive Rate')\n",
        "    axis[4].legend()\n",
        "\n",
        "\n",
        "    save_fig(\"Roc-Curves for test data\")\n",
        "    plt.show()\n",
        "\n",
        "plot_test_performance_roc()"
      ],
      "metadata": {
        "id": "dbo0o_rvwZ-T"
      },
      "id": "dbo0o_rvwZ-T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Precision, Recall und F1-Score\n",
        "\n",
        "Da diese Metriken davon abh√§ngig sind, welche Threshold man f√ºr die Klassifikation w√§hlt, wird die Threshold wie folgt gew√§hlt, um den F1-Score zu maximieren:\n",
        "\n",
        "\n",
        "*   Bestimme die Threshold, f√ºr die die Precision und der Recall gleich sind\n",
        "*   Da es sich bei dem F1-Score um ein harmonisches Mittel handelt, wird so der F1-Score maximiert\n",
        "\n"
      ],
      "metadata": {
        "id": "fda09jLOkFj3"
      },
      "id": "fda09jLOkFj3"
    },
    {
      "cell_type": "code",
      "source": [
        "prec, recall, thresholds = precision_recall_curve(focal_error_df[\"anomaly\"], focal_error_df[\"reconstruction_error\"])\n",
        "\n",
        "best_idx = np.argmin(np.abs(prec-recall)[:int(len(np.abs(prec-recall))*-0.1)]) #to resolve an bug, the last four obsvervations have to be excluded\n",
        "best_prec, best_recall = prec[best_idx], recall[best_idx]\n",
        "print(f\"Best precision {np.round(best_prec, 2)}, \"\\\n",
        "      f\"recall: {np.round(best_recall, 2)} at {np.round(thresholds[best_idx], 2)} threshold.\")"
      ],
      "metadata": {
        "id": "Qncty3ipkCyc"
      },
      "id": "Qncty3ipkCyc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "auc_score = auc(recall, prec)\n",
        "auc_score"
      ],
      "metadata": {
        "id": "X18MiBKgtuoj"
      },
      "id": "X18MiBKgtuoj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualisiere das Ergebnis f√ºr alle Ergebnisse"
      ],
      "metadata": {
        "id": "cHwMS5yjkqVZ"
      },
      "id": "cHwMS5yjkqVZ"
    },
    {
      "cell_type": "code",
      "source": [
        "## Recall Vs Precision\n",
        "def plot_test_performance_recall_vs_precision(error_df = [mse_error_df,\n",
        "                                          mae_error_df,\n",
        "                                          huber_error_df,\n",
        "                                          ce_error_df,\n",
        "                                          focal_error_df]):\n",
        "    figure, axis = plt.subplots(ncols=2,nrows=5, figsize=(30, 30))\n",
        "    # first row: complicated architecture\n",
        "    precision, recall, thresholds = precision_recall_curve(error_df[0].anomaly, error_df[0].reconstruction_error)\n",
        "    best_idx = np.argmin(np.abs(precision-recall)[:-4])\n",
        "    best_prec, best_recall = precision[best_idx], recall[best_idx]\n",
        "    precision_recall_auc = auc(recall, precision)\n",
        "    axis[0,0].plot(recall, precision, label='AUC = %0.4f'% precision_recall_auc)\n",
        "    axis[0,0].legend(loc='lower right')\n",
        "    axis[0,0].set_title('Recall vs Precision for Model_MSE')\n",
        "    axis[0,0].set(xlabel='Recall', ylabel='Precision')\n",
        "    axis[0,1].set_title('Determine the threshold for Model_MSE')\n",
        "    axis[0,1].plot(thresholds, precision[:-1], label=\"precision\")\n",
        "    axis[0,1].plot(thresholds, recall[:-1], label=\"recall\")\n",
        "    axis[0,1].axvline(thresholds[best_idx], 0, 1, c=\"grey\", linestyle=\"--\")\n",
        "    axis[0,1].set_xlabel(\"thresholds\")\n",
        "    axis[0,1].legend()\n",
        "\n",
        "    precision, recall, thresholds = precision_recall_curve(error_df[1].anomaly, error_df[1].reconstruction_error)\n",
        "    best_idx = np.argmin(np.abs(precision-recall)[:-4])\n",
        "    best_prec, best_recall = precision[best_idx], recall[best_idx]\n",
        "    precision_recall_auc = auc(recall, precision)\n",
        "    axis[1,0].plot(recall, precision, label='AUC = %0.4f'% precision_recall_auc)\n",
        "    axis[1,0].legend(loc='lower right')\n",
        "    axis[1,0].set_title('Recall vs Precision for Model_MAE')\n",
        "    axis[1,0].set(xlabel='Recall', ylabel='Precision')\n",
        "    axis[1,1].set_title('Determine the threshold for Model_MAE')\n",
        "    axis[1,1].plot(thresholds, precision[:-1], label=\"precision\")\n",
        "    axis[1,1].plot(thresholds, recall[:-1], label=\"recall\")\n",
        "    axis[1,1].axvline(thresholds[best_idx], 0, 1, c=\"grey\", linestyle=\"--\")\n",
        "    axis[1,1].set_xlabel(\"thresholds\")\n",
        "    axis[1,1].legend()\n",
        "\n",
        "    precision, recall, thresholds = precision_recall_curve(error_df[2].anomaly, error_df[2].reconstruction_error)\n",
        "    best_idx = np.argmin(np.abs(precision-recall)[:-4])\n",
        "    best_prec, best_recall = precision[best_idx], recall[best_idx]\n",
        "    precision_recall_auc = auc(recall, precision)\n",
        "    axis[2,0].plot(recall, precision, label='AUC = %0.4f'% precision_recall_auc)\n",
        "    axis[2,0].legend(loc='lower right')\n",
        "    axis[2,0].set_title('Recall vs Precision for Model_Huber')\n",
        "    axis[2,0].set(xlabel='Recall', ylabel='Precision')\n",
        "    axis[2,1].set_title('Determine the threshold for Model_Huber')\n",
        "    axis[2,1].plot(thresholds, precision[:-1], label=\"precision\")\n",
        "    axis[2,1].plot(thresholds, recall[:-1], label=\"recall\")\n",
        "    axis[2,1].axvline(thresholds[best_idx], 0, 1, c=\"grey\", linestyle=\"--\")\n",
        "    axis[2,1].set_xlabel(\"thresholds\")\n",
        "    axis[2,1].legend()\n",
        "\n",
        "    precision, recall, thresholds = precision_recall_curve(error_df[3].anomaly, error_df[3].reconstruction_error)\n",
        "    best_idx = np.argmin(np.abs(precision-recall)[:-4])\n",
        "    best_prec, best_recall = precision[best_idx], recall[best_idx]\n",
        "    precision_recall_auc = auc(recall, precision)\n",
        "    axis[3,0].plot(recall, precision, label='AUC = %0.4f'% precision_recall_auc)\n",
        "    axis[3,0].legend(loc='lower right')\n",
        "    axis[3,0].set_title('Recall vs Precision for Model_CE')\n",
        "    axis[3,0].set(xlabel='Recall', ylabel='Precision')\n",
        "    axis[3,1].set_title('Determine the threshold for Model_CE')\n",
        "    axis[3,1].plot(thresholds, precision[:-1], label=\"precision\")\n",
        "    axis[3,1].plot(thresholds, recall[:-1], label=\"recall\")\n",
        "    axis[3,1].axvline(thresholds[best_idx], 0, 1, c=\"grey\", linestyle=\"--\")\n",
        "    axis[3,1].set_xlabel(\"thresholds\")\n",
        "    axis[3,1].legend()\n",
        "\n",
        "    precision, recall, thresholds = precision_recall_curve(error_df[4].anomaly, error_df[4].reconstruction_error)\n",
        "    best_idx = np.argmin(np.abs(precision-recall)[:-4])\n",
        "    best_prec, best_recall = precision[best_idx], recall[best_idx]\n",
        "    precision_recall_auc = auc(recall, precision)\n",
        "    axis[4,0].plot(recall, precision, label='AUC = %0.4f'% precision_recall_auc)\n",
        "    axis[4,0].legend(loc='lower right')\n",
        "    axis[4,0].set_title('Recall vs Precision for Model_FL')\n",
        "    axis[4,0].set(xlabel='Recall', ylabel='Precision')\n",
        "    axis[4,1].set_title('Determine the threshold for Model_Focal')\n",
        "    axis[4,1].plot(thresholds, precision[:-1], label=\"precision\")\n",
        "    axis[4,1].plot(thresholds, recall[:-1], label=\"recall\")\n",
        "    axis[4,1].axvline(thresholds[best_idx], 0, 1, c=\"grey\", linestyle=\"--\")\n",
        "    axis[4,1].set_xlabel(\"thresholds\")\n",
        "    axis[4,1].legend()\n",
        "\n",
        "    save_fig(\"Recall vs Precision curves for test data\")\n",
        "    sns.despine()\n",
        "    plt.show()\n",
        "\n",
        "plot_test_performance_recall_vs_precision()"
      ],
      "metadata": {
        "id": "fUY3nZZYg3ty"
      },
      "id": "fUY3nZZYg3ty",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_precision_threshold(error_df):\n",
        "  '''\n",
        "  Return an array, which includes the thresholds with the second highest precision score for each model\n",
        "  '''\n",
        "  prec, recall, threshold = precision_recall_curve(error_df[\"anomaly\"], error_df[\"reconstruction_error\"])\n",
        "  best_idx = np.argmin(np.abs(prec-recall)[:-4])\n",
        "  best_prec, best_recall = prec[best_idx], recall[best_idx]\n",
        "\n",
        "  return threshold[best_idx]\n",
        "\n",
        "for i in [mse_error_df, mae_error_df, huber_error_df, ce_error_df, focal_error_df]:   \n",
        "  print(get_precision_threshold(i))"
      ],
      "metadata": {
        "id": "pQXAUcPkbAd-"
      },
      "id": "pQXAUcPkbAd-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_thresholds_data(index:int,save_name:str,error_df = [mse_error_df,\n",
        "                                          mae_error_df,\n",
        "                                          huber_error_df,\n",
        "                                          ce_error_df,\n",
        "                                          focal_error_df]):\n",
        "  if index > len(error_df) - 1 or index < 0:\n",
        "    raise(\"Index out of range implemented. Please correct your statement\")\n",
        "  else:\n",
        "    groups = error_df[index].groupby('anomaly')\n",
        "    threshold = get_precision_threshold(error_df[index])\n",
        "    fig, ax = plt.subplots()\n",
        "    for name, group in groups:\n",
        "      ax.plot(group.index, group.reconstruction_error, marker='o', ms=3.5, linestyle='',\n",
        "              label= \"Anomaly\" if name == 1 else \"Normal\")\n",
        "    ax.hlines(threshold, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", zorder=100, label='Threshold')\n",
        "    ax.legend()\n",
        "    plt.title(\"Reconstruction error visualisation to identify anomalies\")\n",
        "    plt.ylabel(\"Reconstruction error\")\n",
        "    plt.xlabel(\"Data point index\")\n",
        "    save_fig(save_name)\n",
        "    plt.show();\n"
      ],
      "metadata": {
        "id": "7irw2FnvhxlO"
      },
      "id": "7irw2FnvhxlO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_thresholds_data(0, \"Reconstruction error visualisation to identify anomalies with model_mse\")\n",
        "plot_thresholds_data(1, \"Reconstruction error visualisation to identify anomalies with model_mae\")\n",
        "plot_thresholds_data(2, \"Reconstruction error visualisation to identify anomalies with model_huber\")\n",
        "plot_thresholds_data(3, \"Reconstruction error visualisation to identify anomalies with model_ce\")\n",
        "plot_thresholds_data(4, \"Reconstruction error visualisation to identify anomalies with model_focal\")"
      ],
      "metadata": {
        "id": "fNyor5Gpl986"
      },
      "id": "fNyor5Gpl986",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Predict class based on error threshold\n",
        "\n",
        "# Weitermachen\n",
        "def plot_cm_matrix(index:int,save_name:str,error_df = [mse_error_df,\n",
        "                                          mae_error_df,\n",
        "                                          huber_error_df,\n",
        "                                          ce_error_df,\n",
        "                                          focal_error_df],\n",
        "                   LABELS = [\"Non Fraud\", \"Fraud\"]):\n",
        "  if index > len(error_df) - 1 or index < 0:\n",
        "    raise(\"Index out of range implemented. Please correct your statement\")\n",
        "  else:\n",
        "    error_df = error_df[index]\n",
        "    threshold = get_precision_threshold(error_df)\n",
        "    y_pred = [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\n",
        "    conf_matrix = confusion_matrix(error_df.anomaly, y_pred,labels=[0,1])\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    sns.heatmap(conf_matrix,xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\",cmap='Blues');\n",
        "    plt.title(\"Confusion matrix\")\n",
        "    plt.ylabel('True class')\n",
        "    plt.xlabel('Predicted class')\n",
        "    save_fig(save_name)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "5TALJXevjfyZ"
      },
      "id": "5TALJXevjfyZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cm_matrix(0, \"CM-matrix visualisation to identify anomalies with model_mse\")\n",
        "plot_cm_matrix(1, \"CM-matrix visualisation to identify anomalies with model_mae\")\n",
        "plot_cm_matrix(2, \"CM-matrix visualisation to identify anomalies with model_huber\")\n",
        "plot_cm_matrix(3, \"CM-matrix visualisation to identify anomalies with model_ce\")\n",
        "plot_cm_matrix(4, \"CM-matrix visualisation to identify anomalies with model_focal\")"
      ],
      "metadata": {
        "id": "pEr2o-V2mij-"
      },
      "id": "pEr2o-V2mij-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_f1_scores(index, error_df =  \n",
        "                        [mse_error_df,\n",
        "                         mae_error_df,\n",
        "                         huber_error_df,\n",
        "                         ce_error_df,\n",
        "                         focal_error_df]\n",
        "                        ):\n",
        "  if index > len(error_df) - 1 or index < 0:\n",
        "    raise(\"Index out of range implemented. Please correct your statement\")\n",
        "  error_df = error_df[index]\n",
        "  threshold = get_precision_threshold(error_df)\n",
        "  y_pred =  [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\n",
        "  print('F1_Score with threshold:', threshold)\n",
        "  return(f1_score(error_df.anomaly, y_pred))\n",
        "calculate_f1_scores(0)"
      ],
      "metadata": {
        "id": "z6SVA051j1ei"
      },
      "id": "z6SVA051j1ei",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "error_df = [mse_error_df,\n",
        "             mae_error_df,\n",
        "             huber_error_df,\n",
        "             ce_error_df,\n",
        "             focal_error_df]\n",
        "for i in error_df:\n",
        "  threshold = get_precision_threshold(i) #specify here the index \n",
        "  y_pred =  [1 if e > threshold else 0 for e in i.reconstruction_error.values]\n",
        "\n",
        "  cm1 = confusion_matrix(i.anomaly, y_pred,labels=[1,0])\n",
        "  print('Confusion Matrix Val: \\n', cm1)\n",
        "\n",
        "\n",
        "  total1=sum(sum(cm1))\n",
        "  #####from confusion matrix calculate accuracy\n",
        "\n",
        "  accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
        "  print ('Accuracy Val: ', accuracy1)\n",
        "\n",
        "\n",
        "  sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
        "  print('Sensitivity Val: ', sensitivity1 )\n",
        "\n",
        "\n",
        "  specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
        "  print('Specificity Val: ', specificity1)\n",
        "\n",
        "  KappaValue=cohen_kappa_score(i.anomaly, y_pred)\n",
        "  print(\"Kappa Value :\",KappaValue)\n",
        "  AUC=roc_auc_score(i.anomaly, y_pred)\n",
        "\n",
        "  print(\"AUC         :\",AUC)\n",
        "\n",
        "  print(\"F1-Score Val  : \",f1_score(i.anomaly, y_pred))\n",
        "\n",
        "  print(\"_______________________________________\")"
      ],
      "metadata": {
        "id": "Z_rMWW0NcR0r"
      },
      "id": "Z_rMWW0NcR0r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simulation"
      ],
      "metadata": {
        "id": "4tHAq-c3oQn6"
      },
      "id": "4tHAq-c3oQn6"
    },
    {
      "cell_type": "code",
      "source": [
        "def simulation(\n",
        "    loss,\n",
        "    model,\n",
        "    patience,\n",
        "    epochs,\n",
        "    n:int = 0,\n",
        "    end:int = 30,\n",
        "    optimizer=\"adam\",\n",
        "    test_size = 0.2,\n",
        "    batch_size=16,\n",
        "    validation_batch_size = 16,\n",
        "    delta = 0.5\n",
        "    ):\n",
        "  \n",
        "  stopped_epochs = []\n",
        "  auc_values_roc = []\n",
        "  auc_values_recall_prec = []\n",
        "  f1_values = []\n",
        "\n",
        "  if loss == \"focal_loss\":\n",
        "    loss = sm.losses.BinaryFocalLoss()\n",
        "    print(\"Start the Simulation:\")\n",
        "    while(True):\n",
        "      print(\"Lossfunction: \", loss),\n",
        "      print(\"Iteration: \",n)\n",
        "      #Step 1: Splitt data\n",
        "      train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "        data,\n",
        "        labels_bool,\n",
        "        test_size=test_size\n",
        "        )\n",
        "      train_data_no_validation, train_data_validation, train_data_no_validation_labels, train_data_validation_labels = train_test_split(\n",
        "        train_data,\n",
        "        train_labels,\n",
        "        test_size = test_size\n",
        "        )\n",
        "      \n",
        "      #Step2: Train data\n",
        "      model = model\n",
        "      early_stopping = tf.keras.callbacks.EarlyStopping(patience=patience,\n",
        "                                                      restore_best_weights=True) #generate callback\n",
        "      model.compile(loss=loss, optimizer=optimizer, run_eagerly=True)\n",
        "      history = model.fit(train_data,\n",
        "                      train_data,\n",
        "                      batch_size=batch_size,\n",
        "                      epochs = epochs,\n",
        "                      validation_data=(train_data_validation, train_data_validation),\n",
        "                      callbacks=[\n",
        "                          early_stopping,\n",
        "                          ],\n",
        "                      verbose = 0,\n",
        "                      validation_batch_size = validation_batch_size,\n",
        "                      shuffle=True)\n",
        "      stopped_epochs.append(get_last_epoch(history)) #append stopped epoch \n",
        "\n",
        "      #Step3: Evaluate test data with AUC\n",
        "      error_df = gen_error_df(model)\n",
        "      fpr, tpr, thresholds = roc_curve(error_df.anomaly, error_df.reconstruction_error)\n",
        "      auc_values_roc.append(auc(fpr, tpr))\n",
        "\n",
        "      #Step4: Evaluate test data with AUC\n",
        "      prec, recall, thresholds = precision_recall_curve(error_df[\"anomaly\"], error_df[\"reconstruction_error\"])\n",
        "      auc_values_recall_prec.append(auc(recall, prec))\n",
        "      best_idx = np.argmin(np.abs(prec-recall)[:-4])\n",
        "      best_prec, best_recall = prec[best_idx], recall[best_idx]\n",
        "      threshold = thresholds[best_idx]\n",
        "      y_pred =  [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\n",
        "      f1_values.append(f1_score(error_df.anomaly, y_pred))\n",
        "\n",
        "      if (n >= end):\n",
        "        print(\"Final Output:\")\n",
        "        print(\"Return stopped epochs per iteration, \")\n",
        "        print(\"Return auc values per iteration, \")\n",
        "        print(\"Return precision scores with best thresholds per iteration, \")\n",
        "        print(\"Return recall scores with best thresholds per iteration, \")\n",
        "        print(\"Return f1 scores with best thresholds per iteration, \")\n",
        "        return(stopped_epochs,auc_values_roc,auc_values_recall_prec,f1_values)\n",
        "        break\n",
        "      n+=1\n",
        "  \n",
        "  elif loss == \"huber_loss\":\n",
        "    loss = tf.keras.losses.Huber(delta=delta, reduction=\"auto\", name=\"huber_loss\")\n",
        "    print(\"Start the Simulation:\")\n",
        "    while(True):\n",
        "      print(\"Lossfunction: \", loss),\n",
        "      print(\"Iteration: \",n)\n",
        "      #Step 1: Splitt data\n",
        "      train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "        data,\n",
        "        labels_bool,\n",
        "        test_size=test_size\n",
        "        )\n",
        "      train_data_no_validation, train_data_validation, train_data_no_validation_labels, train_data_validation_labels = train_test_split(\n",
        "        train_data,\n",
        "        train_labels,\n",
        "        test_size = test_size\n",
        "        )\n",
        "      \n",
        "      #Step2: Train data\n",
        "      model = model\n",
        "      early_stopping = tf.keras.callbacks.EarlyStopping(patience=patience,\n",
        "                                                      restore_best_weights=True) #generate callback\n",
        "      model.compile(loss=loss, optimizer=optimizer, run_eagerly=True)\n",
        "      history = model.fit(train_data,\n",
        "                      train_data,\n",
        "                      batch_size=batch_size,\n",
        "                      epochs = epochs,\n",
        "                      validation_data=(train_data_validation, train_data_validation),\n",
        "                      callbacks=[\n",
        "                          early_stopping,\n",
        "                          ],\n",
        "                      verbose = 0,\n",
        "                      validation_batch_size = validation_batch_size,\n",
        "                      shuffle=True)\n",
        "      stopped_epochs.append(get_last_epoch(history)) #append stopped epoch \n",
        "\n",
        "      #Step3: Evaluate test data with AUC\n",
        "      error_df = gen_error_df(model)\n",
        "      fpr, tpr, thresholds = roc_curve(error_df.anomaly, error_df.reconstruction_error)\n",
        "      auc_values_roc.append(auc(fpr, tpr))\n",
        "\n",
        "      #Step4: Evaluate test data with AUC\n",
        "      prec, recall, thresholds = precision_recall_curve(error_df[\"anomaly\"], error_df[\"reconstruction_error\"])\n",
        "      auc_values_recall_prec.append(auc(recall, prec))\n",
        "      best_idx = np.argmin(np.abs(prec-recall)[:-4])\n",
        "      best_prec, best_recall = prec[best_idx], recall[best_idx]\n",
        "      threshold = thresholds[best_idx]\n",
        "      y_pred =  [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\n",
        "      f1_values.append(f1_score(error_df.anomaly, y_pred))\n",
        "\n",
        "      if (n >= end):\n",
        "        print(\"Final Output:\")\n",
        "        print(\"Return stopped epochs per iteration, \")\n",
        "        print(\"Return auc values per iteration, \")\n",
        "        print(\"Return precision scores with best thresholds per iteration, \")\n",
        "        print(\"Return recall scores with best thresholds per iteration, \")\n",
        "        print(\"Return f1 scores with best thresholds per iteration, \")\n",
        "        return(stopped_epochs,auc_values_roc,auc_values_recall_prec,f1_values)\n",
        "        break\n",
        "      n+=1\n",
        "      \n",
        "  else:\n",
        "    print(\"Start the Simulation:\")\n",
        "    while(True):\n",
        "      print(\"Lossfunction: \", loss),\n",
        "      print(\"Iteration: \",n)\n",
        "      #Step 1: Splitt data\n",
        "      train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "        data,\n",
        "        labels_bool,\n",
        "        test_size=test_size\n",
        "        )\n",
        "      train_data_no_validation, train_data_validation, train_data_no_validation_labels, train_data_validation_labels = train_test_split(\n",
        "        train_data,\n",
        "        train_labels,\n",
        "        test_size = test_size\n",
        "        )\n",
        "      #Step2: Train data\n",
        "      model = model\n",
        "      early_stopping = tf.keras.callbacks.EarlyStopping(patience=patience,\n",
        "                                                      restore_best_weights=True) #generate callback\n",
        "      model.compile(loss=loss, optimizer=optimizer, run_eagerly=True)\n",
        "      history = model.fit(train_data,\n",
        "                      train_data,\n",
        "                      batch_size=batch_size,\n",
        "                      epochs = epochs,\n",
        "                      validation_data=(train_data_validation, train_data_validation),\n",
        "                      callbacks=[\n",
        "                          early_stopping,\n",
        "                          ],\n",
        "                      verbose = 0,\n",
        "                      validation_batch_size = validation_batch_size,\n",
        "                      shuffle=True)\n",
        "      stopped_epochs.append(get_last_epoch(history)) #append stopped epoch\n",
        "      \n",
        "      #Step3: Evaluate test data with AUC\n",
        "      error_df = gen_error_df(model)\n",
        "      fpr, tpr, thresholds = roc_curve(error_df.anomaly, error_df.reconstruction_error)\n",
        "      auc_values_roc.append(auc(fpr, tpr))\n",
        "\n",
        "      #Step4: Evaluate test data with AUC\n",
        "      prec, recall, thresholds = precision_recall_curve(error_df[\"anomaly\"], error_df[\"reconstruction_error\"])\n",
        "      auc_values_recall_prec.append(auc(recall, prec))\n",
        "      best_idx = np.argmin(np.abs(prec-recall)[:-4])\n",
        "      best_prec, best_recall = prec[best_idx], recall[best_idx]\n",
        "      threshold = thresholds[best_idx]\n",
        "      y_pred =  [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\n",
        "      f1_values.append(f1_score(error_df.anomaly, y_pred))\n",
        "\n",
        "      if (n >= end):\n",
        "        print(\"Final Output:\")\n",
        "        print(\"Return stopped epochs per iteration, \")\n",
        "        print(\"Return auc values per iteration, \")\n",
        "        print(\"Return precision scores with best thresholds per iteration, \")\n",
        "        print(\"Return recall scores with best thresholds per iteration, \")\n",
        "        print(\"Return f1 scores with best thresholds per iteration, \")\n",
        "        return(stopped_epochs,auc_values_roc,auc_values_recall_prec,f1_values)\n",
        "        break\n",
        "      n+=1"
      ],
      "metadata": {
        "id": "nxHH6UsB2uzQ"
      },
      "id": "nxHH6UsB2uzQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simulation_mse = simulation(model = model_mse,\n",
        "                            end = 30,\n",
        "                            patience = 20, #EaryStopping after 20 epochs,\n",
        "                            epochs = 1000,\n",
        "                            loss = \"mean_squared_error\")\n",
        "with open(\"simulation_mse_breastW_.txt\", \"w\") as f:\n",
        "   # Convert the list to a string and write it to the file\n",
        "    f.write(str(simulation_mse))"
      ],
      "metadata": {
        "id": "fzwsv1odlPh6"
      },
      "id": "fzwsv1odlPh6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simulation_mae = simulation(model = model_mae,\n",
        "                            end = 30,\n",
        "                            patience = 20, #EaryStopping after 20 epochs,\n",
        "                            epochs = 1000,\n",
        "                            loss = \"mean_absolute_error\")\n",
        "with open(\"simulation_mae_breastW_.txt\", \"w\") as f:\n",
        "   # Convert the list to a string and write it to the file\n",
        "    f.write(str(simulation_mae))"
      ],
      "metadata": {
        "id": "IALzuD5m0Mr0"
      },
      "id": "IALzuD5m0Mr0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simulation_huber = simulation(model = model_huber,\n",
        "                            end = 30,\n",
        "                            patience = 20, #EaryStopping after 20 epochs,\n",
        "                            epochs = 1000,\n",
        "                            loss = \"huber_loss\",\n",
        "                            delta = 0.5)\n",
        "with open(\"simulation_huber_breastW_.txt\", \"w\") as f:\n",
        "   # Convert the list to a string and write it to the file\n",
        "    f.write(str(simulation_huber))"
      ],
      "metadata": {
        "id": "KjnbTKOR0QI7"
      },
      "id": "KjnbTKOR0QI7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simulation_ce= simulation(model = model_ce,\n",
        "                          end = 30,\n",
        "                          patience = 20, #EaryStopping after 20 epochs,\n",
        "                          epochs = 1000,\n",
        "                          loss = \"binary_crossentropy\")\n",
        "with open(\"simulation_ce_breastW_.txt\", \"w\") as f:\n",
        "   # Convert the list to a string and write it to the file\n",
        "    f.write(str(simulation_ce))"
      ],
      "metadata": {
        "id": "dffk5V8g0Vd9"
      },
      "id": "dffk5V8g0Vd9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simulation_focal =simulation(model = model_focal,\n",
        "                             end = 30,\n",
        "                             patience = 20, #EaryStopping after 20 epochs,\n",
        "                             epochs = 1000,\n",
        "                             loss =\"focal_loss\")\n",
        "with open(\"simulation_focal_breastW_.txt\", \"w\") as f:\n",
        "   # Convert the list to a string and write it to the file\n",
        "    f.write(str(simulation_focal))"
      ],
      "metadata": {
        "id": "r3ipEXzI0axQ"
      },
      "id": "r3ipEXzI0axQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in [simulation_mse, simulation_mae, simulation_huber, simulation_ce, simulation_focal]:\n",
        "  print(\"_______________________________________________________\")\n",
        "  print(\"Mean and standard deviation of last epochs\")\n",
        "  print(np.asarray(i[0]).mean())\n",
        "  print(np.asarray(i[0]).std())\n",
        "  print(\"Mean and standard deviation of ROC-AUC\")\n",
        "  print(np.asarray(i[1]).mean())\n",
        "  print(np.asarray(i[1]).std())\n",
        "  print(\"Mean and standard deviation of Recall-Precision-AUC\")\n",
        "  print(np.asarray(i[2]).mean())\n",
        "  print(np.asarray(i[2]).std())\n",
        "\n"
      ],
      "metadata": {
        "id": "u7R3LdnWO7bi"
      },
      "id": "u7R3LdnWO7bi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistische Evaluation\n",
        "\n",
        "Die Ergebnisse aus der Simulation wurden in Textfiles gespeichert. Um die Daten zu laden, wurden diese manuell hier herein kopiert. \n",
        "\n",
        "Quelle:\n",
        "\n",
        "\n",
        "*   simulation_mse_breastW_.txt\n",
        "\n",
        "*   simulation_mae_breastW_.txt\n",
        "\n",
        "*   simulation_huber_breastW_.txt\n",
        "\n",
        "*   simulation_ce_breastW_.txt\n",
        "\n",
        "*   simulation_focal_breastW_.txt\n",
        "\n",
        "Um die Resultate der Ausarbeitung zu sehen, den folgenden Block auskommentieren. Ansonsten kann eine neue Simulation verwendet werden (GPU-Unterst√ºtzung sollte daf√ºr aktiv sein)."
      ],
      "metadata": {
        "id": "AS5tHFlVoXKV"
      },
      "id": "AS5tHFlVoXKV"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# simulation_mse = [[83, 215, 57, 338, 44, 115, 71, 98, 41, 34, 97, 47, 28, 50, 47, 87, 92, 40, 34, 73, 77, 29, 26, 80, 63, 39, 60, 50, 22, 35, 32], [0.7263576779026217, 0.7146535580524345, 0.7205056179775281, 0.7315074906367041, 0.7256554307116105, 0.7357209737827715, 0.7312734082397004, 0.723314606741573, 0.7233146067415731, 0.7308052434456929, 0.7305711610486891, 0.7350187265917604, 0.7326779026217228, 0.7322097378277154, 0.7308052434456928, 0.7317415730337079, 0.7359550561797752, 0.7359550561797753, 0.7305711610486891, 0.7296348314606741, 0.7350187265917603, 0.7333801498127341, 0.7347846441947566, 0.7378277153558053, 0.7357209737827716, 0.7336142322097378, 0.7422752808988764, 0.7354868913857677, 0.7432116104868913, 0.7350187265917603, 0.7411048689138577], [0.48176857922048094, 0.4670740675305246, 0.4775567095162599, 0.4740908307338667, 0.4717914709485081, 0.476076902544321, 0.4748747898065486, 0.4698974893962126, 0.46709910802737914, 0.47229246844955336, 0.47117515637650004, 0.47749521431167735, 0.472484780642743, 0.47422774372666937, 0.4728186625328763, 0.47333476032415756, 0.47660213797134116, 0.47974132090976895, 0.47343942129970523, 0.4704574952104742, 0.47337968765219246, 0.47748451066693526, 0.47676490235837227, 0.4824133913904798, 0.48030668774202157, 0.4843403914346208, 0.4844864309003886, 0.48003156565189886, 0.48711794904534494, 0.4800151318751841, 0.4842461099644249], [0.46315789473684216, 0.46315789473684216, 0.4842105263157895, 0.5052631578947369, 0.5052631578947369, 0.5052631578947369, 0.46315789473684216, 0.4842105263157895, 0.4842105263157895, 0.4842105263157895, 0.4842105263157895, 0.46315789473684216, 0.46315789473684216, 0.46315789473684216, 0.4842105263157895, 0.46315789473684216, 0.4842105263157895, 0.4842105263157895, 0.4842105263157895, 0.4842105263157895, 0.4842105263157895, 0.4842105263157895, 0.4842105263157895, 0.4842105263157895, 0.4842105263157895, 0.4842105263157895, 0.4842105263157895, 0.4842105263157895, 0.5052631578947369, 0.4842105263157895, 0.4842105263157895]]\n",
        "# simulation_mae = [[128, 26, 70, 140, 22, 58, 101, 51, 36, 68, 58, 38, 24, 43, 20, 30, 43, 52, 62, 45, 36, 26, 37, 52, 39, 21, 23, 24, 22, 56, 50], [0.7078651685393258, 0.7116104868913857, 0.7158239700374532, 0.7202715355805244, 0.7186329588014981, 0.7109082397003745, 0.7286985018726592, 0.7272940074906367, 0.7249531835205992, 0.7219101123595506, 0.7256554307116105, 0.7258895131086143, 0.7244850187265918, 0.7205056179775281, 0.7244850187265918, 0.7305711610486891, 0.716994382022472, 0.7265917602996255, 0.7212078651685394, 0.7258895131086143, 0.7216760299625469, 0.7289325842696629, 0.7263576779026217, 0.7207397003745318, 0.7216760299625468, 0.7202715355805244, 0.722378277153558, 0.726123595505618, 0.7221441947565543, 0.7289325842696629, 0.7219101123595506], [0.45901317444488926, 0.4619196223201732, 0.4627237874503759, 0.4666265842335252, 0.4650070262117747, 0.45851249836724484, 0.47635534214147834, 0.4750241210448929, 0.4716508800866052, 0.46903469394525377, 0.4723533092192342, 0.47283404692277986, 0.471353590662186, 0.46799211503789206, 0.4712292213811213, 0.4747044461547346, 0.4645095370792167, 0.47353115856730893, 0.4678296348758231, 0.4738615875491491, 0.46922193610331026, 0.4748045446342627, 0.4733699413791063, 0.46876850377452717, 0.4690500513836704, 0.4680798457542617, 0.46983641508631485, 0.47286838051314706, 0.4686185712180406, 0.47458842935518186, 0.4684168894820359], [0.5319148936170214, 0.5473684210526316, 0.0, 0.0, 0.0, 0.0, 0.5473684210526316, 0.5473684210526316, 0.5473684210526316, 0.0, 0.5473684210526316, 0.5473684210526316, 0.0, 0.0, 0.5473684210526316, 0.5473684210526316, 0.0, 0.5473684210526316, 0.0, 0.5473684210526316, 0.0, 0.5473684210526316, 0.5473684210526316, 0.5473684210526316, 0.0, 0.0, 0.5473684210526316, 0.5473684210526316, 0.0, 0.5473684210526316, 0.0]]\n",
        "# simulation_huber = [[22, 38, 104, 47, 85, 49, 22, 55, 33, 23, 35, 39, 32, 36, 84, 66, 69, 46, 45, 63, 32, 36, 30, 36, 27, 39, 22, 41, 46, 26, 48], [0.7055243445692884, 0.6914794007490637, 0.7010767790262172, 0.7057584269662921, 0.7106741573033708, 0.7043539325842697, 0.7080992509363295, 0.696629213483146, 0.7062265917602997, 0.7029494382022472, 0.7045880149812734, 0.6952247191011236, 0.7038857677902621, 0.7062265917602997, 0.7010767790262172, 0.7071629213483146, 0.7073970037453183, 0.7055243445692884, 0.7137172284644194, 0.7057584269662921, 0.7027153558052435, 0.7092696629213483, 0.7066947565543071, 0.7052902621722846, 0.7045880149812734, 0.7085674157303371, 0.7020131086142322, 0.7052902621722846, 0.7055243445692884, 0.7024812734082397, 0.7071629213483146], [0.48511685602285015, 0.4480950747034451, 0.4555284598253575, 0.45877104863888, 0.461021367751396, 0.47958902705348505, 0.46354960643998144, 0.4528752617026056, 0.45890730822589154, 0.4565499774702527, 0.46173628429241165, 0.45211201088517294, 0.4572496369132849, 0.4583010441593189, 0.453886154777847, 0.4618191677894363, 0.4632236357758274, 0.4595699686124987, 0.4857400122070571, 0.45789591490167236, 0.45765476878348255, 0.48556623361873247, 0.4592732580467916, 0.46305588212561893, 0.45744858299427427, 0.46016559096892384, 0.4556737282740109, 0.4597732182194534, 0.45909577716143274, 0.4539226879092227, 0.4596671143555374], [0.5473684210526316, 0.5263157894736842, 0.5052631578947369, 0.5263157894736842, 0.5473684210526316, 0.5052631578947369, 0.5263157894736842, 0.5263157894736842, 0.5052631578947369, 0.5052631578947369, 0.4842105263157895, 0.4842105263157895, 0.5263157894736842, 0.5263157894736842, 0.5263157894736842, 0.5052631578947369, 0.5263157894736842, 0.5263157894736842, 0.5052631578947369, 0.5263157894736842, 0.5263157894736842, 0.5263157894736842, 0.5052631578947369, 0.5263157894736842, 0.5263157894736842, 0.5473684210526316, 0.5052631578947369, 0.5052631578947369, 0.5052631578947369, 0.5052631578947369, 0.4842105263157895]]\n",
        "# simulation_ce = [[485, 189, 78, 62, 22, 27, 68, 39, 39, 108, 80, 54, 91, 56, 36, 32, 60, 33, 27, 58, 51, 81, 34, 41, 51, 92, 107, 32, 30, 21, 26], [0.7120300751879699, 0.7255639097744362, 0.7313283208020049, 0.7233082706766917, 0.7436090225563909, 0.7413533834586467, 0.712531328320802, 0.7208020050125313, 0.7360902255639098, 0.7255639097744361, 0.7263157894736842, 0.7345864661654136, 0.7343358395989975, 0.7378446115288221, 0.7328320802005012, 0.7340852130325815, 0.7413533834586467, 0.7418546365914787, 0.7466165413533834, 0.737593984962406, 0.7491228070175439, 0.7483709273182957, 0.7448621553884711, 0.7335839598997493, 0.7451127819548873, 0.756641604010025, 0.7546365914786968, 0.7526315789473683, 0.7493734335839598, 0.7506265664160401, 0.7421052631578947], [0.47619047619047616, 0.5238095238095238, 0.5, 0.5, 0.5238095238095238, 0.5238095238095238, 0.5, 0.5238095238095238, 0.5238095238095238, 0.5, 0.47619047619047616, 0.5238095238095238, 0.5, 0.5238095238095238, 0.5, 0.5, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5], [0.47619047619047616, 0.5238095238095238, 0.5, 0.5, 0.5238095238095238, 0.5238095238095238, 0.5, 0.5238095238095238, 0.5238095238095238, 0.5, 0.47619047619047616, 0.5238095238095238, 0.5, 0.5238095238095238, 0.5, 0.5, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5], [0.4819277108433735, 0.5060240963855421, 0.5060240963855421, 0.5060240963855421, 0.5301204819277109, 0.5301204819277109, 0.4819277108433735, 0.5060240963855421, 0.5060240963855421, 0.4819277108433735, 0.4819277108433735, 0.5301204819277109, 0.5060240963855421, 0.5060240963855421, 0.5060240963855421, 0.5060240963855421, 0.5301204819277109, 0.5301204819277109, 0.5301204819277109, 0.5060240963855421, 0.5301204819277109, 0.5301204819277109, 0.5301204819277109, 0.5301204819277109, 0.5301204819277109, 0.5301204819277109, 0.5301204819277109, 0.5301204819277109, 0.5060240963855421, 0.5301204819277109, 0.5060240963855421]]\n",
        "# simulation_focal = [[32, 26, 44, 35, 71, 21, 46, 74, 48, 34, 24, 39, 30, 27, 20, 44, 20, 25, 22, 38, 20, 26, 81, 31, 20, 27, 20, 41, 29, 30, 23], [0.4740168539325842, 0.4691011235955056, 0.4676966292134831, 0.46980337078651685, 0.47425093632958804, 0.47191011235955055, 0.4679307116104869, 0.4662921348314606, 0.46956928838951306, 0.4700374531835205, 0.4777621722846442, 0.46652621722846443, 0.46722846441947563, 0.4735486891385768, 0.47425093632958804, 0.4726123595505618, 0.4719101123595505, 0.4665262172284643, 0.46863295880149813, 0.4726123595505618, 0.4691011235955056, 0.46816479400749056, 0.47659176029962547, 0.47659176029962547, 0.47354868913857684, 0.4698033707865169, 0.46863295880149813, 0.47448501872659177, 0.4737827715355805, 0.4691011235955056, 0.4669943820224719], [0.31915164371855187, 0.31658236479639157, 0.3159374131222968, 0.3168479031200666, 0.31908087560415554, 0.31769398481776634, 0.31632000546305017, 0.3153905225025381, 0.31718954444088937, 0.3172862206179785, 0.3212815320978825, 0.3156302684987513, 0.3157176067165215, 0.3189039476561454, 0.31910487418090017, 0.31852958677993504, 0.3180342331985332, 0.3156481398496002, 0.31683704307058663, 0.3184592858328014, 0.31677252895686214, 0.3162018731406295, 0.320709574079507, 0.32025286140578596, 0.31846127498792043, 0.3175745337847127, 0.3167964246710562, 0.3196675170673019, 0.3189920353355094, 0.31743411013544826, 0.31570847905756383], [0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105, 0.3157894736842105]]\n"
      ],
      "metadata": {
        "id": "cDgl_i80w-EZ"
      },
      "id": "cDgl_i80w-EZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = [\n",
        "    simulation_mse[0],\n",
        "    simulation_mae[0],\n",
        "    simulation_huber[0],\n",
        "    simulation_ce[0],\n",
        "    simulation_focal[0],\n",
        "    simulation_mse[1],\n",
        "    simulation_mae[1],\n",
        "    simulation_huber[1],\n",
        "    simulation_ce[1],\n",
        "    simulation_focal[1],\n",
        "    simulation_mse[2],\n",
        "    simulation_mae[2],\n",
        "    simulation_huber[2],\n",
        "    simulation_ce[2],\n",
        "    simulation_focal[2]   \n",
        "]"
      ],
      "metadata": {
        "id": "3wIl_qAEtB8a"
      },
      "execution_count": null,
      "outputs": [],
      "id": "3wIl_qAEtB8a"
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame(zip(*summary),\n",
        "                       columns=[\n",
        "                           \"last_epochs_mse\",\n",
        "                           \"last_epochs_mae\",\n",
        "                           \"last_epochs_huber\",\n",
        "                           \"last_epochs_ce\",\n",
        "                           \"last_epochs_focal\",\n",
        "                           \"auc_mse\",\n",
        "                           \"auc_mae\",\n",
        "                           \"auc_huber\",\n",
        "                           \"auc_ce\",\n",
        "                           \"auc_focal\",\n",
        "                           \"auprc_mse\",\n",
        "                           \"auprc_mae\",\n",
        "                           \"auprc_huber\",\n",
        "                           \"auprc_ce\",\n",
        "                           \"auprc_focal\"\n",
        "                       ]\n",
        "\n",
        ")\n",
        "\n",
        "results"
      ],
      "metadata": {
        "id": "mDjVmp6QtB8b"
      },
      "execution_count": null,
      "outputs": [],
      "id": "mDjVmp6QtB8b"
    },
    {
      "cell_type": "code",
      "source": [
        "#print boxplot for each variable\n",
        "fig, axs = plt.subplots(ncols=5, nrows=3, figsize=(20, 10))\n",
        "index = 0\n",
        "axs = axs.flatten()\n",
        "for k,v in results.items():\n",
        "    sns.boxplot(y=k, data=results, ax=axs[index])\n",
        "    index += 1\n",
        "save_fig(\"boxplots_results\")\n",
        "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)"
      ],
      "metadata": {
        "id": "16FvdHuetB8b"
      },
      "execution_count": null,
      "outputs": [],
      "id": "16FvdHuetB8b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kruskal-Wallis-Test"
      ],
      "metadata": {
        "id": "DhBiT7he15_B"
      },
      "id": "DhBiT7he15_B"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Der [Kruskal-Wallis-Test](https:https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kruskal.html//) ist ein nichtparametrischer Test, der zum Vergleich der Mittelwerte von drei oder mehr unabh√§ngigen Gruppen verwendet werden kann. Er √§hnelt der einseitigen ANOVA, setzt aber nicht voraus, dass die Daten normal verteilt sind.\n",
        "\n",
        "Zur Durchf√ºhrung des Kruskal-Wallis-Tests werden die Daten zun√§chst vom niedrigsten zum h√∂chsten Wert geordnet. Die R√§nge werden dann zur Berechnung einer Teststatistik verwendet, die mit einem kritischen Wert verglichen wird, um festzustellen, ob der Unterschied zwischen den Gruppenmitteln statistisch signifikant ist.\n",
        "\n",
        "Der Kruskal-Wallis-Test ist geeignet, wenn die zu vergleichenden Populationen unabh√§ngig sind und die Daten ordinal (d. h., sie k√∂nnen in eine Rangfolge gebracht werden) oder kontinuierlich (aber nicht normalverteilt) sind. Er ist auch geeignet, wenn die Stichprobengr√∂√üe klein oder ungleich ist.\n",
        "\n"
      ],
      "metadata": {
        "id": "cAJ0U1zb1-eW"
      },
      "id": "cAJ0U1zb1-eW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last-Epochs: Gibt es Unterschiede bez√ºglich Konvergenz? "
      ],
      "metadata": {
        "id": "TiECRXFK4Zpp"
      },
      "id": "TiECRXFK4Zpp"
    },
    {
      "cell_type": "code",
      "source": [
        "for i in [simulation_mse, simulation_mae, simulation_huber, simulation_ce, simulation_focal]:\n",
        "  print(\"_______________________________________________________\")\n",
        "  print(\"Mean and standard deviation of last epochs\")\n",
        "  print(np.asarray(i[0]).mean())\n",
        "  print(np.asarray(i[0]).std())"
      ],
      "metadata": {
        "id": "nuQTmqfp_YIz"
      },
      "id": "nuQTmqfp_YIz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "'''\n",
        "Null hypothesis that the population median of all of the groups are equal\n",
        "\n",
        "Here: stopped epochs to evaluate the convergence ability \n",
        "'''\n",
        "stats.kruskal(\n",
        "    np.hstack(simulation_mse[0]),\n",
        "    np.hstack(simulation_mae[0]),\n",
        "    np.hstack(simulation_huber[0]),\n",
        "    np.hstack(simulation_ce[0]),\n",
        "    np.hstack(simulation_focal[0]),\n",
        "     )\n",
        "# KruskalResult(statistic=22.23847083667068, pvalue=0.0001796605135019599)\n"
      ],
      "metadata": {
        "id": "f0WyXHEZPkEP"
      },
      "id": "f0WyXHEZPkEP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AUC: Gibt es Unterschiede f√ºr die Performance in den Testdaten?"
      ],
      "metadata": {
        "id": "fteyhLbW4f_n"
      },
      "id": "fteyhLbW4f_n"
    },
    {
      "cell_type": "code",
      "source": [
        "for i in [simulation_mse, simulation_mae, simulation_huber, simulation_ce, simulation_focal]:\n",
        "  print(\"_______________________________________________________\")\n",
        "  print(\"Mean and standard deviation of roc_auc\")\n",
        "  print(np.asarray(i[1]).mean())\n",
        "  print(np.asarray(i[1]).std())"
      ],
      "metadata": {
        "id": "sSoLCgzh_coN"
      },
      "id": "sSoLCgzh_coN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "'''\n",
        "Null hypothesis that the population median of all of the groups are equal\n",
        "\n",
        "Here: stopped epochs to evaluate the convergence ability \n",
        "'''\n",
        "stats.kruskal(\n",
        "    np.hstack(simulation_mse[1]),\n",
        "    np.hstack(simulation_mae[1]),\n",
        "    np.hstack(simulation_huber[1]),\n",
        "    np.hstack(simulation_ce[1]),\n",
        "    np.hstack(simulation_focal[1]),\n",
        "     )\n",
        " # KruskalResult(statistic=131.99214948486943, pvalue=1.459883736690209e-27)\n"
      ],
      "metadata": {
        "id": "IAVYjCdx2-lw"
      },
      "id": "IAVYjCdx2-lw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AUPRC: Gibt es Unterschiede f√ºr die Performance in den Testdaten?"
      ],
      "metadata": {
        "id": "CtNMo1Cq4ryp"
      },
      "id": "CtNMo1Cq4ryp"
    },
    {
      "cell_type": "code",
      "source": [
        "for i in [simulation_mse, simulation_mae, simulation_huber, simulation_ce, simulation_focal]:\n",
        "  print(\"_______________________________________________________\")\n",
        "  print(\"Mean and standard deviation of roc_recall_precision\")\n",
        "  print(np.asarray(i[2]).mean())\n",
        "  print(np.asarray(i[2]).std())"
      ],
      "metadata": {
        "id": "h1eQ8WSp_j7p"
      },
      "id": "h1eQ8WSp_j7p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "'''\n",
        "Null hypothesis that the population median of all of the groups are equal\n",
        "\n",
        "Here: stopped epochs to evaluate the convergence ability \n",
        "'''\n",
        "stats.kruskal(\n",
        "    np.hstack(simulation_mse[2]),\n",
        "    np.hstack(simulation_mae[2]),\n",
        "    np.hstack(simulation_huber[2]),\n",
        "    np.hstack(simulation_ce[2]),\n",
        "    np.hstack(simulation_focal[2]),\n",
        "     )\n",
        "# KruskalResult(statistic=131.86117955510082, pvalue=1.557160300415864e-27)\n"
      ],
      "metadata": {
        "id": "yvYPtm1z2_e-"
      },
      "id": "yvYPtm1z2_e-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wenn die Nullhypothese des Kruskal-Wallis-Tests abgelehnt wird, bedeutet dies, dass sich mindestens einer der Gruppenmittelwerte signifikant von den anderen unterscheidet. Der Kruskal-Wallis-Test sagt jedoch nicht aus, welche Gruppe(n) sich unterscheiden. Um festzustellen, welche Gruppe(n) sich unterscheidet/unterscheiden, muss eine zus√§tzliche Analyse durchgef√ºhrt werden, z. B. ein Post-hoc-Test.\n",
        "\n",
        "Es gibt mehrere Post-hoc-Tests, die zum Vergleich der Mittelwerte bestimmter Gruppenpaare verwendet werden k√∂nnen. Zu den g√§ngigen Tests geh√∂ren der [Dunn-Test](https://scikit-posthocs.readthedocs.io/en/latest/generated/scikit_posthocs.posthoc_dunn), der [Conover-Test](https://scikit-posthocs.readthedocs.io/en/latest/generated/scikit_posthocs.posthoc_conover) und der [Steel-Dwass-Test](https://scikit-posthocs.readthedocs.io/en/latest/generated/scikit_posthocs.posthoc_dscf). Diese Tests verwenden die Rangdaten aus dem Kruskal-Wallis-Test, um festzustellen, welche Gruppenpaare signifikant unterschiedliche Mittelwerte aufweisen.\n",
        "\n",
        "Alle diese Post-hoc-Tests k√∂nnen verwendet werden, um festzustellen, welche Gruppenpaare nach Durchf√ºhrung des Kruskal-Wallis-Tests signifikant unterschiedliche Mittelwerte aufweisen.\n",
        "\n"
      ],
      "metadata": {
        "id": "uyzXbYfm2oRg"
      },
      "id": "uyzXbYfm2oRg"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-posthocs"
      ],
      "metadata": {
        "id": "JdIVIJbU7fvV"
      },
      "id": "JdIVIJbU7fvV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Dunn-Test](https://scikit-posthocs.readthedocs.io/en/latest/generated/scikit_posthocs.posthoc_dunn)\n",
        "\n",
        "Der Dunn-Test ist ein Post-hoc-Test, der die Mittelwerte aller Gruppenpaare unter Verwendung der Rangdaten aus dem Kruskal-Wallis-Test vergleicht. Er basiert auf der Differenz der R√§nge zwischen den beiden zu vergleichenden Gruppen und passt sich durch Kontrolle der Falschentdeckungsrate an Mehrfachvergleiche an."
      ],
      "metadata": {
        "id": "2khLAXlO6McU"
      },
      "id": "2khLAXlO6McU"
    },
    {
      "cell_type": "code",
      "source": [
        "# last-epochs\n",
        "import scikit_posthocs as sp\n",
        "\n",
        "last_epochs = [np.hstack(simulation_mse[0]),\n",
        "    np.hstack(simulation_mae[0]),\n",
        "    np.hstack(simulation_huber[0]),\n",
        "    np.hstack(simulation_ce[0]),\n",
        "    np.hstack(simulation_focal[0])]\n",
        "\n",
        "sp.posthoc_dunn(last_epochs, p_adjust = 'holm')"
      ],
      "metadata": {
        "id": "2Wrb8pRT6t0T"
      },
      "id": "2Wrb8pRT6t0T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beobachtung:\n",
        "\n",
        "\n",
        "*   Der Focal-Loss entscheidet sich signifikant bzgl. Konvergenz von MSE und der CE \n",
        "*   Ansonsten wurde keine signifikanten Ergebnisse erzielt\n",
        "\n"
      ],
      "metadata": {
        "id": "d382uY7b7wfE"
      },
      "id": "d382uY7b7wfE"
    },
    {
      "cell_type": "code",
      "source": [
        "# roc_auc\n",
        "import scikit_posthocs as sp\n",
        "\n",
        "roc_auc = [np.hstack(simulation_mse[1]),\n",
        "    np.hstack(simulation_mae[1]),\n",
        "    np.hstack(simulation_huber[1]),\n",
        "    np.hstack(simulation_ce[1]),\n",
        "    np.hstack(simulation_focal[1])]\n",
        "\n",
        "sp.posthoc_dunn(roc_auc, p_adjust = 'holm')"
      ],
      "metadata": {
        "id": "cTslCFAi6twY"
      },
      "id": "cTslCFAi6twY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# recall_precision_auc\n",
        "import scikit_posthocs as sp\n",
        "\n",
        "recall_precision_auc = [np.hstack(simulation_mse[2]),\n",
        "    np.hstack(simulation_mae[2]),\n",
        "    np.hstack(simulation_huber[2]),\n",
        "    np.hstack(simulation_ce[2]),\n",
        "    np.hstack(simulation_focal[2])]\n",
        "\n",
        "sp.posthoc_dunn(recall_precision_auc, p_adjust = 'holm')"
      ],
      "metadata": {
        "id": "tjqH44_Z6tlf"
      },
      "id": "tjqH44_Z6tlf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Endresultat f√ºr breastW"
      ],
      "metadata": {
        "id": "rJ2dqOkYBi1i"
      },
      "id": "rJ2dqOkYBi1i"
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Performance                  | Loss       |\n",
        "|------------------------------|------------|\n",
        "| Schnellste Konvergenz?       | FL (gegen√ºber MSE und CE)         |\n",
        "| Bester ROC-AUC?              | MSE & CE   |\n",
        "| Bester ROC-Recall-Precision? | CE         |\n",
        "\n",
        "Anhand dieser Simulation hat die CE die beste Performance gezeigt mit AUPRC und AUC. Der MSE ist in der Tabelle gelistet, da dieser sich mit der AUC-Metrik sich nicht signifikant zum CE-Modell unterscheidet. Der CE erzielt im Schnitt  die langsamste Konvergenz.\n",
        "\n",
        "Aufgrund der Performance mit der AUPRC-Metrik hat die CE die beste Performance insgesamt gezeigt und wird als finalles Modell in dieser Simulation ausgew√§hlt.\n",
        "\n",
        "Der Focal-Loss hat zwar die schnellste Konvergenz aufgezeigt, performt im Gegensatz zu den anderen Modellen signifikant am schlechtesten.\n",
        "\n"
      ],
      "metadata": {
        "id": "oJxRjhOTAYGP"
      },
      "id": "oJxRjhOTAYGP"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 28.912306,
      "end_time": "2021-07-25T13:21:52.129684",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-07-25T13:21:23.217378",
      "version": "2.3.3"
    },
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}